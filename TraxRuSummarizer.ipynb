{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPpv/3f02Xb/bAGBUbpvNnz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed37ba2b-9924-4a01-c569-f83d8cd5b017"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 7.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 13.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 18.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 49.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 57.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 12.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 51.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 52.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 47.1MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "fabd7b3f-1b26-43c5-d7e6-fe46785bb4d9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "# data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "# data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "# data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "# data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZ9ElEQVR4nO3df5AcZ53f8fcn0skIG1sSJhOXpNyKQyElrOSQtmylOKg1IvLKcMhJDCWXC605Haor7ItJlAL5qEQU4Co7Vz4fTsBXOqSSRHyWfT4oqc5yhCI8oa4qMv6J17IxWsvirC1ZCpaQb7HBt+SbP/pZ6Fvm2fX07MyOpc+ramq7v/083d/pne3vdvczM4oIzMzMGvlH052AmZl1LxcJMzPLcpEwM7MsFwkzM8tykTAzs6yZ053AVLv44oujp6enUt+f/vSnnH/++VOb0BRwXs1xXs3p1ryge3M7G/N67LHHfhwR7/i1BRFxVj2WL18eVT300EOV+7aT82qO82pOt+YV0b25nY15AY9Gg2OqLzeZmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWdZZ97EcZt1qcPgM1296YFq2ffTWD0/Ldu3Nz2cSZmaW5SJhZmZZLhJmZpblImFmZlmTFglJ2ySdlPR0g2UbJYWki9O8JN0paUjSU5KWldoOSDqcHgOl+HJJg6nPnZKU4vMk7U/t90uaOzVP2czM3qg3ciaxHegfH5S0EFgF/G0pvBpYnB4bgLtS23nAZuBy4DJgc+mgfxfwqVK/sW1tAg5ExGLgQJo3M7MOmrRIRMR3gVMNFt0BfBaIUmwNsDN9h8VBYI6kS4Argf0RcSoiTgP7gf607MKIOJi+9GIncHVpXTvS9I5S3MzMOqTS+yQkrQGGI+L76erQmPnAi6X5Yyk2UfxYgzhALSKOp+mXgNoE+WygOHOhVqtRr9ebfEaFkZGRyn3byXk1p1vzqs2GjUtHp2XbE+2Pbt1f0L25nUt5NV0kJL0V+COKS00dEREhKSZYvgXYAtDb2xt9fX2VtlOv16nat52cV3O6Na//dvdubh+cnvevHr2uL7usW/cXdG9u51JeVUY3/RawCPi+pKPAAuBxSf8EGAYWltouSLGJ4gsaxAFOpMtRpJ8nK+RqZmYtaLpIRMRgRPzjiOiJiB6KS0TLIuIlYA+wLo1yWgGcSZeM9gGrJM1NN6xXAfvSslckrUijmtYBu9Om9gBjo6AGSnEzM+uQNzIE9h7g/wDvlnRM0voJmu8FjgBDwJ8DnwaIiFPAl4BH0uOLKUZq8/XU53ngwRS/FfjXkg4DH0rzZmbWQZNeII2IaydZ3lOaDuCGTLttwLYG8UeBSxvEXwZWTpafmZm1j99xbWZmWS4SZmaW5SJhZmZZ/tIhs3NAzwRfdrRx6WjbvgzJX3b05uczCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7OsSYuEpG2STkp6uhT7Y0k/kPSUpG9JmlNadrOkIUnPSbqyFO9PsSFJm0rxRZIeTvF7Jc1K8fPS/FBa3jNVT9rMzN6YN3ImsR3oHxfbD1waEf8C+CFwM4CkJcBa4D2pz9ckzZA0A/gqsBpYAlyb2gLcBtwREe8CTgPrU3w9cDrF70jtzMysgyYtEhHxXeDUuNi3I2I0zR4EFqTpNcCuiPh5RLwADAGXpcdQRByJiNeBXcAaSQI+CNyf+u8Ari6ta0eavh9YmdqbmVmHTMV3XP8ecG+ank9RNMYcSzGAF8fFLwfeDvykVHDK7eeP9YmIUUlnUvsfj09A0gZgA0CtVqNer1d6IiMjI5X7tpPzas5keQ0On+lcMiW12cX3SXebdubV6uvjzfoamy7tyKulIiHp88AocPfUpFNNRGwBtgD09vZGX19fpfXU63Wq9m0n59WcyfK6ftMDnUumZOPSUW4fnIr/y6ZWO/M6el1fS/3frK+x6dKOvCq/MiRdD3wEWBkRkcLDwMJSswUpRib+MjBH0sx0NlFuP7auY5JmAhel9mZm1iGVhsBK6gc+C3w0Il4tLdoDrE0jkxYBi4HvAY8Ai9NIplkUN7f3pOLyEHBN6j8A7C6tayBNXwN8p1SMzMysAyY9k5B0D9AHXCzpGLCZYjTTecD+dC/5YET8QUQcknQf8AzFZagbIuIXaT03AvuAGcC2iDiUNvE5YJekLwNPAFtTfCvwDUlDFDfO107B8zUzsyZMWiQi4toG4a0NYmPtbwFuaRDfC+xtED9CMfppfPxnwMcmy8/MzNrH77g2M7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8vqvu9SNLOzRk+LXxW7celo5a+bPXrrh1vathV8JmFmZlkuEmZmluUiYWZmWS4SZmaWNWmRkLRN0klJT5di8yTtl3Q4/Zyb4pJ0p6QhSU9JWlbqM5DaH5Y0UIovlzSY+twpSRNtw8zMOueNnElsB/rHxTYBByJiMXAgzQOsBhanxwbgLigO+MBm4HLgMmBz6aB/F/CpUr/+SbZhZmYdMmmRiIjvAqfGhdcAO9L0DuDqUnxnFA4CcyRdAlwJ7I+IUxFxGtgP9KdlF0bEwYgIYOe4dTXahpmZdUjV90nUIuJ4mn4JqKXp+cCLpXbHUmyi+LEG8Ym28WskbaA4c6FWq1Gv15t8OoWRkZHKfdvJeTVnsrw2Lh3tXDIltdnTt+2JdGte0Fpu7Xxtvllf+1W0/Ga6iAhJMRXJVN1GRGwBtgD09vZGX19fpe3U63Wq9m0n59WcyfKq+uasVm1cOsrtg933/tVuzQtay+3odX1Tm0zJm/W1X0XV0U0n0qUi0s+TKT4MLCy1W5BiE8UXNIhPtA0zM+uQqkViDzA2QmkA2F2Kr0ujnFYAZ9Ilo33AKklz0w3rVcC+tOwVSSvSqKZ149bVaBtmZtYhk57HSboH6AMulnSMYpTSrcB9ktYDPwI+nprvBa4ChoBXgU8CRMQpSV8CHkntvhgRYzfDP00xgmo28GB6MME2zMysQyYtEhFxbWbRygZtA7ghs55twLYG8UeBSxvEX260DTMz6xy/49rMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJaKhKS/oOkQ5KelnSPpLdIWiTpYUlDku6VNCu1PS/ND6XlPaX13Jziz0m6shTvT7EhSZtaydXMzJpXuUhImg/8e6A3Ii4FZgBrgduAOyLiXcBpYH3qsh44neJ3pHZIWpL6vQfoB74maYakGcBXgdXAEuDa1NbMzDqk1ctNM4HZkmYCbwWOAx8E7k/LdwBXp+k1aZ60fKUkpfiuiPh5RLwADAGXpcdQRByJiNeBXamtmZl1iCKiemfpJuAW4DXg28BNwMF0toCkhcCDEXGppKeB/og4lpY9D1wOfCH1+R8pvhV4MG2iPyJ+P8U/AVweETc2yGMDsAGgVqst37VrV6XnMzIywgUXXFCpbzs5r+ZMltfg8JkOZvMrtdlw4rVp2fSEujUvaC23pfMvmtpkSt6sr/2JXHHFFY9FRO/4+MyqyUiaS/Gf/SLgJ8BfUlwu6riI2AJsAejt7Y2+vr5K66nX61Tt207OqzmT5XX9pgc6l0zJxqWj3D5Y+U+ubbo1L2gtt6PX9U1tMiVv1td+Fa1cbvoQ8EJE/N+I+Hvgm8D7gDnp8hPAAmA4TQ8DCwHS8ouAl8vxcX1ycTMz65BWisTfAiskvTXdW1gJPAM8BFyT2gwAu9P0njRPWv6dKK517QHWptFPi4DFwPeAR4DFabTULIqb23tayNfMzJpU+RwzIh6WdD/wODAKPEFxyecBYJekL6fY1tRlK/ANSUPAKYqDPhFxSNJ9FAVmFLghIn4BIOlGYB/FyKltEXGoar5mZta8li5ERsRmYPO48BGKkUnj2/4M+FhmPbdQ3AAfH98L7G0lRzMzq87vuDYzsywXCTMzy+rOcW9mZi3qaeNQ541LR7NDqY/e+uG2bXc6+EzCzMyyXCTMzCzLRcLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLHxVu02K6PsbZzJrjMwkzM8tykTAzs6yWioSkOZLul/QDSc9K+leS5knaL+lw+jk3tZWkOyUNSXpK0rLSegZS+8OSBkrx5ZIGU587JamVfM3MrDmtnkl8BfifEfHPgX8JPAtsAg5ExGLgQJoHWA0sTo8NwF0AkuYBm4HLgcuAzWOFJbX5VKlff4v5mplZEyoXCUkXAR8AtgJExOsR8RNgDbAjNdsBXJ2m1wA7o3AQmCPpEuBKYH9EnIqI08B+oD8tuzAiDkZEADtL6zIzsw5Qcfyt0FH6bWAL8AzFWcRjwE3AcETMSW0EnI6IOZL+Grg1Iv4mLTsAfA7oA94SEV9O8f8MvAbUU/sPpfj7gc9FxEca5LKB4uyEWq22fNeuXZWe08jICBdccEGlvu10NuY1OHxmirP5ldpsOPFa21ZfmfNqXrfmNlFeS+df1NlkSlr5m7ziiisei4je8fFWhsDOBJYBfxgRD0v6Cr+6tARARISkalWoCRGxhaJg0dvbG319fZXWU6/Xqdq3nc7GvNo5RHXj0lFuH+y+0d3Oq3ndmttEeR29rq+zyZS041jRyj2JY8CxiHg4zd9PUTROpEtFpJ8n0/JhYGGp/4IUmyi+oEHczMw6pHKRiIiXgBclvTuFVlJcetoDjI1QGgB2p+k9wLo0ymkFcCYijgP7gFWS5qYb1quAfWnZK5JWpMtW60rrMjOzDmj1PO4PgbslzQKOAJ+kKDz3SVoP/Aj4eGq7F7gKGAJeTW2JiFOSvgQ8ktp9MSJOpelPA9uB2cCD6WFmZh3SUpGIiCeBX7vRQXFWMb5tADdk1rMN2NYg/ihwaSs5mplZdX7HtZmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVlW930Gr5nZm1hPGz8GfzLb+8+f8nX6TMLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLKvlIiFphqQnJP11ml8k6WFJQ5LulTQrxc9L80NpeU9pHTen+HOSrizF+1NsSNKmVnM1M7PmTMWZxE3As6X524A7IuJdwGlgfYqvB06n+B2pHZKWAGuB9wD9wNdS4ZkBfBVYDSwBrk1tzcysQ1oqEpIWAB8Gvp7mBXwQuD812QFcnabXpHnS8pWp/RpgV0T8PCJeAIaAy9JjKCKORMTrwK7U1szMOqTVz276U+CzwNvS/NuBn0TEaJo/BsxP0/OBFwEiYlTSmdR+PnCwtM5ynxfHxS9vlISkDcAGgFqtRr1er/RkRkZGKvdtp7Mxr41LRydvVFFtdnvXX5Xzal635tatebXjWFG5SEj6CHAyIh6T1Dd1KTUvIrYAWwB6e3ujr69aOvV6nap92+lszOv6Nn4I2salo9w+2H2fXem8mtetuXVrXtv7z5/yY0Urz/J9wEclXQW8BbgQ+AowR9LMdDaxABhO7YeBhcAxSTOBi4CXS/Ex5T65uJmZdUDlexIRcXNELIiIHoobz9+JiOuAh4BrUrMBYHea3pPmScu/ExGR4mvT6KdFwGLge8AjwOI0WmpW2saeqvmamVnz2nG+9Dlgl6QvA08AW1N8K/ANSUPAKYqDPhFxSNJ9wDPAKHBDRPwCQNKNwD5gBrAtIg61IV8zM8uYkiIREXWgnqaPUIxMGt/mZ8DHMv1vAW5pEN8L7J2KHM3MrHl+x7WZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZVvd9k7d1VM+mByr33bh0lOtb6G9m3c9nEmZmluUiYWZmWZWLhKSFkh6S9IykQ5JuSvF5kvZLOpx+zk1xSbpT0pCkpyQtK61rILU/LGmgFF8uaTD1uVOSWnmyZmbWnFbOJEaBjRGxBFgB3CBpCbAJOBARi4EDaR5gNbA4PTYAd0FRVIDNwOXAZcDmscKS2nyq1K+/hXzNzKxJlYtERByPiMfT9N8BzwLzgTXAjtRsB3B1ml4D7IzCQWCOpEuAK4H9EXEqIk4D+4H+tOzCiDgYEQHsLK3LzMw6YEpGN0nqAd4LPAzUIuJ4WvQSUEvT84EXS92OpdhE8WMN4o22v4Hi7IRarUa9Xq/0PEZGRir3bad25rVx6WjlvrXZrfVvF+fVnG7NC7o3t27Nqx3HipaLhKQLgL8CPhMRr5RvG0RESIpWtzGZiNgCbAHo7e2Nvr6+Suup1+tU7dtO7cyrlSGsG5eOcvtg942idl7N6da8oHtz69a8tvefP+XHipZGN0n6DYoCcXdEfDOFT6RLRaSfJ1N8GFhY6r4gxSaKL2gQNzOzDmlldJOArcCzEfEnpUV7gLERSgPA7lJ8XRrltAI4ky5L7QNWSZqbblivAvalZa9IWpG2ta60LjMz64BWzpfeB3wCGJT0ZIr9EXArcJ+k9cCPgI+nZXuBq4Ah4FXgkwARcUrSl4BHUrsvRsSpNP1pYDswG3gwPczMrEMqF4mI+Bsg976FlQ3aB3BDZl3bgG0N4o8Cl1bN0czMWuN3XJuZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlldd83eZ+jejY9kF22ceko10+w3MysXXwmYWZmWS4SZmaW5SJhZmZZXX9PQlI/8BVgBvD1iLi1XdsaHD7ja/9mZiVdfSYhaQbwVWA1sAS4VtKS6c3KzOzc0dVFArgMGIqIIxHxOrALWDPNOZmZnTMUEdOdQ5aka4D+iPj9NP8J4PKIuHFcuw3AhjT7buC5ipu8GPhxxb7t5Lya47ya0615Qffmdjbm9ZsR8Y7xwa6/J/FGRMQWYEur65H0aET0TkFKU8p5Ncd5Nadb84Luze1cyqvbLzcNAwtL8wtSzMzMOqDbi8QjwGJJiyTNAtYCe6Y5JzOzc0ZXX26KiFFJNwL7KIbAbouIQ23cZMuXrNrEeTXHeTWnW/OC7s3tnMmrq29cm5nZ9Or2y01mZjaNXCTMzCzLRSKR1C/pOUlDkjZ1cLsLJT0k6RlJhyTdlOJfkDQs6cn0uKrU5+aU53OSrmxzfkclDaYcHk2xeZL2Szqcfs5NcUm6M+X2lKRlbcrp3aX98qSkVyR9Zjr2maRtkk5KeroUa3r/SBpI7Q9LGmhTXn8s6Qdp29+SNCfFeyS9Vtpvf1bqszz9/odS7mpDXk3/3qb67zWT172lnI5KejLFO7m/cseHzr3GIuKcf1DcFH8eeCcwC/g+sKRD274EWJam3wb8kOIjSL4A/KcG7Zek/M4DFqW8Z7Qxv6PAxeNi/xXYlKY3Abel6auABwEBK4CHO/S7ewn4zenYZ8AHgGXA01X3DzAPOJJ+zk3Tc9uQ1ypgZpq+rZRXT7nduPV8L+WqlPvqNuTV1O+tHX+vjfIat/x24L9Mw/7KHR869hrzmURh2j7+IyKOR8TjafrvgGeB+RN0WQPsioifR8QLwBBF/p20BtiRpncAV5fiO6NwEJgj6ZI257ISeD4ifjRBm7bts4j4LnCqwfaa2T9XAvsj4lREnAb2A/1TnVdEfDsiRtPsQYr3HWWl3C6MiINRHGl2lp7LlOU1gdzvbcr/XifKK50NfBy4Z6J1tGl/5Y4PHXuNuUgU5gMvluaPMfGBui0k9QDvBR5OoRvTKeO2sdNJOp9rAN+W9JiKjz8BqEXE8TT9ElCbptygeO9M+Y+3G/ZZs/tnOvbb71H8xzlmkaQnJP1vSe9Psfkpl07k1czvrdP76/3AiYg4XIp1fH+NOz507DXmItElJF0A/BXwmYh4BbgL+C3gt4HjFKe70+F3ImIZxSfx3iDpA+WF6T+maRlHreINlh8F/jKFumWf/dJ07p8cSZ8HRoG7U+g48E8j4r3AfwT+QtKFHUyp635v41zLP/xHpOP7q8Hx4Zfa/RpzkShM68d/SPoNihfA3RHxTYCIOBERv4iI/wf8Ob+6PNLRXCNiOP08CXwr5XFi7DJS+nlyOnKjKFyPR8SJlGNX7DOa3z8dy0/S9cBHgOvSwYV0OeflNP0YxfX+f5ZyKF+SakteFX5vndxfM4F/C9xbyrej+6vR8YEOvsZcJArT9vEf6XrnVuDZiPiTUrx8Lf/fAGOjLvYAayWdJ2kRsJjiZlk7cjtf0tvGpilufD6dchgbHTEA7C7lti6NsFgBnCmdErfDP/gPrxv2WWl7zeyffcAqSXPTpZZVKTalVHyB12eBj0bEq6X4O1R8dwuS3kmxf46k3F6RtCK9TteVnstU5tXs762Tf68fAn4QEb+8jNTJ/ZU7PtDJ11grd97PpgfFqIAfUvxX8PkObvd3KE4VnwKeTI+rgG8Agym+B7ik1OfzKc/naHH0xCS5vZNi5Mj3gUNj+wV4O3AAOAz8L2BeioviS6KeT7n3tjG384GXgYtKsY7vM4oidRz4e4rrvOur7B+KewRD6fHJNuU1RHFdeux19mep7b9Lv98ngceB3y2tp5fioP088N9Jn9IwxXk1/Xub6r/XRnml+HbgD8a17eT+yh0fOvYa88dymJlZli83mZlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZ1v8HU/k6fiwkw0wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "# text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "# for i in tqdm(range(data.shape[0])):\r\n",
        "    # if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        # text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafd90d3-592e-44a8-d602-30424903033f"
      },
      "source": [
        "# text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "9a443a1c-929d-46b7-a380-16c307c65131"
      },
      "source": [
        "# s0 = text_pairs[10][0]\r\n",
        "# text_list = wrapper.wrap(s0[:300])\r\n",
        "# for line in text_list:\r\n",
        "#     print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLsMmUFFnHA",
        "outputId": "a73bc202-a4ce-4a18-85dd-b8a392a193ac"
      },
      "source": [
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n",
            "в киеве исключили новый раунд минских переговоров\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "6c6da3f7-43f9-45bc-e815-d8b23744dee8"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC79r_7EPy6",
        "outputId": "cd61d5fe-2072-4b6f-93b4-017c41551397"
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4892054-f8ad-4a45-fcb0-bac846690a33"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8b9803-f951-4448-9257-3beb79a6ff17"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0185458f-887e-49f5-c35b-4412cee760ed"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        "    trax.data.FilterByLength(2048)\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030c7afc-c395-422e-c6d0-6b84f8c58e92"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   17   871   508  1595   173 15949     1     0  2872   861 14052  1571\n",
            "  9703    95  1641  2335  5063 10856   194     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "bc8f67df-30d6-4ddf-967c-fb34bfe05b0c"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "786dbbcb-b869-4ac7-dce1-6a114117fd52"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   64, 15022,  1215,  2233,  2691, 15945,   570, 15088,   303,\n",
              "           5,  5900, 14590, 11381,   167,    62, 15933, 15977,  3588,\n",
              "       10273, 12939,    25,  2194,   835,    86,  3156, 15945, 12930,\n",
              "        5579, 15949,    63,   226,  1203,  1530,    81,  3177, 15957,\n",
              "          48,   867,    25,  2253,  4354, 15949,  2523,  2946,  2691,\n",
              "        1951,    43,  1015, 15928,   330,     4,  1118,  2848,   173,\n",
              "       15945,    41,    64,  3583,   173,     5,  7441, 10273,  8973,\n",
              "         707,  9451,   309,   554,  1728,  2691, 15949,  4696, 15933,\n",
              "         657,  6775,   249,  4835,  7205,  3514,  1430,  2691, 15949,\n",
              "           5,  1770, 15861,  7850,    43,  2590, 15945,   616,  9956,\n",
              "       15662,  4696, 15933, 15945,   237, 14784, 10996, 14642, 11412,\n",
              "         189, 15949,  5355,  2619,    46,   322,  5577, 11029,   452,\n",
              "       13422,    70,   183,   521,   572, 15945,   505,  5208,  1403,\n",
              "         173,  3476,   657,  1404,  3475,    70,  6810,   521,   572,\n",
              "       15949, 10273, 14949,    25,  2194,   835,  2691,    86,  4696,\n",
              "       15933,   348,  4280,     4,  1808,  1056,  1409,   173, 15949,\n",
              "        6240,    64,  7324,    81, 15925,  9184,   212, 11381, 15957,\n",
              "          25,  2313,    70,  2277,  1141,  1485,   173,  9633,     4,\n",
              "        2590,   278,  3499,   353,  5376,  1423,  4485,    25,  2537,\n",
              "         835, 11381, 14773, 15945,  1204,   186,  2132,  1754,  4771,\n",
              "        8395,  8451,  6126, 11703, 13111,     5,  1712,   913,  2887,\n",
              "         182,  5450, 15949,     5,  4354,  3966, 15945,    79,  2351,\n",
              "        3047,  3920,  5331, 15945,     5,   538,  1259,  3920,   310,\n",
              "       11255,   255,  8644, 15945,   766,  5130,  6373,  9211, 15949,\n",
              "       10830,  2691, 15960,  5791,   117,   454,  3698,  2590,   278,\n",
              "          57,  3536,  1840, 15949,  1414,     5,   278,  3818,  1977,\n",
              "        1379,  2691, 15945, 14926, 15975,    86,  1035,  2815, 10693,\n",
              "        2307, 14590, 11381, 15949,     5,  1409,   316,  2159,  3981,\n",
              "         678, 15945,  2262,  2832,  7205, 10450,  3317,  2691, 14036,\n",
              "          17,  9184,   810, 11381, 15945,  3612,   741,  2826,  1119,\n",
              "         572, 15949,     1,     0,     5,  5900, 14590, 11381, 15088,\n",
              "         162,  5579,  1430,  2691,     1,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "4ba65397-64ca-47b9-aa43-a02e12c603d8"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.0001)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.0001), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "dd21ad71-436c-4eda-cf07-f9da4f13beb7"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  140100: Ran 100 train steps in 84.32 secs\n",
            "Step  140100: train CrossEntropyLoss |  4.25123358\n",
            "Step  140100: eval  CrossEntropyLoss |  4.36563492\n",
            "Step  140100: eval          Accuracy |  0.33600003\n",
            "\n",
            "Step  140200: Ran 100 train steps in 63.80 secs\n",
            "Step  140200: train CrossEntropyLoss |  4.22494316\n",
            "Step  140200: eval  CrossEntropyLoss |  4.09253597\n",
            "Step  140200: eval          Accuracy |  0.35211268\n",
            "\n",
            "Step  140300: Ran 100 train steps in 48.06 secs\n",
            "Step  140300: train CrossEntropyLoss |  4.22333050\n",
            "Step  140300: eval  CrossEntropyLoss |  4.07544041\n",
            "Step  140300: eval          Accuracy |  0.37500000\n",
            "\n",
            "Step  140400: Ran 100 train steps in 47.01 secs\n",
            "Step  140400: train CrossEntropyLoss |  4.12671995\n",
            "Step  140400: eval  CrossEntropyLoss |  3.60316205\n",
            "Step  140400: eval          Accuracy |  0.41584158\n",
            "\n",
            "Step  140500: Ran 100 train steps in 47.66 secs\n",
            "Step  140500: train CrossEntropyLoss |  4.02641916\n",
            "Step  140500: eval  CrossEntropyLoss |  3.81946254\n",
            "Step  140500: eval          Accuracy |  0.36448598\n",
            "\n",
            "Step  140600: Ran 100 train steps in 47.38 secs\n",
            "Step  140600: train CrossEntropyLoss |  3.95848799\n",
            "Step  140600: eval  CrossEntropyLoss |  4.13105488\n",
            "Step  140600: eval          Accuracy |  0.33913043\n",
            "\n",
            "Step  140700: Ran 100 train steps in 47.65 secs\n",
            "Step  140700: train CrossEntropyLoss |  3.95597148\n",
            "Step  140700: eval  CrossEntropyLoss |  4.15280056\n",
            "Step  140700: eval          Accuracy |  0.37735850\n",
            "\n",
            "Step  140800: Ran 100 train steps in 47.63 secs\n",
            "Step  140800: train CrossEntropyLoss |  3.95108938\n",
            "Step  140800: eval  CrossEntropyLoss |  4.20599842\n",
            "Step  140800: eval          Accuracy |  0.39814815\n",
            "\n",
            "Step  140900: Ran 100 train steps in 47.57 secs\n",
            "Step  140900: train CrossEntropyLoss |  3.82818007\n",
            "Step  140900: eval  CrossEntropyLoss |  3.82122350\n",
            "Step  140900: eval          Accuracy |  0.38679245\n",
            "\n",
            "Step  141000: Ran 100 train steps in 47.51 secs\n",
            "Step  141000: train CrossEntropyLoss |  3.83620358\n",
            "Step  141000: eval  CrossEntropyLoss |  3.56027055\n",
            "Step  141000: eval          Accuracy |  0.42533937\n",
            "\n",
            "Step  141100: Ran 100 train steps in 47.74 secs\n",
            "Step  141100: train CrossEntropyLoss |  3.86734581\n",
            "Step  141100: eval  CrossEntropyLoss |  3.96992183\n",
            "Step  141100: eval          Accuracy |  0.37815127\n",
            "\n",
            "Step  141200: Ran 100 train steps in 47.74 secs\n",
            "Step  141200: train CrossEntropyLoss |  3.84271073\n",
            "Step  141200: eval  CrossEntropyLoss |  4.04554844\n",
            "Step  141200: eval          Accuracy |  0.40869564\n",
            "\n",
            "Step  141300: Ran 100 train steps in 47.99 secs\n",
            "Step  141300: train CrossEntropyLoss |  3.83323622\n",
            "Step  141300: eval  CrossEntropyLoss |  3.65060425\n",
            "Step  141300: eval          Accuracy |  0.38888890\n",
            "\n",
            "Step  141400: Ran 100 train steps in 47.76 secs\n",
            "Step  141400: train CrossEntropyLoss |  3.74318147\n",
            "Step  141400: eval  CrossEntropyLoss |  3.75474024\n",
            "Step  141400: eval          Accuracy |  0.33684212\n",
            "\n",
            "Step  141500: Ran 100 train steps in 47.85 secs\n",
            "Step  141500: train CrossEntropyLoss |  3.83778286\n",
            "Step  141500: eval  CrossEntropyLoss |  4.45541000\n",
            "Step  141500: eval          Accuracy |  0.32631579\n",
            "\n",
            "Step  141600: Ran 100 train steps in 47.82 secs\n",
            "Step  141600: train CrossEntropyLoss |  3.82214355\n",
            "Step  141600: eval  CrossEntropyLoss |  4.15245914\n",
            "Step  141600: eval          Accuracy |  0.36538464\n",
            "\n",
            "Step  141700: Ran 100 train steps in 47.81 secs\n",
            "Step  141700: train CrossEntropyLoss |  3.74874783\n",
            "Step  141700: eval  CrossEntropyLoss |  3.75446510\n",
            "Step  141700: eval          Accuracy |  0.39062500\n",
            "\n",
            "Step  141800: Ran 100 train steps in 47.93 secs\n",
            "Step  141800: train CrossEntropyLoss |  3.73276854\n",
            "Step  141800: eval  CrossEntropyLoss |  3.77151513\n",
            "Step  141800: eval          Accuracy |  0.38461539\n",
            "\n",
            "Step  141900: Ran 100 train steps in 47.64 secs\n",
            "Step  141900: train CrossEntropyLoss |  3.73192859\n",
            "Step  141900: eval  CrossEntropyLoss |  4.45410156\n",
            "Step  141900: eval          Accuracy |  0.34883720\n",
            "\n",
            "Step  142000: Ran 100 train steps in 47.80 secs\n",
            "Step  142000: train CrossEntropyLoss |  3.76630187\n",
            "Step  142000: eval  CrossEntropyLoss |  4.25670099\n",
            "Step  142000: eval          Accuracy |  0.36046511\n",
            "\n",
            "Step  142100: Ran 100 train steps in 48.04 secs\n",
            "Step  142100: train CrossEntropyLoss |  3.75430012\n",
            "Step  142100: eval  CrossEntropyLoss |  3.19686532\n",
            "Step  142100: eval          Accuracy |  0.49462366\n",
            "\n",
            "Step  142200: Ran 100 train steps in 47.91 secs\n",
            "Step  142200: train CrossEntropyLoss |  3.75382400\n",
            "Step  142200: eval  CrossEntropyLoss |  4.32477379\n",
            "Step  142200: eval          Accuracy |  0.31313130\n",
            "\n",
            "Step  142300: Ran 100 train steps in 48.12 secs\n",
            "Step  142300: train CrossEntropyLoss |  3.71019411\n",
            "Step  142300: eval  CrossEntropyLoss |  4.34796810\n",
            "Step  142300: eval          Accuracy |  0.28947368\n",
            "\n",
            "Step  142400: Ran 100 train steps in 47.70 secs\n",
            "Step  142400: train CrossEntropyLoss |  3.74596977\n",
            "Step  142400: eval  CrossEntropyLoss |  3.59460258\n",
            "Step  142400: eval          Accuracy |  0.34444445\n",
            "\n",
            "Step  142500: Ran 100 train steps in 47.81 secs\n",
            "Step  142500: train CrossEntropyLoss |  3.65455031\n",
            "Step  142500: eval  CrossEntropyLoss |  3.51488614\n",
            "Step  142500: eval          Accuracy |  0.41818181\n",
            "\n",
            "Step  142600: Ran 100 train steps in 47.66 secs\n",
            "Step  142600: train CrossEntropyLoss |  3.70562005\n",
            "Step  142600: eval  CrossEntropyLoss |  3.94904280\n",
            "Step  142600: eval          Accuracy |  0.32420093\n",
            "\n",
            "Step  142700: Ran 100 train steps in 47.88 secs\n",
            "Step  142700: train CrossEntropyLoss |  3.64806151\n",
            "Step  142700: eval  CrossEntropyLoss |  4.60651541\n",
            "Step  142700: eval          Accuracy |  0.29999998\n",
            "\n",
            "Step  142800: Ran 100 train steps in 47.95 secs\n",
            "Step  142800: train CrossEntropyLoss |  3.72971702\n",
            "Step  142800: eval  CrossEntropyLoss |  4.02848625\n",
            "Step  142800: eval          Accuracy |  0.36842105\n",
            "\n",
            "Step  142900: Ran 100 train steps in 47.69 secs\n",
            "Step  142900: train CrossEntropyLoss |  3.68468738\n",
            "Step  142900: eval  CrossEntropyLoss |  3.84254885\n",
            "Step  142900: eval          Accuracy |  0.40611354\n",
            "\n",
            "Step  143000: Ran 100 train steps in 47.76 secs\n",
            "Step  143000: train CrossEntropyLoss |  3.66094971\n",
            "Step  143000: eval  CrossEntropyLoss |  3.41923380\n",
            "Step  143000: eval          Accuracy |  0.47252747\n",
            "\n",
            "Step  143100: Ran 100 train steps in 47.79 secs\n",
            "Step  143100: train CrossEntropyLoss |  3.75712609\n",
            "Step  143100: eval  CrossEntropyLoss |  3.53830600\n",
            "Step  143100: eval          Accuracy |  0.42735046\n",
            "\n",
            "Step  143200: Ran 100 train steps in 47.93 secs\n",
            "Step  143200: train CrossEntropyLoss |  3.64995790\n",
            "Step  143200: eval  CrossEntropyLoss |  3.44625306\n",
            "Step  143200: eval          Accuracy |  0.45562130\n",
            "\n",
            "Step  143300: Ran 100 train steps in 48.25 secs\n",
            "Step  143300: train CrossEntropyLoss |  3.62338042\n",
            "Step  143300: eval  CrossEntropyLoss |  2.85986209\n",
            "Step  143300: eval          Accuracy |  0.52293575\n",
            "\n",
            "Step  143400: Ran 100 train steps in 47.83 secs\n",
            "Step  143400: train CrossEntropyLoss |  3.64596581\n",
            "Step  143400: eval  CrossEntropyLoss |  4.46547651\n",
            "Step  143400: eval          Accuracy |  0.33333334\n",
            "\n",
            "Step  143500: Ran 100 train steps in 47.97 secs\n",
            "Step  143500: train CrossEntropyLoss |  3.59593916\n",
            "Step  143500: eval  CrossEntropyLoss |  2.88488388\n",
            "Step  143500: eval          Accuracy |  0.50515467\n",
            "\n",
            "Step  143600: Ran 100 train steps in 47.86 secs\n",
            "Step  143600: train CrossEntropyLoss |  3.59909439\n",
            "Step  143600: eval  CrossEntropyLoss |  3.91752887\n",
            "Step  143600: eval          Accuracy |  0.36771303\n",
            "\n",
            "Step  143700: Ran 100 train steps in 47.83 secs\n",
            "Step  143700: train CrossEntropyLoss |  3.65512514\n",
            "Step  143700: eval  CrossEntropyLoss |  4.17067337\n",
            "Step  143700: eval          Accuracy |  0.32773110\n",
            "\n",
            "Step  143800: Ran 100 train steps in 48.02 secs\n",
            "Step  143800: train CrossEntropyLoss |  3.61438370\n",
            "Step  143800: eval  CrossEntropyLoss |  2.91364050\n",
            "Step  143800: eval          Accuracy |  0.46078432\n",
            "\n",
            "Step  143900: Ran 100 train steps in 47.75 secs\n",
            "Step  143900: train CrossEntropyLoss |  3.60169554\n",
            "Step  143900: eval  CrossEntropyLoss |  3.68808293\n",
            "Step  143900: eval          Accuracy |  0.36702126\n",
            "\n",
            "Step  144000: Ran 100 train steps in 47.99 secs\n",
            "Step  144000: train CrossEntropyLoss |  3.63581681\n",
            "Step  144000: eval  CrossEntropyLoss |  3.42107248\n",
            "Step  144000: eval          Accuracy |  0.42105263\n",
            "\n",
            "Step  144100: Ran 100 train steps in 47.93 secs\n",
            "Step  144100: train CrossEntropyLoss |  3.60650516\n",
            "Step  144100: eval  CrossEntropyLoss |  4.33375597\n",
            "Step  144100: eval          Accuracy |  0.35454544\n",
            "\n",
            "Step  144200: Ran 100 train steps in 47.80 secs\n",
            "Step  144200: train CrossEntropyLoss |  3.59386921\n",
            "Step  144200: eval  CrossEntropyLoss |  3.65298343\n",
            "Step  144200: eval          Accuracy |  0.42613637\n",
            "\n",
            "Step  144300: Ran 100 train steps in 47.87 secs\n",
            "Step  144300: train CrossEntropyLoss |  3.63951349\n",
            "Step  144300: eval  CrossEntropyLoss |  4.21680450\n",
            "Step  144300: eval          Accuracy |  0.36448598\n",
            "\n",
            "Step  144400: Ran 100 train steps in 47.69 secs\n",
            "Step  144400: train CrossEntropyLoss |  3.61866903\n",
            "Step  144400: eval  CrossEntropyLoss |  3.44242644\n",
            "Step  144400: eval          Accuracy |  0.42708334\n",
            "\n",
            "Step  144500: Ran 100 train steps in 48.00 secs\n",
            "Step  144500: train CrossEntropyLoss |  3.60937619\n",
            "Step  144500: eval  CrossEntropyLoss |  3.59277248\n",
            "Step  144500: eval          Accuracy |  0.43975905\n",
            "\n",
            "Step  144600: Ran 100 train steps in 47.79 secs\n",
            "Step  144600: train CrossEntropyLoss |  3.61462021\n",
            "Step  144600: eval  CrossEntropyLoss |  3.37332463\n",
            "Step  144600: eval          Accuracy |  0.38947371\n",
            "\n",
            "Step  144700: Ran 100 train steps in 47.92 secs\n",
            "Step  144700: train CrossEntropyLoss |  3.61484885\n",
            "Step  144700: eval  CrossEntropyLoss |  2.96847868\n",
            "Step  144700: eval          Accuracy |  0.49107146\n",
            "\n",
            "Step  144800: Ran 100 train steps in 47.80 secs\n",
            "Step  144800: train CrossEntropyLoss |  3.57362914\n",
            "Step  144800: eval  CrossEntropyLoss |  4.13680124\n",
            "Step  144800: eval          Accuracy |  0.33644858\n",
            "\n",
            "Step  144900: Ran 100 train steps in 47.91 secs\n",
            "Step  144900: train CrossEntropyLoss |  3.57669830\n",
            "Step  144900: eval  CrossEntropyLoss |  4.57111502\n",
            "Step  144900: eval          Accuracy |  0.33663365\n",
            "\n",
            "Step  145000: Ran 100 train steps in 47.86 secs\n",
            "Step  145000: train CrossEntropyLoss |  3.53873467\n",
            "Step  145000: eval  CrossEntropyLoss |  3.86389923\n",
            "Step  145000: eval          Accuracy |  0.38888890\n",
            "\n",
            "Step  145100: Ran 100 train steps in 47.77 secs\n",
            "Step  145100: train CrossEntropyLoss |  3.57872796\n",
            "Step  145100: eval  CrossEntropyLoss |  3.58006501\n",
            "Step  145100: eval          Accuracy |  0.41025642\n",
            "\n",
            "Step  145200: Ran 100 train steps in 47.85 secs\n",
            "Step  145200: train CrossEntropyLoss |  3.58591700\n",
            "Step  145200: eval  CrossEntropyLoss |  4.33132076\n",
            "Step  145200: eval          Accuracy |  0.34615386\n",
            "\n",
            "Step  145300: Ran 100 train steps in 47.90 secs\n",
            "Step  145300: train CrossEntropyLoss |  3.53168726\n",
            "Step  145300: eval  CrossEntropyLoss |  3.33835006\n",
            "Step  145300: eval          Accuracy |  0.40740740\n",
            "\n",
            "Step  145400: Ran 100 train steps in 47.83 secs\n",
            "Step  145400: train CrossEntropyLoss |  3.58081150\n",
            "Step  145400: eval  CrossEntropyLoss |  3.18227315\n",
            "Step  145400: eval          Accuracy |  0.49532709\n",
            "\n",
            "Step  145500: Ran 100 train steps in 47.94 secs\n",
            "Step  145500: train CrossEntropyLoss |  3.55144739\n",
            "Step  145500: eval  CrossEntropyLoss |  3.42816234\n",
            "Step  145500: eval          Accuracy |  0.42391306\n",
            "\n",
            "Step  145600: Ran 100 train steps in 47.90 secs\n",
            "Step  145600: train CrossEntropyLoss |  3.53679252\n",
            "Step  145600: eval  CrossEntropyLoss |  4.14520216\n",
            "Step  145600: eval          Accuracy |  0.39603961\n",
            "\n",
            "Step  145700: Ran 100 train steps in 47.83 secs\n",
            "Step  145700: train CrossEntropyLoss |  3.53927112\n",
            "Step  145700: eval  CrossEntropyLoss |  3.60466552\n",
            "Step  145700: eval          Accuracy |  0.42990652\n",
            "\n",
            "Step  145800: Ran 100 train steps in 48.19 secs\n",
            "Step  145800: train CrossEntropyLoss |  3.55695987\n",
            "Step  145800: eval  CrossEntropyLoss |  3.75660443\n",
            "Step  145800: eval          Accuracy |  0.39823008\n",
            "\n",
            "Step  145900: Ran 100 train steps in 47.73 secs\n",
            "Step  145900: train CrossEntropyLoss |  3.56167555\n",
            "Step  145900: eval  CrossEntropyLoss |  3.86022353\n",
            "Step  145900: eval          Accuracy |  0.38918918\n",
            "\n",
            "Step  146000: Ran 100 train steps in 47.98 secs\n",
            "Step  146000: train CrossEntropyLoss |  3.47029924\n",
            "Step  146000: eval  CrossEntropyLoss |  3.96805048\n",
            "Step  146000: eval          Accuracy |  0.35454544\n",
            "\n",
            "Step  146100: Ran 100 train steps in 47.76 secs\n",
            "Step  146100: train CrossEntropyLoss |  3.51499438\n",
            "Step  146100: eval  CrossEntropyLoss |  3.65658021\n",
            "Step  146100: eval          Accuracy |  0.38532108\n",
            "\n",
            "Step  146200: Ran 100 train steps in 47.93 secs\n",
            "Step  146200: train CrossEntropyLoss |  3.53044415\n",
            "Step  146200: eval  CrossEntropyLoss |  3.36858177\n",
            "Step  146200: eval          Accuracy |  0.41145834\n",
            "\n",
            "Step  146300: Ran 100 train steps in 47.95 secs\n",
            "Step  146300: train CrossEntropyLoss |  3.56094646\n",
            "Step  146300: eval  CrossEntropyLoss |  3.51285362\n",
            "Step  146300: eval          Accuracy |  0.45744678\n",
            "\n",
            "Step  146400: Ran 100 train steps in 47.97 secs\n",
            "Step  146400: train CrossEntropyLoss |  3.53225851\n",
            "Step  146400: eval  CrossEntropyLoss |  2.91068935\n",
            "Step  146400: eval          Accuracy |  0.44897959\n",
            "\n",
            "Step  146500: Ran 100 train steps in 48.11 secs\n",
            "Step  146500: train CrossEntropyLoss |  3.49035811\n",
            "Step  146500: eval  CrossEntropyLoss |  3.62355113\n",
            "Step  146500: eval          Accuracy |  0.45283020\n",
            "\n",
            "Step  146600: Ran 100 train steps in 47.99 secs\n",
            "Step  146600: train CrossEntropyLoss |  3.54641962\n",
            "Step  146600: eval  CrossEntropyLoss |  3.67180634\n",
            "Step  146600: eval          Accuracy |  0.39195979\n",
            "\n",
            "Step  146700: Ran 100 train steps in 48.02 secs\n",
            "Step  146700: train CrossEntropyLoss |  3.50497651\n",
            "Step  146700: eval  CrossEntropyLoss |  4.43782187\n",
            "Step  146700: eval          Accuracy |  0.32323232\n",
            "\n",
            "Step  146800: Ran 100 train steps in 47.92 secs\n",
            "Step  146800: train CrossEntropyLoss |  3.54098535\n",
            "Step  146800: eval  CrossEntropyLoss |  3.42410636\n",
            "Step  146800: eval          Accuracy |  0.46875000\n",
            "\n",
            "Step  146900: Ran 100 train steps in 47.93 secs\n",
            "Step  146900: train CrossEntropyLoss |  3.46725154\n",
            "Step  146900: eval  CrossEntropyLoss |  3.36077976\n",
            "Step  146900: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  147000: Ran 100 train steps in 47.96 secs\n",
            "Step  147000: train CrossEntropyLoss |  3.53223801\n",
            "Step  147000: eval  CrossEntropyLoss |  3.93990946\n",
            "Step  147000: eval          Accuracy |  0.41379309\n",
            "\n",
            "Step  147100: Ran 100 train steps in 47.90 secs\n",
            "Step  147100: train CrossEntropyLoss |  3.48988247\n",
            "Step  147100: eval  CrossEntropyLoss |  3.53328562\n",
            "Step  147100: eval          Accuracy |  0.43518519\n",
            "\n",
            "Step  147200: Ran 100 train steps in 48.09 secs\n",
            "Step  147200: train CrossEntropyLoss |  3.59813046\n",
            "Step  147200: eval  CrossEntropyLoss |  3.50471759\n",
            "Step  147200: eval          Accuracy |  0.47321430\n",
            "\n",
            "Step  147300: Ran 100 train steps in 47.94 secs\n",
            "Step  147300: train CrossEntropyLoss |  3.50429773\n",
            "Step  147300: eval  CrossEntropyLoss |  4.49727201\n",
            "Step  147300: eval          Accuracy |  0.33928573\n",
            "\n",
            "Step  147400: Ran 100 train steps in 47.76 secs\n",
            "Step  147400: train CrossEntropyLoss |  3.48687434\n",
            "Step  147400: eval  CrossEntropyLoss |  3.41637897\n",
            "Step  147400: eval          Accuracy |  0.47422683\n",
            "\n",
            "Step  147500: Ran 100 train steps in 47.88 secs\n",
            "Step  147500: train CrossEntropyLoss |  3.42462540\n",
            "Step  147500: eval  CrossEntropyLoss |  3.06895852\n",
            "Step  147500: eval          Accuracy |  0.48760328\n",
            "\n",
            "Step  147600: Ran 100 train steps in 47.92 secs\n",
            "Step  147600: train CrossEntropyLoss |  3.42879701\n",
            "Step  147600: eval  CrossEntropyLoss |  3.65964174\n",
            "Step  147600: eval          Accuracy |  0.43103448\n",
            "\n",
            "Step  147700: Ran 100 train steps in 47.95 secs\n",
            "Step  147700: train CrossEntropyLoss |  3.46643662\n",
            "Step  147700: eval  CrossEntropyLoss |  3.75204563\n",
            "Step  147700: eval          Accuracy |  0.37931034\n",
            "\n",
            "Step  147800: Ran 100 train steps in 47.86 secs\n",
            "Step  147800: train CrossEntropyLoss |  3.51799178\n",
            "Step  147800: eval  CrossEntropyLoss |  3.31117940\n",
            "Step  147800: eval          Accuracy |  0.45454544\n",
            "\n",
            "Step  147900: Ran 100 train steps in 47.98 secs\n",
            "Step  147900: train CrossEntropyLoss |  3.44110918\n",
            "Step  147900: eval  CrossEntropyLoss |  3.14205980\n",
            "Step  147900: eval          Accuracy |  0.43529412\n",
            "\n",
            "Step  148000: Ran 100 train steps in 47.92 secs\n",
            "Step  148000: train CrossEntropyLoss |  3.48351312\n",
            "Step  148000: eval  CrossEntropyLoss |  3.60435462\n",
            "Step  148000: eval          Accuracy |  0.41237116\n",
            "\n",
            "Step  148100: Ran 100 train steps in 47.90 secs\n",
            "Step  148100: train CrossEntropyLoss |  3.40981579\n",
            "Step  148100: eval  CrossEntropyLoss |  3.17735624\n",
            "Step  148100: eval          Accuracy |  0.45054945\n",
            "\n",
            "Step  148200: Ran 100 train steps in 47.96 secs\n",
            "Step  148200: train CrossEntropyLoss |  3.50055408\n",
            "Step  148200: eval  CrossEntropyLoss |  3.60299969\n",
            "Step  148200: eval          Accuracy |  0.42512077\n",
            "\n",
            "Step  148300: Ran 100 train steps in 48.02 secs\n",
            "Step  148300: train CrossEntropyLoss |  3.47461510\n",
            "Step  148300: eval  CrossEntropyLoss |  3.48803568\n",
            "Step  148300: eval          Accuracy |  0.47787610\n",
            "\n",
            "Step  148400: Ran 100 train steps in 48.02 secs\n",
            "Step  148400: train CrossEntropyLoss |  3.50281787\n",
            "Step  148400: eval  CrossEntropyLoss |  3.32768655\n",
            "Step  148400: eval          Accuracy |  0.42857146\n",
            "\n",
            "Step  148500: Ran 100 train steps in 47.78 secs\n",
            "Step  148500: train CrossEntropyLoss |  3.45795989\n",
            "Step  148500: eval  CrossEntropyLoss |  3.18460631\n",
            "Step  148500: eval          Accuracy |  0.47926268\n",
            "\n",
            "Step  148600: Ran 100 train steps in 47.85 secs\n",
            "Step  148600: train CrossEntropyLoss |  3.49634671\n",
            "Step  148600: eval  CrossEntropyLoss |  3.19579983\n",
            "Step  148600: eval          Accuracy |  0.43877551\n",
            "\n",
            "Step  148700: Ran 100 train steps in 47.93 secs\n",
            "Step  148700: train CrossEntropyLoss |  3.46779418\n",
            "Step  148700: eval  CrossEntropyLoss |  4.04833937\n",
            "Step  148700: eval          Accuracy |  0.32673267\n",
            "\n",
            "Step  148800: Ran 100 train steps in 47.97 secs\n",
            "Step  148800: train CrossEntropyLoss |  3.41615319\n",
            "Step  148800: eval  CrossEntropyLoss |  3.40348482\n",
            "Step  148800: eval          Accuracy |  0.40760872\n",
            "\n",
            "Step  148900: Ran 100 train steps in 47.89 secs\n",
            "Step  148900: train CrossEntropyLoss |  3.41720104\n",
            "Step  148900: eval  CrossEntropyLoss |  3.79181314\n",
            "Step  148900: eval          Accuracy |  0.36607143\n",
            "\n",
            "Step  149000: Ran 100 train steps in 47.98 secs\n",
            "Step  149000: train CrossEntropyLoss |  3.45052075\n",
            "Step  149000: eval  CrossEntropyLoss |  3.13968992\n",
            "Step  149000: eval          Accuracy |  0.45454547\n",
            "\n",
            "Step  149100: Ran 100 train steps in 47.86 secs\n",
            "Step  149100: train CrossEntropyLoss |  3.47695804\n",
            "Step  149100: eval  CrossEntropyLoss |  4.07488918\n",
            "Step  149100: eval          Accuracy |  0.38888893\n",
            "\n",
            "Step  149200: Ran 100 train steps in 48.18 secs\n",
            "Step  149200: train CrossEntropyLoss |  3.52542591\n",
            "Step  149200: eval  CrossEntropyLoss |  3.37082314\n",
            "Step  149200: eval          Accuracy |  0.42735046\n",
            "\n",
            "Step  149300: Ran 100 train steps in 47.91 secs\n",
            "Step  149300: train CrossEntropyLoss |  3.46262288\n",
            "Step  149300: eval  CrossEntropyLoss |  4.11599398\n",
            "Step  149300: eval          Accuracy |  0.32989693\n",
            "\n",
            "Step  149400: Ran 100 train steps in 47.91 secs\n",
            "Step  149400: train CrossEntropyLoss |  3.45805311\n",
            "Step  149400: eval  CrossEntropyLoss |  3.51855469\n",
            "Step  149400: eval          Accuracy |  0.41081083\n",
            "\n",
            "Step  149500: Ran 100 train steps in 48.08 secs\n",
            "Step  149500: train CrossEntropyLoss |  3.50201321\n",
            "Step  149500: eval  CrossEntropyLoss |  2.90696907\n",
            "Step  149500: eval          Accuracy |  0.45081970\n",
            "\n",
            "Step  149600: Ran 100 train steps in 47.85 secs\n",
            "Step  149600: train CrossEntropyLoss |  3.41468859\n",
            "Step  149600: eval  CrossEntropyLoss |  4.08672047\n",
            "Step  149600: eval          Accuracy |  0.28971961\n",
            "\n",
            "Step  149700: Ran 100 train steps in 48.07 secs\n",
            "Step  149700: train CrossEntropyLoss |  3.40779281\n",
            "Step  149700: eval  CrossEntropyLoss |  3.58414078\n",
            "Step  149700: eval          Accuracy |  0.41847828\n",
            "\n",
            "Step  149800: Ran 100 train steps in 47.82 secs\n",
            "Step  149800: train CrossEntropyLoss |  3.39431238\n",
            "Step  149800: eval  CrossEntropyLoss |  3.86190701\n",
            "Step  149800: eval          Accuracy |  0.39669418\n",
            "\n",
            "Step  149900: Ran 100 train steps in 47.97 secs\n",
            "Step  149900: train CrossEntropyLoss |  3.45030499\n",
            "Step  149900: eval  CrossEntropyLoss |  3.82771420\n",
            "Step  149900: eval          Accuracy |  0.35789475\n",
            "\n",
            "Step  150000: Ran 100 train steps in 47.81 secs\n",
            "Step  150000: train CrossEntropyLoss |  3.40194988\n",
            "Step  150000: eval  CrossEntropyLoss |  3.62179947\n",
            "Step  150000: eval          Accuracy |  0.34375000\n",
            "\n",
            "Step  150100: Ran 100 train steps in 47.90 secs\n",
            "Step  150100: train CrossEntropyLoss |  3.40357327\n",
            "Step  150100: eval  CrossEntropyLoss |  3.57285929\n",
            "Step  150100: eval          Accuracy |  0.38317755\n",
            "\n",
            "Step  150200: Ran 100 train steps in 47.92 secs\n",
            "Step  150200: train CrossEntropyLoss |  3.40178466\n",
            "Step  150200: eval  CrossEntropyLoss |  3.30782509\n",
            "Step  150200: eval          Accuracy |  0.43902439\n",
            "\n",
            "Step  150300: Ran 100 train steps in 48.18 secs\n",
            "Step  150300: train CrossEntropyLoss |  3.44400382\n",
            "Step  150300: eval  CrossEntropyLoss |  2.72325563\n",
            "Step  150300: eval          Accuracy |  0.46938774\n",
            "\n",
            "Step  150400: Ran 100 train steps in 48.08 secs\n",
            "Step  150400: train CrossEntropyLoss |  3.44003534\n",
            "Step  150400: eval  CrossEntropyLoss |  3.47733688\n",
            "Step  150400: eval          Accuracy |  0.38613862\n",
            "\n",
            "Step  150500: Ran 100 train steps in 48.15 secs\n",
            "Step  150500: train CrossEntropyLoss |  3.38746524\n",
            "Step  150500: eval  CrossEntropyLoss |  3.55898547\n",
            "Step  150500: eval          Accuracy |  0.42857143\n",
            "\n",
            "Step  150600: Ran 100 train steps in 48.02 secs\n",
            "Step  150600: train CrossEntropyLoss |  3.48674464\n",
            "Step  150600: eval  CrossEntropyLoss |  3.88478827\n",
            "Step  150600: eval          Accuracy |  0.40677965\n",
            "\n",
            "Step  150700: Ran 100 train steps in 48.04 secs\n",
            "Step  150700: train CrossEntropyLoss |  3.38094258\n",
            "Step  150700: eval  CrossEntropyLoss |  4.14429712\n",
            "Step  150700: eval          Accuracy |  0.38271606\n",
            "\n",
            "Step  150800: Ran 100 train steps in 48.47 secs\n",
            "Step  150800: train CrossEntropyLoss |  3.47589207\n",
            "Step  150800: eval  CrossEntropyLoss |  3.05050373\n",
            "Step  150800: eval          Accuracy |  0.50490201\n",
            "\n",
            "Step  150900: Ran 100 train steps in 48.09 secs\n",
            "Step  150900: train CrossEntropyLoss |  3.45516539\n",
            "Step  150900: eval  CrossEntropyLoss |  3.21943712\n",
            "Step  150900: eval          Accuracy |  0.43956044\n",
            "\n",
            "Step  151000: Ran 100 train steps in 47.89 secs\n",
            "Step  151000: train CrossEntropyLoss |  3.40760326\n",
            "Step  151000: eval  CrossEntropyLoss |  3.48816419\n",
            "Step  151000: eval          Accuracy |  0.44660196\n",
            "\n",
            "Step  151100: Ran 100 train steps in 47.97 secs\n",
            "Step  151100: train CrossEntropyLoss |  3.40690565\n",
            "Step  151100: eval  CrossEntropyLoss |  3.60008597\n",
            "Step  151100: eval          Accuracy |  0.40314138\n",
            "\n",
            "Step  151200: Ran 100 train steps in 48.06 secs\n",
            "Step  151200: train CrossEntropyLoss |  3.41779661\n",
            "Step  151200: eval  CrossEntropyLoss |  3.81892228\n",
            "Step  151200: eval          Accuracy |  0.39047620\n",
            "\n",
            "Step  151300: Ran 100 train steps in 47.93 secs\n",
            "Step  151300: train CrossEntropyLoss |  3.44628167\n",
            "Step  151300: eval  CrossEntropyLoss |  3.48090601\n",
            "Step  151300: eval          Accuracy |  0.43269232\n",
            "\n",
            "Step  151400: Ran 100 train steps in 48.06 secs\n",
            "Step  151400: train CrossEntropyLoss |  3.42248154\n",
            "Step  151400: eval  CrossEntropyLoss |  3.24568081\n",
            "Step  151400: eval          Accuracy |  0.45081970\n",
            "\n",
            "Step  151500: Ran 100 train steps in 47.83 secs\n",
            "Step  151500: train CrossEntropyLoss |  3.34832478\n",
            "Step  151500: eval  CrossEntropyLoss |  3.53626990\n",
            "Step  151500: eval          Accuracy |  0.43888891\n",
            "\n",
            "Step  151600: Ran 100 train steps in 48.16 secs\n",
            "Step  151600: train CrossEntropyLoss |  3.42199397\n",
            "Step  151600: eval  CrossEntropyLoss |  3.03799152\n",
            "Step  151600: eval          Accuracy |  0.53684211\n",
            "\n",
            "Step  151700: Ran 100 train steps in 48.07 secs\n",
            "Step  151700: train CrossEntropyLoss |  3.38084698\n",
            "Step  151700: eval  CrossEntropyLoss |  3.32813907\n",
            "Step  151700: eval          Accuracy |  0.42148760\n",
            "\n",
            "Step  151800: Ran 100 train steps in 47.90 secs\n",
            "Step  151800: train CrossEntropyLoss |  3.42757416\n",
            "Step  151800: eval  CrossEntropyLoss |  3.09636760\n",
            "Step  151800: eval          Accuracy |  0.46039602\n",
            "\n",
            "Step  151900: Ran 100 train steps in 48.18 secs\n",
            "Step  151900: train CrossEntropyLoss |  3.44866776\n",
            "Step  151900: eval  CrossEntropyLoss |  3.38633823\n",
            "Step  151900: eval          Accuracy |  0.41176471\n",
            "\n",
            "Step  152000: Ran 100 train steps in 48.03 secs\n",
            "Step  152000: train CrossEntropyLoss |  3.32705951\n",
            "Step  152000: eval  CrossEntropyLoss |  3.69544435\n",
            "Step  152000: eval          Accuracy |  0.44859812\n",
            "\n",
            "Step  152100: Ran 100 train steps in 48.10 secs\n",
            "Step  152100: train CrossEntropyLoss |  3.44657397\n",
            "Step  152100: eval  CrossEntropyLoss |  3.67566609\n",
            "Step  152100: eval          Accuracy |  0.35964912\n",
            "\n",
            "Step  152200: Ran 100 train steps in 48.22 secs\n",
            "Step  152200: train CrossEntropyLoss |  3.43322325\n",
            "Step  152200: eval  CrossEntropyLoss |  2.91198730\n",
            "Step  152200: eval          Accuracy |  0.47474748\n",
            "\n",
            "Step  152300: Ran 100 train steps in 48.10 secs\n",
            "Step  152300: train CrossEntropyLoss |  3.36092830\n",
            "Step  152300: eval  CrossEntropyLoss |  4.03234053\n",
            "Step  152300: eval          Accuracy |  0.31192660\n",
            "\n",
            "Step  152400: Ran 100 train steps in 48.22 secs\n",
            "Step  152400: train CrossEntropyLoss |  3.44935775\n",
            "Step  152400: eval  CrossEntropyLoss |  3.87130880\n",
            "Step  152400: eval          Accuracy |  0.41880345\n",
            "\n",
            "Step  152500: Ran 100 train steps in 48.10 secs\n",
            "Step  152500: train CrossEntropyLoss |  3.34184194\n",
            "Step  152500: eval  CrossEntropyLoss |  2.99662399\n",
            "Step  152500: eval          Accuracy |  0.54506439\n",
            "\n",
            "Step  152600: Ran 100 train steps in 47.99 secs\n",
            "Step  152600: train CrossEntropyLoss |  3.36699557\n",
            "Step  152600: eval  CrossEntropyLoss |  3.31960773\n",
            "Step  152600: eval          Accuracy |  0.42857146\n",
            "\n",
            "Step  152700: Ran 100 train steps in 47.97 secs\n",
            "Step  152700: train CrossEntropyLoss |  3.38923740\n",
            "Step  152700: eval  CrossEntropyLoss |  3.48943615\n",
            "Step  152700: eval          Accuracy |  0.42537314\n",
            "\n",
            "Step  152800: Ran 100 train steps in 47.97 secs\n",
            "Step  152800: train CrossEntropyLoss |  3.38251519\n",
            "Step  152800: eval  CrossEntropyLoss |  3.57370496\n",
            "Step  152800: eval          Accuracy |  0.40686277\n",
            "\n",
            "Step  152900: Ran 100 train steps in 47.96 secs\n",
            "Step  152900: train CrossEntropyLoss |  3.37772107\n",
            "Step  152900: eval  CrossEntropyLoss |  3.48305726\n",
            "Step  152900: eval          Accuracy |  0.41666669\n",
            "\n",
            "Step  153000: Ran 100 train steps in 47.91 secs\n",
            "Step  153000: train CrossEntropyLoss |  3.42363477\n",
            "Step  153000: eval  CrossEntropyLoss |  3.50997162\n",
            "Step  153000: eval          Accuracy |  0.43697482\n",
            "\n",
            "Step  153100: Ran 100 train steps in 48.03 secs\n",
            "Step  153100: train CrossEntropyLoss |  3.39315629\n",
            "Step  153100: eval  CrossEntropyLoss |  3.87942553\n",
            "Step  153100: eval          Accuracy |  0.43925232\n",
            "\n",
            "Step  153200: Ran 100 train steps in 47.92 secs\n",
            "Step  153200: train CrossEntropyLoss |  3.31058598\n",
            "Step  153200: eval  CrossEntropyLoss |  3.27212262\n",
            "Step  153200: eval          Accuracy |  0.47663549\n",
            "\n",
            "Step  153300: Ran 100 train steps in 47.92 secs\n",
            "Step  153300: train CrossEntropyLoss |  3.38182569\n",
            "Step  153300: eval  CrossEntropyLoss |  3.50893068\n",
            "Step  153300: eval          Accuracy |  0.38834953\n",
            "\n",
            "Step  153400: Ran 100 train steps in 48.09 secs\n",
            "Step  153400: train CrossEntropyLoss |  3.36871552\n",
            "Step  153400: eval  CrossEntropyLoss |  3.86745024\n",
            "Step  153400: eval          Accuracy |  0.35245904\n",
            "\n",
            "Step  153500: Ran 100 train steps in 47.96 secs\n",
            "Step  153500: train CrossEntropyLoss |  3.40897083\n",
            "Step  153500: eval  CrossEntropyLoss |  3.17072749\n",
            "Step  153500: eval          Accuracy |  0.42288557\n",
            "\n",
            "Step  153600: Ran 100 train steps in 47.89 secs\n",
            "Step  153600: train CrossEntropyLoss |  3.31006336\n",
            "Step  153600: eval  CrossEntropyLoss |  2.82075763\n",
            "Step  153600: eval          Accuracy |  0.55833334\n",
            "\n",
            "Step  153700: Ran 100 train steps in 48.30 secs\n",
            "Step  153700: train CrossEntropyLoss |  3.40722752\n",
            "Step  153700: eval  CrossEntropyLoss |  3.34956551\n",
            "Step  153700: eval          Accuracy |  0.42400002\n",
            "\n",
            "Step  153800: Ran 100 train steps in 47.91 secs\n",
            "Step  153800: train CrossEntropyLoss |  3.43016434\n",
            "Step  153800: eval  CrossEntropyLoss |  3.28526449\n",
            "Step  153800: eval          Accuracy |  0.48245615\n",
            "\n",
            "Step  153900: Ran 100 train steps in 48.28 secs\n",
            "Step  153900: train CrossEntropyLoss |  3.37152696\n",
            "Step  153900: eval  CrossEntropyLoss |  3.22224212\n",
            "Step  153900: eval          Accuracy |  0.44334975\n",
            "\n",
            "Step  154000: Ran 100 train steps in 47.96 secs\n",
            "Step  154000: train CrossEntropyLoss |  3.38169742\n",
            "Step  154000: eval  CrossEntropyLoss |  3.57816195\n",
            "Step  154000: eval          Accuracy |  0.39639640\n",
            "\n",
            "Step  154100: Ran 100 train steps in 48.07 secs\n",
            "Step  154100: train CrossEntropyLoss |  3.33609867\n",
            "Step  154100: eval  CrossEntropyLoss |  3.87742090\n",
            "Step  154100: eval          Accuracy |  0.38392860\n",
            "\n",
            "Step  154200: Ran 100 train steps in 47.96 secs\n",
            "Step  154200: train CrossEntropyLoss |  3.34246325\n",
            "Step  154200: eval  CrossEntropyLoss |  3.16731596\n",
            "Step  154200: eval          Accuracy |  0.46031749\n",
            "\n",
            "Step  154300: Ran 100 train steps in 47.90 secs\n",
            "Step  154300: train CrossEntropyLoss |  3.25759411\n",
            "Step  154300: eval  CrossEntropyLoss |  3.27171564\n",
            "Step  154300: eval          Accuracy |  0.42424244\n",
            "\n",
            "Step  154400: Ran 100 train steps in 48.22 secs\n",
            "Step  154400: train CrossEntropyLoss |  3.37805271\n",
            "Step  154400: eval  CrossEntropyLoss |  4.46025038\n",
            "Step  154400: eval          Accuracy |  0.26168224\n",
            "\n",
            "Step  154500: Ran 100 train steps in 48.17 secs\n",
            "Step  154500: train CrossEntropyLoss |  3.40323782\n",
            "Step  154500: eval  CrossEntropyLoss |  2.31522822\n",
            "Step  154500: eval          Accuracy |  0.57471263\n",
            "\n",
            "Step  154600: Ran 100 train steps in 47.87 secs\n",
            "Step  154600: train CrossEntropyLoss |  3.38861084\n",
            "Step  154600: eval  CrossEntropyLoss |  3.68483186\n",
            "Step  154600: eval          Accuracy |  0.39473686\n",
            "\n",
            "Step  154700: Ran 100 train steps in 47.99 secs\n",
            "Step  154700: train CrossEntropyLoss |  3.37233114\n",
            "Step  154700: eval  CrossEntropyLoss |  2.96420503\n",
            "Step  154700: eval          Accuracy |  0.47959182\n",
            "\n",
            "Step  154800: Ran 100 train steps in 47.99 secs\n",
            "Step  154800: train CrossEntropyLoss |  3.36043572\n",
            "Step  154800: eval  CrossEntropyLoss |  3.73810935\n",
            "Step  154800: eval          Accuracy |  0.36036038\n",
            "\n",
            "Step  154900: Ran 100 train steps in 47.93 secs\n",
            "Step  154900: train CrossEntropyLoss |  3.31800961\n",
            "Step  154900: eval  CrossEntropyLoss |  4.12036133\n",
            "Step  154900: eval          Accuracy |  0.31896552\n",
            "\n",
            "Step  155000: Ran 100 train steps in 48.09 secs\n",
            "Step  155000: train CrossEntropyLoss |  3.29212189\n",
            "Step  155000: eval  CrossEntropyLoss |  3.53421640\n",
            "Step  155000: eval          Accuracy |  0.43283582\n",
            "\n",
            "Step  155100: Ran 100 train steps in 47.90 secs\n",
            "Step  155100: train CrossEntropyLoss |  3.37850070\n",
            "Step  155100: eval  CrossEntropyLoss |  3.34060669\n",
            "Step  155100: eval          Accuracy |  0.39655173\n",
            "\n",
            "Step  155200: Ran 100 train steps in 47.98 secs\n",
            "Step  155200: train CrossEntropyLoss |  3.38673568\n",
            "Step  155200: eval  CrossEntropyLoss |  3.59304237\n",
            "Step  155200: eval          Accuracy |  0.43220338\n",
            "\n",
            "Step  155300: Ran 100 train steps in 47.76 secs\n",
            "Step  155300: train CrossEntropyLoss |  3.33931112\n",
            "Step  155300: eval  CrossEntropyLoss |  3.67135763\n",
            "Step  155300: eval          Accuracy |  0.40625000\n",
            "\n",
            "Step  155400: Ran 100 train steps in 48.06 secs\n",
            "Step  155400: train CrossEntropyLoss |  3.38249636\n",
            "Step  155400: eval  CrossEntropyLoss |  3.35847282\n",
            "Step  155400: eval          Accuracy |  0.44973546\n",
            "\n",
            "Step  155500: Ran 100 train steps in 48.04 secs\n",
            "Step  155500: train CrossEntropyLoss |  3.35945797\n",
            "Step  155500: eval  CrossEntropyLoss |  3.16724682\n",
            "Step  155500: eval          Accuracy |  0.45360827\n",
            "\n",
            "Step  155600: Ran 100 train steps in 48.02 secs\n",
            "Step  155600: train CrossEntropyLoss |  3.42359257\n",
            "Step  155600: eval  CrossEntropyLoss |  3.17276049\n",
            "Step  155600: eval          Accuracy |  0.51785719\n",
            "\n",
            "Step  155700: Ran 100 train steps in 47.86 secs\n",
            "Step  155700: train CrossEntropyLoss |  3.34362936\n",
            "Step  155700: eval  CrossEntropyLoss |  2.99505496\n",
            "Step  155700: eval          Accuracy |  0.49494949\n",
            "\n",
            "Step  155800: Ran 100 train steps in 47.87 secs\n",
            "Step  155800: train CrossEntropyLoss |  3.32830095\n",
            "Step  155800: eval  CrossEntropyLoss |  3.71425533\n",
            "Step  155800: eval          Accuracy |  0.41847828\n",
            "\n",
            "Step  155900: Ran 100 train steps in 47.98 secs\n",
            "Step  155900: train CrossEntropyLoss |  3.27971053\n",
            "Step  155900: eval  CrossEntropyLoss |  3.30510426\n",
            "Step  155900: eval          Accuracy |  0.46774191\n",
            "\n",
            "Step  156000: Ran 100 train steps in 48.10 secs\n",
            "Step  156000: train CrossEntropyLoss |  3.29863763\n",
            "Step  156000: eval  CrossEntropyLoss |  3.84193659\n",
            "Step  156000: eval          Accuracy |  0.36697245\n",
            "\n",
            "Step  156100: Ran 100 train steps in 47.98 secs\n",
            "Step  156100: train CrossEntropyLoss |  3.38637137\n",
            "Step  156100: eval  CrossEntropyLoss |  4.02348042\n",
            "Step  156100: eval          Accuracy |  0.35779816\n",
            "\n",
            "Step  156200: Ran 100 train steps in 47.90 secs\n",
            "Step  156200: train CrossEntropyLoss |  3.34732389\n",
            "Step  156200: eval  CrossEntropyLoss |  2.85219526\n",
            "Step  156200: eval          Accuracy |  0.52380955\n",
            "\n",
            "Step  156300: Ran 100 train steps in 48.03 secs\n",
            "Step  156300: train CrossEntropyLoss |  3.35994458\n",
            "Step  156300: eval  CrossEntropyLoss |  3.35318637\n",
            "Step  156300: eval          Accuracy |  0.45614034\n",
            "\n",
            "Step  156400: Ran 100 train steps in 47.90 secs\n",
            "Step  156400: train CrossEntropyLoss |  3.38742828\n",
            "Step  156400: eval  CrossEntropyLoss |  3.49504280\n",
            "Step  156400: eval          Accuracy |  0.42592594\n",
            "\n",
            "Step  156500: Ran 100 train steps in 48.17 secs\n",
            "Step  156500: train CrossEntropyLoss |  3.29135275\n",
            "Step  156500: eval  CrossEntropyLoss |  3.56492448\n",
            "Step  156500: eval          Accuracy |  0.43589747\n",
            "\n",
            "Step  156600: Ran 100 train steps in 48.03 secs\n",
            "Step  156600: train CrossEntropyLoss |  3.30425167\n",
            "Step  156600: eval  CrossEntropyLoss |  3.28350687\n",
            "Step  156600: eval          Accuracy |  0.37383175\n",
            "\n",
            "Step  156700: Ran 100 train steps in 48.10 secs\n",
            "Step  156700: train CrossEntropyLoss |  3.30372834\n",
            "Step  156700: eval  CrossEntropyLoss |  3.01836896\n",
            "Step  156700: eval          Accuracy |  0.43654820\n",
            "\n",
            "Step  156800: Ran 100 train steps in 48.05 secs\n",
            "Step  156800: train CrossEntropyLoss |  3.29520917\n",
            "Step  156800: eval  CrossEntropyLoss |  3.06703496\n",
            "Step  156800: eval          Accuracy |  0.49056605\n",
            "\n",
            "Step  156900: Ran 100 train steps in 47.37 secs\n",
            "Step  156900: train CrossEntropyLoss |  3.32206964\n",
            "Step  156900: eval  CrossEntropyLoss |  4.20279503\n",
            "Step  156900: eval          Accuracy |  0.41592920\n",
            "\n",
            "Step  157000: Ran 100 train steps in 47.04 secs\n",
            "Step  157000: train CrossEntropyLoss |  3.28441358\n",
            "Step  157000: eval  CrossEntropyLoss |  2.76969266\n",
            "Step  157000: eval          Accuracy |  0.48351648\n",
            "\n",
            "Step  157100: Ran 100 train steps in 46.72 secs\n",
            "Step  157100: train CrossEntropyLoss |  3.27092862\n",
            "Step  157100: eval  CrossEntropyLoss |  3.32970738\n",
            "Step  157100: eval          Accuracy |  0.40366971\n",
            "\n",
            "Step  157200: Ran 100 train steps in 46.49 secs\n",
            "Step  157200: train CrossEntropyLoss |  3.25735068\n",
            "Step  157200: eval  CrossEntropyLoss |  4.27892351\n",
            "Step  157200: eval          Accuracy |  0.32653061\n",
            "\n",
            "Step  157300: Ran 100 train steps in 46.30 secs\n",
            "Step  157300: train CrossEntropyLoss |  3.30332422\n",
            "Step  157300: eval  CrossEntropyLoss |  3.21333361\n",
            "Step  157300: eval          Accuracy |  0.54385966\n",
            "\n",
            "Step  157400: Ran 100 train steps in 46.21 secs\n",
            "Step  157400: train CrossEntropyLoss |  3.28747916\n",
            "Step  157400: eval  CrossEntropyLoss |  3.06918144\n",
            "Step  157400: eval          Accuracy |  0.48039219\n",
            "\n",
            "Step  157500: Ran 100 train steps in 46.20 secs\n",
            "Step  157500: train CrossEntropyLoss |  3.33706498\n",
            "Step  157500: eval  CrossEntropyLoss |  3.74352336\n",
            "Step  157500: eval          Accuracy |  0.41666669\n",
            "\n",
            "Step  157600: Ran 100 train steps in 46.34 secs\n",
            "Step  157600: train CrossEntropyLoss |  3.30581498\n",
            "Step  157600: eval  CrossEntropyLoss |  3.20995569\n",
            "Step  157600: eval          Accuracy |  0.46428573\n",
            "\n",
            "Step  157700: Ran 100 train steps in 46.27 secs\n",
            "Step  157700: train CrossEntropyLoss |  3.36216187\n",
            "Step  157700: eval  CrossEntropyLoss |  3.74865460\n",
            "Step  157700: eval          Accuracy |  0.41758242\n",
            "\n",
            "Step  157800: Ran 100 train steps in 46.30 secs\n",
            "Step  157800: train CrossEntropyLoss |  3.21890354\n",
            "Step  157800: eval  CrossEntropyLoss |  2.93128490\n",
            "Step  157800: eval          Accuracy |  0.51200002\n",
            "\n",
            "Step  157900: Ran 100 train steps in 46.23 secs\n",
            "Step  157900: train CrossEntropyLoss |  3.28104568\n",
            "Step  157900: eval  CrossEntropyLoss |  2.89315963\n",
            "Step  157900: eval          Accuracy |  0.48044696\n",
            "\n",
            "Step  158000: Ran 100 train steps in 46.05 secs\n",
            "Step  158000: train CrossEntropyLoss |  3.28704119\n",
            "Step  158000: eval  CrossEntropyLoss |  3.30566978\n",
            "Step  158000: eval          Accuracy |  0.43956044\n",
            "\n",
            "Step  158100: Ran 100 train steps in 46.13 secs\n",
            "Step  158100: train CrossEntropyLoss |  3.22875929\n",
            "Step  158100: eval  CrossEntropyLoss |  3.51485515\n",
            "Step  158100: eval          Accuracy |  0.43442625\n",
            "\n",
            "Step  158200: Ran 100 train steps in 46.11 secs\n",
            "Step  158200: train CrossEntropyLoss |  3.27238059\n",
            "Step  158200: eval  CrossEntropyLoss |  3.30784321\n",
            "Step  158200: eval          Accuracy |  0.46907219\n",
            "\n",
            "Step  158300: Ran 100 train steps in 46.51 secs\n",
            "Step  158300: train CrossEntropyLoss |  3.31606793\n",
            "Step  158300: eval  CrossEntropyLoss |  3.36766720\n",
            "Step  158300: eval          Accuracy |  0.46236560\n",
            "\n",
            "Step  158400: Ran 100 train steps in 46.07 secs\n",
            "Step  158400: train CrossEntropyLoss |  3.32559109\n",
            "Step  158400: eval  CrossEntropyLoss |  2.54662204\n",
            "Step  158400: eval          Accuracy |  0.53211010\n",
            "\n",
            "Step  158500: Ran 100 train steps in 46.28 secs\n",
            "Step  158500: train CrossEntropyLoss |  3.31402469\n",
            "Step  158500: eval  CrossEntropyLoss |  4.13353586\n",
            "Step  158500: eval          Accuracy |  0.38053098\n",
            "\n",
            "Step  158600: Ran 100 train steps in 46.15 secs\n",
            "Step  158600: train CrossEntropyLoss |  3.27759552\n",
            "Step  158600: eval  CrossEntropyLoss |  3.35476375\n",
            "Step  158600: eval          Accuracy |  0.42857143\n",
            "\n",
            "Step  158700: Ran 100 train steps in 46.28 secs\n",
            "Step  158700: train CrossEntropyLoss |  3.30345154\n",
            "Step  158700: eval  CrossEntropyLoss |  3.25038719\n",
            "Step  158700: eval          Accuracy |  0.48387095\n",
            "\n",
            "Step  158800: Ran 100 train steps in 46.26 secs\n",
            "Step  158800: train CrossEntropyLoss |  3.32164359\n",
            "Step  158800: eval  CrossEntropyLoss |  2.99304700\n",
            "Step  158800: eval          Accuracy |  0.53061223\n",
            "\n",
            "Step  158900: Ran 100 train steps in 46.14 secs\n",
            "Step  158900: train CrossEntropyLoss |  3.28827286\n",
            "Step  158900: eval  CrossEntropyLoss |  3.60229921\n",
            "Step  158900: eval          Accuracy |  0.43877551\n",
            "\n",
            "Step  159000: Ran 100 train steps in 46.26 secs\n",
            "Step  159000: train CrossEntropyLoss |  3.30821037\n",
            "Step  159000: eval  CrossEntropyLoss |  3.28631020\n",
            "Step  159000: eval          Accuracy |  0.44761905\n",
            "\n",
            "Step  159100: Ran 100 train steps in 46.06 secs\n",
            "Step  159100: train CrossEntropyLoss |  3.22748303\n",
            "Step  159100: eval  CrossEntropyLoss |  3.44399142\n",
            "Step  159100: eval          Accuracy |  0.39896375\n",
            "\n",
            "Step  159200: Ran 100 train steps in 46.22 secs\n",
            "Step  159200: train CrossEntropyLoss |  3.26508164\n",
            "Step  159200: eval  CrossEntropyLoss |  3.56410265\n",
            "Step  159200: eval          Accuracy |  0.42268044\n",
            "\n",
            "Step  159300: Ran 100 train steps in 46.34 secs\n",
            "Step  159300: train CrossEntropyLoss |  3.30228782\n",
            "Step  159300: eval  CrossEntropyLoss |  3.16491246\n",
            "Step  159300: eval          Accuracy |  0.49438202\n",
            "\n",
            "Step  159400: Ran 100 train steps in 47.82 secs\n",
            "Step  159400: train CrossEntropyLoss |  3.33376265\n",
            "Step  159400: eval  CrossEntropyLoss |  3.26965928\n",
            "Step  159400: eval          Accuracy |  0.47222224\n",
            "\n",
            "Step  159500: Ran 100 train steps in 48.15 secs\n",
            "Step  159500: train CrossEntropyLoss |  3.31836271\n",
            "Step  159500: eval  CrossEntropyLoss |  3.11063433\n",
            "Step  159500: eval          Accuracy |  0.47272727\n",
            "\n",
            "Step  159600: Ran 100 train steps in 48.24 secs\n",
            "Step  159600: train CrossEntropyLoss |  3.24473929\n",
            "Step  159600: eval  CrossEntropyLoss |  3.38465571\n",
            "Step  159600: eval          Accuracy |  0.44247788\n",
            "\n",
            "Step  159700: Ran 100 train steps in 48.29 secs\n",
            "Step  159700: train CrossEntropyLoss |  3.20198846\n",
            "Step  159700: eval  CrossEntropyLoss |  3.15019727\n",
            "Step  159700: eval          Accuracy |  0.45812806\n",
            "\n",
            "Step  159800: Ran 100 train steps in 48.39 secs\n",
            "Step  159800: train CrossEntropyLoss |  3.20715499\n",
            "Step  159800: eval  CrossEntropyLoss |  3.40522242\n",
            "Step  159800: eval          Accuracy |  0.44660196\n",
            "\n",
            "Step  159900: Ran 100 train steps in 48.11 secs\n",
            "Step  159900: train CrossEntropyLoss |  3.27915335\n",
            "Step  159900: eval  CrossEntropyLoss |  2.64746118\n",
            "Step  159900: eval          Accuracy |  0.51020408\n",
            "\n",
            "Step  160000: Ran 100 train steps in 47.98 secs\n",
            "Step  160000: train CrossEntropyLoss |  3.22117233\n",
            "Step  160000: eval  CrossEntropyLoss |  3.23263121\n",
            "Step  160000: eval          Accuracy |  0.41904762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "# !rsync -a /content/drive/MyDrive/model2/ ~/\r\n",
        "\r\n",
        "# copy the model to Google Drive\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model/\r\n",
        "\r\n",
        "# sync Google Drive dir with the train dir\r\n",
        "# !rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e4b51c-fad7-4701-8ba8-ed45a0a60b7d"
      },
      "source": [
        "train_article = train_text_pairs[5][0]\r\n",
        "train_summary = train_text_pairs[5][1]\r\n",
        "print(wrapper.fill(train_article))\r\n",
        "print('')\r\n",
        "eval_article = eval_text_pairs[1][0]\r\n",
        "eval_summary = eval_text_pairs[1][1]\r\n",
        "print(wrapper.fill(eval_article))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые придумали новый способ взаимодействия с графеном, который\n",
            "позволяет избавиться от \"слипающихся\" листов. статья ученых появилась\n",
            "в журнале acs nano, а ее краткое изложение приводится на сайте северо-\n",
            "западного университета, сотрудники которого принимали участие в\n",
            "работе. известно, что основной трудностью при работе с графеновыми\n",
            "листами является то, что при соприкосновении они слипаются под\n",
            "воздействием сил ван-дер-ваальса между собой при наложении друг на\n",
            "друга. это приводит к потере большинства уникальных свойств материала.\n",
            "для решения подобной проблемы, например, некоторые исследователи\n",
            "кладут между листами прокладки из другого материала, однако такое\n",
            "решение часто не слишком эффективно - атомы прокладки могут\n",
            "образовывать связи с атомами углерода в графене, что снова приводит к\n",
            "появлению дефектов в материале. в рамках нового исследования ученые\n",
            "предложили использовать графен не в виде ровных листов, а в виде\n",
            "смятых в комок листов. по словам исследователей, в подобном виде\n",
            "графен ведет себя как бумажные комки в мусорной корзине - несмотря на\n",
            "достаточно плотное расположение, поверхности листов, из которых они\n",
            "состоят, не соприкасаются. расчеты показывают, что при подобной\n",
            "упаковке листов графен сохраняет около 45 процентов исходной площади\n",
            "поверхности. для сравнения, при других способах организации удается\n",
            "спасти не более 16 процентов площади. графен как теоретическая\n",
            "абстракция рассматривался еще в конце 20-х годов прошлого века.\n",
            "начиная с 1960-х годов, он выступал в качестве удобной математической\n",
            "модели для расчетов в квантовой механике. впервые графен получили на\n",
            "практике константин новоселов и андрей гейм в 2004 году.\n",
            "\n",
            "сша планируют сократить численность военного контингента в южной\n",
            "корее. по информации корейского министерства иностранных дел, к концу\n",
            "2005 года из страны будет выведена треть американского контингента,\n",
            "составляющего в настоящее время 37500 военнослужащих, сообщает\n",
            "reuters. всего к концу 2005 года страну покинут 12500 американских\n",
            "солдат. 3600 из них продолжат службу в ираке. глава корейского мид\n",
            "отметил, что сша подходят к выводу войск очень внимательно, так как\n",
            "ситуация на полуострове остается напряженной. тем не менее, сша пошли\n",
            "навстречу желанию властей южной кореи иметь более независимую армию, и\n",
            "обещают оказать им в этом всяческое содействие. собственные силы южной\n",
            "кореи составляют на сегодняшний день 690 000 человек. армия северной\n",
            "кореи насчитывает 1 100 000 военнослужащих.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a8f0ec-2614-469c-d64e-65e30f69f1a4"
      },
      "source": [
        "# checking first symbol generation\r\n",
        "print(detokenize([next_symbol(tokenize(train_article)+[0], model)]))\r\n",
        "print(detokenize([next_symbol(tokenize(eval_article)+[0], model)]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые\n",
            "сша\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        \r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f70b658-0894-4fec-b590-d01f095b5944"
      },
      "source": [
        "print(train_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(train_article, model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые предложили использовать графен в мятом виде\n",
            "\n",
            "\n",
            "ученые\n",
            "ученые объяснили\n",
            "ученые объяснили отказ\n",
            "ученые объяснили отказ от\n",
            "ученые объяснили отказ от \"\n",
            "ученые объяснили отказ от \"сли\n",
            "ученые объяснили отказ от \"слипа\n",
            "ученые объяснили отказ от \"слипанных\n",
            "ученые объяснили отказ от \"слипанных\"\n",
            "ученые объяснили отказ от \"слипанных\" ли\n",
            "ученые объяснили отказ от \"слипанных\" листов\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d91f2e9c-bff9-41cc-e04f-dd7c9f8dfacb"
      },
      "source": [
        "print(eval_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article, model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша на треть сократят корейскую группировку\n",
            "\n",
            "\n",
            "сша\n",
            "сша готовы\n",
            "сша готовы сократить\n",
            "сша готовы сократить численность\n",
            "сша готовы сократить численность войск\n",
            "сша готовы сократить численность войск в\n",
            "сша готовы сократить численность войск в южной\n",
            "сша готовы сократить численность войск в южной корее\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L83lEskk4L7"
      },
      "source": [
        "model = SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='eval',\r\n",
        "                  ff_activation=tl.Relu)\r\n",
        "\r\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSDbAXjlF2f"
      },
      "source": [
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True, input_signature=shape11)\r\n",
        "\r\n",
        "# save the starting state\r\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-yINo6McPK1",
        "outputId": "c163f460-e5ce-4b41-d4f3-dc85f032398d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.array(tokenize(eval_article))[None, :]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  401,  5641,  7162, 10989,  4302, 15374,     5,  3396, 11441,\n",
              "        15949,    17,  1205,  8980,   204,  2919,  2799,  1996, 15945,\n",
              "           64,  6453,  2730,   173,    86,   719,   372,   102,  1982,\n",
              "        15926,  1469,  3148, 15374, 15945,  1191,  1839,     5,  1416,\n",
              "          368,  4897, 15974,   423,  4061, 15945,   258,  1761, 15949,\n",
              "          983,    64,  6453,  2730,   173,  4387,  2208,  1132, 14538,\n",
              "          423,  3019,  4848, 15949,  3525,   423,    86,   994,  1869,\n",
              "        15930,  6533,     5,  3961, 15949,   915,  8980,   204,  2872,\n",
              "          996, 15945,    79,   401,   149,  2692,    64,  6696,  3516,\n",
              "         2176,  2557,   833, 15945,   208,   207,  4369,    25, 12391,\n",
              "         4875,  6328, 11725, 15949,   734,    57,  1977, 15945,   401,\n",
              "         6176,    25, 15933,   443,  1323,  3054,   810,  2964,  3396,\n",
              "         5813,  6940,   565,  3121,  4819, 13988, 15945,    16,  4424,\n",
              "          162,  9965,   680,     5,   226,  6550,  7020,    48,  8164,\n",
              "        15949,  9568,  3864,  3396,  5813,  8089,    25,  7553,   892,\n",
              "         9250, 15958, 15924, 10749,   534, 15949,  7248,  3768,  5813,\n",
              "        15210,   107,  1728, 15924, 10749,  4061, 15949,     1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba3022d-d15d-41ea-d342-244ecaef1ea6"
      },
      "source": [
        "\r\n",
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "model.state = STARTING_STATE\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(tokenize(eval_article))[None, :],\r\n",
        "                                        temperature=0.0, max_length=20)\r\n",
        "print(wrapper.fill(detokenize(output[0])))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша евро евро евро евро евро евро евро евро евро евро евро евро евро\n",
            "евро евро евро евро евро евро\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}