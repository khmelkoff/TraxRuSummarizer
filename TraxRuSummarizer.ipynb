{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPCRx1KtowjMRn0QOx/8QG0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "# from trax.fastmath import numpy as jnp\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "394e5dcc-1777-430d-8a5c-f605af6ae42e"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "881db58e-9110-41c3-f3eb-751afd0c7766"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "9a92c548-7291-4ab1-8964-101f9d203a5b"
      },
      "source": [
        "data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fad88341710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "7365d1ff-e480-409a-cf0f-a3b7fa21ab28"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "for i in tqdm(range(data.shape[0])):\r\n",
        "    if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:03<00:00, 12705.50it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df5db5b-90ca-4aaa-f55f-0606dc9091ef"
      },
      "source": [
        "text_pairs[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "36b883d6-7a21-4036-a418-cb1726a5cddc"
      },
      "source": [
        "s0 = text_pairs[10][0]\r\n",
        "text_list = wrapper.wrap(s0[:300])\r\n",
        "for line in text_list:\r\n",
        "    print(line)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "b9544361-1c4b-4df2-d8c2-cebde018c5ba"
      },
      "source": [
        "# inintial shuffling\r\n",
        "random.shuffle(text_pairs)\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b847700b-4fa0-4ba1-be81-eec09123103c"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e448b93-b4c8-4be5-ea1b-2338c54b8039"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21dc39a-a3dc-4e7c-a34b-e34faef70894"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb",
        "outputId": "2e6ef865-20df-46fb-e719-cb0babf1fc74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "print('tokenized:')\r\n",
        "print(tokenized)\r\n",
        "print('len=', len(tokenized))\r\n",
        "detokenized = detokenize(tokenized)\r\n",
        "print('detokenized:')\r\n",
        "print(detokenized)\r\n",
        "print('len=', len(detokenized.split()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized:\n",
            "[3044, 11, 1627, 1080, 25, 1445, 288, 4205, 5442, 15945, 939, 11463, 1410, 164, 13393, 164, 8476, 1]\n",
            "len= 18\n",
            "detokenized:\n",
            "сведения о пассажирах на всех видах транспорта, где используются\n",
            "именные проездные билеты\n",
            "len= 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA-D8rF6JANP",
        "outputId": "df743609-fc17-44e7-ab03-2388f7350983"
      },
      "source": [
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 6062    56   107  4229 15983 15962  1401 15949     1     0  1025 13896\n",
            "  3019  4848  1515    78  8788     5 12703     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "637bc31a-ef67-4ade-8534-483f656a1542"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "2cb5b4bf-f0dd-486e-864b-065723a8e234"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6411,  2046,     5,  3603, 14037,  1072,    11,  4350,   730,\n",
              "          25,   154,  2223,   106,  3421,  4363,    75,  3190,   890,\n",
              "         701, 15945,  2049,   586, 11492, 12894,  1797,  1111, 15945,\n",
              "         258,    81,  1234,   295,   586,  4075,  6993,   309,  3197,\n",
              "        3233,     5,   421,   191,  6311,   462,  2066,  5031, 15949,\n",
              "        1002,    26,    25,  5026,  2046,   657,  1325,    10,    81,\n",
              "        3737, 11650,   580,  1110, 15957,   245,  3072,   653, 15945,\n",
              "        5425,   193,   249,  4835, 10278,    16,  5069,    17,    81,\n",
              "         103,  2223,   768,  2431,   295,  5355,  7519, 12894,   890,\n",
              "        1446,    17,   199,  8654,  2044,    16,  9284,   237,  3197,\n",
              "        3233,     5,   421,   191,  6311,   462,  2066,  5031, 15949,\n",
              "         505, 15504,   907,  7047,   794,  1324, 12291, 15949,  6223,\n",
              "        4654, 13090,  7978,    64,  2676,   768, 15341, 10188, 15949,\n",
              "        6411, 15945,     5,  1140,  2162, 15945,  9576, 11538,  5823,\n",
              "          75,   890,   701,     5,  4969,    25,  7259,     5,  2525,\n",
              "        1074, 15949,    81,  2854, 15948,  2408,    25,  7680,  1586,\n",
              "        1074,   294,   245,  1956,   759,    86,  3015,    81,  3737,\n",
              "        1462,   580,  1110, 15957,  1937,   432,    52, 15273, 15949,\n",
              "        1280,    11,  4350,   730,    25,   154,  2223,   106,  3421,\n",
              "         483,  2903,    17,  2293,  2046,    81,  2458, 15953,   708,\n",
              "         294,   976,  5693,   580,  1110, 15949,  4430,   200,  1441,\n",
              "           5,  5020,     4,  7234, 15949,  7943,  3350,    46,  2615,\n",
              "        2224,  1280,    17,  9317,    81,    83,  6324,   651,  9310,\n",
              "       15957,    16,    81,   473,   179,   146,  7997,     5,  1813,\n",
              "        3297,   907,   295,   898,   534,    17,  3102,  2431,  6854,\n",
              "       15949,   593,   445,  1826,  7192,  7348,   398,    12,   712,\n",
              "        1672,   149,  2202,   188,    11, 12672, 15949,   759,  6841,\n",
              "        1824, 15805,   268,   277,  5549,  2202,   787,    11, 12672,\n",
              "       15949,     1,     0,  9619,  5475,    75,   890,   701,     5,\n",
              "        3603, 14037,    81,   103,  2223,   132,  1072, 15957,     1,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=8,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "6456f89b-9580-4c1b-f5d0-1aa2f83b7e77"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=4000, max_value=0.00025)\r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.0001)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.0001), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=8,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "!rsync -a /content/drive/MyDrive/model2/ ~/"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "e7f8c576-f381-4a08-fb54-f45cf5db8cda"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(40000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  40100: Ran 100 train steps in 106.61 secs\n",
            "Step  40100: train CrossEntropyLoss |  5.99441719\n",
            "Step  40100: eval  CrossEntropyLoss |  5.75384235\n",
            "Step  40100: eval          Accuracy |  0.16417910\n",
            "\n",
            "Step  40200: Ran 100 train steps in 78.38 secs\n",
            "Step  40200: train CrossEntropyLoss |  5.97902918\n",
            "Step  40200: eval  CrossEntropyLoss |  5.87748003\n",
            "Step  40200: eval          Accuracy |  0.13333334\n",
            "\n",
            "Step  40300: Ran 100 train steps in 57.37 secs\n",
            "Step  40300: train CrossEntropyLoss |  5.87072420\n",
            "Step  40300: eval  CrossEntropyLoss |  5.79720879\n",
            "Step  40300: eval          Accuracy |  0.18584071\n",
            "\n",
            "Step  40400: Ran 100 train steps in 56.85 secs\n",
            "Step  40400: train CrossEntropyLoss |  5.87892008\n",
            "Step  40400: eval  CrossEntropyLoss |  5.76516867\n",
            "Step  40400: eval          Accuracy |  0.19431281\n",
            "\n",
            "Step  40500: Ran 100 train steps in 57.58 secs\n",
            "Step  40500: train CrossEntropyLoss |  5.80265379\n",
            "Step  40500: eval  CrossEntropyLoss |  5.93631124\n",
            "Step  40500: eval          Accuracy |  0.13178295\n",
            "\n",
            "Step  40600: Ran 100 train steps in 58.07 secs\n",
            "Step  40600: train CrossEntropyLoss |  5.78017378\n",
            "Step  40600: eval  CrossEntropyLoss |  5.98625898\n",
            "Step  40600: eval          Accuracy |  0.10434783\n",
            "\n",
            "Step  40700: Ran 100 train steps in 57.81 secs\n",
            "Step  40700: train CrossEntropyLoss |  5.72859192\n",
            "Step  40700: eval  CrossEntropyLoss |  5.97464609\n",
            "Step  40700: eval          Accuracy |  0.20754717\n",
            "\n",
            "Step  40800: Ran 100 train steps in 57.94 secs\n",
            "Step  40800: train CrossEntropyLoss |  5.77076292\n",
            "Step  40800: eval  CrossEntropyLoss |  5.59531307\n",
            "Step  40800: eval          Accuracy |  0.19607843\n",
            "\n",
            "Step  40900: Ran 100 train steps in 58.49 secs\n",
            "Step  40900: train CrossEntropyLoss |  5.70014811\n",
            "Step  40900: eval  CrossEntropyLoss |  5.56061459\n",
            "Step  40900: eval          Accuracy |  0.19791667\n",
            "\n",
            "Step  41000: Ran 100 train steps in 58.48 secs\n",
            "Step  41000: train CrossEntropyLoss |  5.70889759\n",
            "Step  41000: eval  CrossEntropyLoss |  5.82806921\n",
            "Step  41000: eval          Accuracy |  0.20652175\n",
            "\n",
            "Step  41100: Ran 100 train steps in 58.15 secs\n",
            "Step  41100: train CrossEntropyLoss |  5.75121975\n",
            "Step  41100: eval  CrossEntropyLoss |  6.39787436\n",
            "Step  41100: eval          Accuracy |  0.13207547\n",
            "\n",
            "Step  41200: Ran 100 train steps in 58.19 secs\n",
            "Step  41200: train CrossEntropyLoss |  5.66007376\n",
            "Step  41200: eval  CrossEntropyLoss |  5.53785706\n",
            "Step  41200: eval          Accuracy |  0.19047619\n",
            "\n",
            "Step  41300: Ran 100 train steps in 58.56 secs\n",
            "Step  41300: train CrossEntropyLoss |  5.65251446\n",
            "Step  41300: eval  CrossEntropyLoss |  5.60732508\n",
            "Step  41300: eval          Accuracy |  0.19607843\n",
            "\n",
            "Step  41400: Ran 100 train steps in 58.53 secs\n",
            "Step  41400: train CrossEntropyLoss |  5.64149666\n",
            "Step  41400: eval  CrossEntropyLoss |  5.84458399\n",
            "Step  41400: eval          Accuracy |  0.14150944\n",
            "\n",
            "Step  41500: Ran 100 train steps in 58.62 secs\n",
            "Step  41500: train CrossEntropyLoss |  5.61806679\n",
            "Step  41500: eval  CrossEntropyLoss |  5.68752241\n",
            "Step  41500: eval          Accuracy |  0.16666667\n",
            "\n",
            "Step  41600: Ran 100 train steps in 58.62 secs\n",
            "Step  41600: train CrossEntropyLoss |  5.65493679\n",
            "Step  41600: eval  CrossEntropyLoss |  5.45104980\n",
            "Step  41600: eval          Accuracy |  0.21857923\n",
            "\n",
            "Step  41700: Ran 100 train steps in 58.12 secs\n",
            "Step  41700: train CrossEntropyLoss |  5.64070845\n",
            "Step  41700: eval  CrossEntropyLoss |  5.21230364\n",
            "Step  41700: eval          Accuracy |  0.22772276\n",
            "\n",
            "Step  41800: Ran 100 train steps in 58.21 secs\n",
            "Step  41800: train CrossEntropyLoss |  5.61336613\n",
            "Step  41800: eval  CrossEntropyLoss |  5.23863602\n",
            "Step  41800: eval          Accuracy |  0.18918920\n",
            "\n",
            "Step  41900: Ran 100 train steps in 59.05 secs\n",
            "Step  41900: train CrossEntropyLoss |  5.63461924\n",
            "Step  41900: eval  CrossEntropyLoss |  5.57529545\n",
            "Step  41900: eval          Accuracy |  0.13084112\n",
            "\n",
            "Step  42000: Ran 100 train steps in 58.08 secs\n",
            "Step  42000: train CrossEntropyLoss |  5.60227394\n",
            "Step  42000: eval  CrossEntropyLoss |  5.08311367\n",
            "Step  42000: eval          Accuracy |  0.25438598\n",
            "\n",
            "Step  42100: Ran 100 train steps in 58.64 secs\n",
            "Step  42100: train CrossEntropyLoss |  5.55993986\n",
            "Step  42100: eval  CrossEntropyLoss |  5.58753777\n",
            "Step  42100: eval          Accuracy |  0.14423077\n",
            "\n",
            "Step  42200: Ran 100 train steps in 58.83 secs\n",
            "Step  42200: train CrossEntropyLoss |  5.56847382\n",
            "Step  42200: eval  CrossEntropyLoss |  5.72999144\n",
            "Step  42200: eval          Accuracy |  0.14678898\n",
            "\n",
            "Step  42300: Ran 100 train steps in 58.40 secs\n",
            "Step  42300: train CrossEntropyLoss |  5.52270842\n",
            "Step  42300: eval  CrossEntropyLoss |  5.32927036\n",
            "Step  42300: eval          Accuracy |  0.17000000\n",
            "\n",
            "Step  42400: Ran 100 train steps in 58.58 secs\n",
            "Step  42400: train CrossEntropyLoss |  5.55244064\n",
            "Step  42400: eval  CrossEntropyLoss |  5.43847179\n",
            "Step  42400: eval          Accuracy |  0.21287128\n",
            "\n",
            "Step  42500: Ran 100 train steps in 58.85 secs\n",
            "Step  42500: train CrossEntropyLoss |  5.57983494\n",
            "Step  42500: eval  CrossEntropyLoss |  5.46670818\n",
            "Step  42500: eval          Accuracy |  0.17204301\n",
            "\n",
            "Step  42600: Ran 100 train steps in 59.07 secs\n",
            "Step  42600: train CrossEntropyLoss |  5.52940655\n",
            "Step  42600: eval  CrossEntropyLoss |  5.86861515\n",
            "Step  42600: eval          Accuracy |  0.21212122\n",
            "\n",
            "Step  42700: Ran 100 train steps in 58.89 secs\n",
            "Step  42700: train CrossEntropyLoss |  5.50158310\n",
            "Step  42700: eval  CrossEntropyLoss |  5.58761644\n",
            "Step  42700: eval          Accuracy |  0.21052632\n",
            "\n",
            "Step  42800: Ran 100 train steps in 58.84 secs\n",
            "Step  42800: train CrossEntropyLoss |  5.49735832\n",
            "Step  42800: eval  CrossEntropyLoss |  5.56134129\n",
            "Step  42800: eval          Accuracy |  0.17318437\n",
            "\n",
            "Step  42900: Ran 100 train steps in 59.01 secs\n",
            "Step  42900: train CrossEntropyLoss |  5.47089148\n",
            "Step  42900: eval  CrossEntropyLoss |  5.68658590\n",
            "Step  42900: eval          Accuracy |  0.16216217\n",
            "\n",
            "Step  43000: Ran 100 train steps in 59.02 secs\n",
            "Step  43000: train CrossEntropyLoss |  5.53731012\n",
            "Step  43000: eval  CrossEntropyLoss |  5.96950674\n",
            "Step  43000: eval          Accuracy |  0.18085106\n",
            "\n",
            "Step  43100: Ran 100 train steps in 58.32 secs\n",
            "Step  43100: train CrossEntropyLoss |  5.46736240\n",
            "Step  43100: eval  CrossEntropyLoss |  5.33879900\n",
            "Step  43100: eval          Accuracy |  0.21319796\n",
            "\n",
            "Step  43200: Ran 100 train steps in 58.93 secs\n",
            "Step  43200: train CrossEntropyLoss |  5.55281401\n",
            "Step  43200: eval  CrossEntropyLoss |  5.50625896\n",
            "Step  43200: eval          Accuracy |  0.17021276\n",
            "\n",
            "Step  43300: Ran 100 train steps in 58.95 secs\n",
            "Step  43300: train CrossEntropyLoss |  5.45045090\n",
            "Step  43300: eval  CrossEntropyLoss |  5.73281288\n",
            "Step  43300: eval          Accuracy |  0.14000000\n",
            "\n",
            "Step  43400: Ran 100 train steps in 59.13 secs\n",
            "Step  43400: train CrossEntropyLoss |  5.50915051\n",
            "Step  43400: eval  CrossEntropyLoss |  5.17227077\n",
            "Step  43400: eval          Accuracy |  0.20202020\n",
            "\n",
            "Step  43500: Ran 100 train steps in 58.49 secs\n",
            "Step  43500: train CrossEntropyLoss |  5.50955868\n",
            "Step  43500: eval  CrossEntropyLoss |  5.85741615\n",
            "Step  43500: eval          Accuracy |  0.15384616\n",
            "\n",
            "Step  43600: Ran 100 train steps in 58.85 secs\n",
            "Step  43600: train CrossEntropyLoss |  5.50747490\n",
            "Step  43600: eval  CrossEntropyLoss |  5.15050030\n",
            "Step  43600: eval          Accuracy |  0.20370370\n",
            "\n",
            "Step  43700: Ran 100 train steps in 58.94 secs\n",
            "Step  43700: train CrossEntropyLoss |  5.51599598\n",
            "Step  43700: eval  CrossEntropyLoss |  5.29655313\n",
            "Step  43700: eval          Accuracy |  0.21052632\n",
            "\n",
            "Step  43800: Ran 100 train steps in 59.18 secs\n",
            "Step  43800: train CrossEntropyLoss |  5.45640898\n",
            "Step  43800: eval  CrossEntropyLoss |  5.71673393\n",
            "Step  43800: eval          Accuracy |  0.16379310\n",
            "\n",
            "Step  43900: Ran 100 train steps in 59.18 secs\n",
            "Step  43900: train CrossEntropyLoss |  5.44260788\n",
            "Step  43900: eval  CrossEntropyLoss |  5.49695969\n",
            "Step  43900: eval          Accuracy |  0.18750000\n",
            "\n",
            "Step  44000: Ran 100 train steps in 58.69 secs\n",
            "Step  44000: train CrossEntropyLoss |  5.51151180\n",
            "Step  44000: eval  CrossEntropyLoss |  5.49269009\n",
            "Step  44000: eval          Accuracy |  0.19491525\n",
            "\n",
            "Step  44100: Ran 100 train steps in 59.04 secs\n",
            "Step  44100: train CrossEntropyLoss |  5.39673328\n",
            "Step  44100: eval  CrossEntropyLoss |  5.52280140\n",
            "Step  44100: eval          Accuracy |  0.18279570\n",
            "\n",
            "Step  44200: Ran 100 train steps in 59.30 secs\n",
            "Step  44200: train CrossEntropyLoss |  5.46475410\n",
            "Step  44200: eval  CrossEntropyLoss |  5.23725224\n",
            "Step  44200: eval          Accuracy |  0.20105821\n",
            "\n",
            "Step  44300: Ran 100 train steps in 58.43 secs\n",
            "Step  44300: train CrossEntropyLoss |  5.44663000\n",
            "Step  44300: eval  CrossEntropyLoss |  5.71451569\n",
            "Step  44300: eval          Accuracy |  0.15094340\n",
            "\n",
            "Step  44400: Ran 100 train steps in 59.34 secs\n",
            "Step  44400: train CrossEntropyLoss |  5.39955997\n",
            "Step  44400: eval  CrossEntropyLoss |  4.50436401\n",
            "Step  44400: eval          Accuracy |  0.25263160\n",
            "\n",
            "Step  44500: Ran 100 train steps in 59.00 secs\n",
            "Step  44500: train CrossEntropyLoss |  5.46929550\n",
            "Step  44500: eval  CrossEntropyLoss |  5.29471684\n",
            "Step  44500: eval          Accuracy |  0.22164950\n",
            "\n",
            "Step  44600: Ran 100 train steps in 58.50 secs\n",
            "Step  44600: train CrossEntropyLoss |  5.37375736\n",
            "Step  44600: eval  CrossEntropyLoss |  5.56607437\n",
            "Step  44600: eval          Accuracy |  0.19191919\n",
            "\n",
            "Step  44700: Ran 100 train steps in 59.28 secs\n",
            "Step  44700: train CrossEntropyLoss |  5.40675259\n",
            "Step  44700: eval  CrossEntropyLoss |  5.56217003\n",
            "Step  44700: eval          Accuracy |  0.19379845\n",
            "\n",
            "Step  44800: Ran 100 train steps in 59.44 secs\n",
            "Step  44800: train CrossEntropyLoss |  5.44127178\n",
            "Step  44800: eval  CrossEntropyLoss |  5.51904535\n",
            "Step  44800: eval          Accuracy |  0.20430107\n",
            "\n",
            "Step  44900: Ran 100 train steps in 59.28 secs\n",
            "Step  44900: train CrossEntropyLoss |  5.39503574\n",
            "Step  44900: eval  CrossEntropyLoss |  5.66148853\n",
            "Step  44900: eval          Accuracy |  0.15306123\n",
            "\n",
            "Step  45000: Ran 100 train steps in 58.58 secs\n",
            "Step  45000: train CrossEntropyLoss |  5.43930387\n",
            "Step  45000: eval  CrossEntropyLoss |  5.40351772\n",
            "Step  45000: eval          Accuracy |  0.20812182\n",
            "\n",
            "Step  45100: Ran 100 train steps in 58.72 secs\n",
            "Step  45100: train CrossEntropyLoss |  5.37106848\n",
            "Step  45100: eval  CrossEntropyLoss |  5.43670464\n",
            "Step  45100: eval          Accuracy |  0.24271844\n",
            "\n",
            "Step  45200: Ran 100 train steps in 59.17 secs\n",
            "Step  45200: train CrossEntropyLoss |  5.37004375\n",
            "Step  45200: eval  CrossEntropyLoss |  5.86776924\n",
            "Step  45200: eval          Accuracy |  0.16346155\n",
            "\n",
            "Step  45300: Ran 100 train steps in 59.16 secs\n",
            "Step  45300: train CrossEntropyLoss |  5.37307835\n",
            "Step  45300: eval  CrossEntropyLoss |  5.30819511\n",
            "Step  45300: eval          Accuracy |  0.16753927\n",
            "\n",
            "Step  45400: Ran 100 train steps in 59.13 secs\n",
            "Step  45400: train CrossEntropyLoss |  5.40608740\n",
            "Step  45400: eval  CrossEntropyLoss |  5.65596294\n",
            "Step  45400: eval          Accuracy |  0.13636363\n",
            "\n",
            "Step  45500: Ran 100 train steps in 58.54 secs\n",
            "Step  45500: train CrossEntropyLoss |  5.39756775\n",
            "Step  45500: eval  CrossEntropyLoss |  5.25666428\n",
            "Step  45500: eval          Accuracy |  0.20588236\n",
            "\n",
            "Step  45600: Ran 100 train steps in 59.23 secs\n",
            "Step  45600: train CrossEntropyLoss |  5.33646488\n",
            "Step  45600: eval  CrossEntropyLoss |  5.21971416\n",
            "Step  45600: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  45700: Ran 100 train steps in 58.82 secs\n",
            "Step  45700: train CrossEntropyLoss |  5.40178585\n",
            "Step  45700: eval  CrossEntropyLoss |  5.45044899\n",
            "Step  45700: eval          Accuracy |  0.18269232\n",
            "\n",
            "Step  45800: Ran 100 train steps in 59.09 secs\n",
            "Step  45800: train CrossEntropyLoss |  5.38570166\n",
            "Step  45800: eval  CrossEntropyLoss |  5.43899298\n",
            "Step  45800: eval          Accuracy |  0.18333334\n",
            "\n",
            "Step  45900: Ran 100 train steps in 58.78 secs\n",
            "Step  45900: train CrossEntropyLoss |  5.35985041\n",
            "Step  45900: eval  CrossEntropyLoss |  5.11173248\n",
            "Step  45900: eval          Accuracy |  0.20975609\n",
            "\n",
            "Step  46000: Ran 100 train steps in 59.17 secs\n",
            "Step  46000: train CrossEntropyLoss |  5.35238409\n",
            "Step  46000: eval  CrossEntropyLoss |  5.10855770\n",
            "Step  46000: eval          Accuracy |  0.20952381\n",
            "\n",
            "Step  46100: Ran 100 train steps in 59.41 secs\n",
            "Step  46100: train CrossEntropyLoss |  5.38932228\n",
            "Step  46100: eval  CrossEntropyLoss |  5.26878929\n",
            "Step  46100: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  46200: Ran 100 train steps in 58.67 secs\n",
            "Step  46200: train CrossEntropyLoss |  5.38116455\n",
            "Step  46200: eval  CrossEntropyLoss |  5.34677553\n",
            "Step  46200: eval          Accuracy |  0.22522523\n",
            "\n",
            "Step  46300: Ran 100 train steps in 58.94 secs\n",
            "Step  46300: train CrossEntropyLoss |  5.38282013\n",
            "Step  46300: eval  CrossEntropyLoss |  5.61328363\n",
            "Step  46300: eval          Accuracy |  0.22222222\n",
            "\n",
            "Step  46400: Ran 100 train steps in 58.66 secs\n",
            "Step  46400: train CrossEntropyLoss |  5.33742619\n",
            "Step  46400: eval  CrossEntropyLoss |  5.23624229\n",
            "Step  46400: eval          Accuracy |  0.17647059\n",
            "\n",
            "Step  46500: Ran 100 train steps in 58.67 secs\n",
            "Step  46500: train CrossEntropyLoss |  5.38950253\n",
            "Step  46500: eval  CrossEntropyLoss |  6.21126366\n",
            "Step  46500: eval          Accuracy |  0.17475729\n",
            "\n",
            "Step  46600: Ran 100 train steps in 59.59 secs\n",
            "Step  46600: train CrossEntropyLoss |  5.38611031\n",
            "Step  46600: eval  CrossEntropyLoss |  5.36544180\n",
            "Step  46600: eval          Accuracy |  0.19642858\n",
            "\n",
            "Step  46700: Ran 100 train steps in 59.00 secs\n",
            "Step  46700: train CrossEntropyLoss |  5.37012625\n",
            "Step  46700: eval  CrossEntropyLoss |  5.33908176\n",
            "Step  46700: eval          Accuracy |  0.19819820\n",
            "\n",
            "Step  46800: Ran 100 train steps in 59.29 secs\n",
            "Step  46800: train CrossEntropyLoss |  5.35508156\n",
            "Step  46800: eval  CrossEntropyLoss |  5.54688072\n",
            "Step  46800: eval          Accuracy |  0.19354838\n",
            "\n",
            "Step  46900: Ran 100 train steps in 58.96 secs\n",
            "Step  46900: train CrossEntropyLoss |  5.37496614\n",
            "Step  46900: eval  CrossEntropyLoss |  5.44926214\n",
            "Step  46900: eval          Accuracy |  0.18224299\n",
            "\n",
            "Step  47000: Ran 100 train steps in 58.99 secs\n",
            "Step  47000: train CrossEntropyLoss |  5.29849100\n",
            "Step  47000: eval  CrossEntropyLoss |  4.63774967\n",
            "Step  47000: eval          Accuracy |  0.23364486\n",
            "\n",
            "Step  47100: Ran 100 train steps in 59.27 secs\n",
            "Step  47100: train CrossEntropyLoss |  5.34000778\n",
            "Step  47100: eval  CrossEntropyLoss |  4.90851784\n",
            "Step  47100: eval          Accuracy |  0.24299064\n",
            "\n",
            "Step  47200: Ran 100 train steps in 59.02 secs\n",
            "Step  47200: train CrossEntropyLoss |  5.33260489\n",
            "Step  47200: eval  CrossEntropyLoss |  5.44224215\n",
            "Step  47200: eval          Accuracy |  0.16101696\n",
            "\n",
            "Step  47300: Ran 100 train steps in 59.10 secs\n",
            "Step  47300: train CrossEntropyLoss |  5.26937103\n",
            "Step  47300: eval  CrossEntropyLoss |  5.35700464\n",
            "Step  47300: eval          Accuracy |  0.15740740\n",
            "\n",
            "Step  47400: Ran 100 train steps in 59.54 secs\n",
            "Step  47400: train CrossEntropyLoss |  5.35944510\n",
            "Step  47400: eval  CrossEntropyLoss |  4.99080992\n",
            "Step  47400: eval          Accuracy |  0.27127659\n",
            "\n",
            "Step  47500: Ran 100 train steps in 60.23 secs\n",
            "Step  47500: train CrossEntropyLoss |  5.30896950\n",
            "Step  47500: eval  CrossEntropyLoss |  5.60405302\n",
            "Step  47500: eval          Accuracy |  0.20175439\n",
            "\n",
            "Step  47600: Ran 100 train steps in 60.57 secs\n",
            "Step  47600: train CrossEntropyLoss |  5.35968781\n",
            "Step  47600: eval  CrossEntropyLoss |  5.93214607\n",
            "Step  47600: eval          Accuracy |  0.19191919\n",
            "\n",
            "Step  47700: Ran 100 train steps in 59.06 secs\n",
            "Step  47700: train CrossEntropyLoss |  5.33657932\n",
            "Step  47700: eval  CrossEntropyLoss |  5.08936024\n",
            "Step  47700: eval          Accuracy |  0.14432991\n",
            "\n",
            "Step  47800: Ran 100 train steps in 59.47 secs\n",
            "Step  47800: train CrossEntropyLoss |  5.32749748\n",
            "Step  47800: eval  CrossEntropyLoss |  4.98041344\n",
            "Step  47800: eval          Accuracy |  0.21078432\n",
            "\n",
            "Step  47900: Ran 100 train steps in 58.58 secs\n",
            "Step  47900: train CrossEntropyLoss |  5.32393408\n",
            "Step  47900: eval  CrossEntropyLoss |  4.74061823\n",
            "Step  47900: eval          Accuracy |  0.26881722\n",
            "\n",
            "Step  48000: Ran 100 train steps in 59.00 secs\n",
            "Step  48000: train CrossEntropyLoss |  5.28971291\n",
            "Step  48000: eval  CrossEntropyLoss |  5.33418989\n",
            "Step  48000: eval          Accuracy |  0.19230770\n",
            "\n",
            "Step  48100: Ran 100 train steps in 58.93 secs\n",
            "Step  48100: train CrossEntropyLoss |  5.30930281\n",
            "Step  48100: eval  CrossEntropyLoss |  5.32857037\n",
            "Step  48100: eval          Accuracy |  0.15652174\n",
            "\n",
            "Step  48200: Ran 100 train steps in 58.62 secs\n",
            "Step  48200: train CrossEntropyLoss |  5.29305220\n",
            "Step  48200: eval  CrossEntropyLoss |  5.04484463\n",
            "Step  48200: eval          Accuracy |  0.20207255\n",
            "\n",
            "Step  48300: Ran 100 train steps in 59.02 secs\n",
            "Step  48300: train CrossEntropyLoss |  5.27072477\n",
            "Step  48300: eval  CrossEntropyLoss |  5.66349173\n",
            "Step  48300: eval          Accuracy |  0.19819820\n",
            "\n",
            "Step  48400: Ran 100 train steps in 58.37 secs\n",
            "Step  48400: train CrossEntropyLoss |  5.28671980\n",
            "Step  48400: eval  CrossEntropyLoss |  5.54445314\n",
            "Step  48400: eval          Accuracy |  0.17924529\n",
            "\n",
            "Step  48500: Ran 100 train steps in 58.39 secs\n",
            "Step  48500: train CrossEntropyLoss |  5.27795410\n",
            "Step  48500: eval  CrossEntropyLoss |  5.59253407\n",
            "Step  48500: eval          Accuracy |  0.18556702\n",
            "\n",
            "Step  48600: Ran 100 train steps in 58.69 secs\n",
            "Step  48600: train CrossEntropyLoss |  5.28343534\n",
            "Step  48600: eval  CrossEntropyLoss |  6.02820349\n",
            "Step  48600: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step  48700: Ran 100 train steps in 58.84 secs\n",
            "Step  48700: train CrossEntropyLoss |  5.37881374\n",
            "Step  48700: eval  CrossEntropyLoss |  5.06942129\n",
            "Step  48700: eval          Accuracy |  0.23204421\n",
            "\n",
            "Step  48800: Ran 100 train steps in 59.78 secs\n",
            "Step  48800: train CrossEntropyLoss |  5.25745106\n",
            "Step  48800: eval  CrossEntropyLoss |  4.38587141\n",
            "Step  48800: eval          Accuracy |  0.27966103\n",
            "\n",
            "Step  48900: Ran 100 train steps in 61.44 secs\n",
            "Step  48900: train CrossEntropyLoss |  5.30148220\n",
            "Step  48900: eval  CrossEntropyLoss |  5.03484821\n",
            "Step  48900: eval          Accuracy |  0.19191919\n",
            "\n",
            "Step  49000: Ran 100 train steps in 61.20 secs\n",
            "Step  49000: train CrossEntropyLoss |  5.27023649\n",
            "Step  49000: eval  CrossEntropyLoss |  5.46790218\n",
            "Step  49000: eval          Accuracy |  0.21782178\n",
            "\n",
            "Step  49100: Ran 100 train steps in 61.57 secs\n",
            "Step  49100: train CrossEntropyLoss |  5.33072186\n",
            "Step  49100: eval  CrossEntropyLoss |  5.43706989\n",
            "Step  49100: eval          Accuracy |  0.16585366\n",
            "\n",
            "Step  49200: Ran 100 train steps in 61.50 secs\n",
            "Step  49200: train CrossEntropyLoss |  5.25976324\n",
            "Step  49200: eval  CrossEntropyLoss |  5.61656809\n",
            "Step  49200: eval          Accuracy |  0.16981132\n",
            "\n",
            "Step  49300: Ran 100 train steps in 61.00 secs\n",
            "Step  49300: train CrossEntropyLoss |  5.24883795\n",
            "Step  49300: eval  CrossEntropyLoss |  5.33407116\n",
            "Step  49300: eval          Accuracy |  0.20895521\n",
            "\n",
            "Step  49400: Ran 100 train steps in 61.40 secs\n",
            "Step  49400: train CrossEntropyLoss |  5.32518959\n",
            "Step  49400: eval  CrossEntropyLoss |  5.46241999\n",
            "Step  49400: eval          Accuracy |  0.13888890\n",
            "\n",
            "Step  49500: Ran 100 train steps in 61.18 secs\n",
            "Step  49500: train CrossEntropyLoss |  5.21518040\n",
            "Step  49500: eval  CrossEntropyLoss |  5.36179018\n",
            "Step  49500: eval          Accuracy |  0.20833334\n",
            "\n",
            "Step  49600: Ran 100 train steps in 61.40 secs\n",
            "Step  49600: train CrossEntropyLoss |  5.25785732\n",
            "Step  49600: eval  CrossEntropyLoss |  4.86695528\n",
            "Step  49600: eval          Accuracy |  0.22018348\n",
            "\n",
            "Step  49700: Ran 100 train steps in 59.51 secs\n",
            "Step  49700: train CrossEntropyLoss |  5.27818108\n",
            "Step  49700: eval  CrossEntropyLoss |  5.19016457\n",
            "Step  49700: eval          Accuracy |  0.21276595\n",
            "\n",
            "Step  49800: Ran 100 train steps in 60.19 secs\n",
            "Step  49800: train CrossEntropyLoss |  5.30832577\n",
            "Step  49800: eval  CrossEntropyLoss |  4.57157612\n",
            "Step  49800: eval          Accuracy |  0.26130652\n",
            "\n",
            "Step  49900: Ran 100 train steps in 59.14 secs\n",
            "Step  49900: train CrossEntropyLoss |  5.28528690\n",
            "Step  49900: eval  CrossEntropyLoss |  5.37629604\n",
            "Step  49900: eval          Accuracy |  0.20689654\n",
            "\n",
            "Step  50000: Ran 100 train steps in 59.62 secs\n",
            "Step  50000: train CrossEntropyLoss |  5.28179312\n",
            "Step  50000: eval  CrossEntropyLoss |  5.44930458\n",
            "Step  50000: eval          Accuracy |  0.15789473\n",
            "\n",
            "Step  50100: Ran 100 train steps in 59.65 secs\n",
            "Step  50100: train CrossEntropyLoss |  5.30451727\n",
            "Step  50100: eval  CrossEntropyLoss |  5.04515314\n",
            "Step  50100: eval          Accuracy |  0.21461189\n",
            "\n",
            "Step  50200: Ran 100 train steps in 59.10 secs\n",
            "Step  50200: train CrossEntropyLoss |  5.28344584\n",
            "Step  50200: eval  CrossEntropyLoss |  5.50725698\n",
            "Step  50200: eval          Accuracy |  0.12820514\n",
            "\n",
            "Step  50300: Ran 100 train steps in 58.88 secs\n",
            "Step  50300: train CrossEntropyLoss |  5.28443813\n",
            "Step  50300: eval  CrossEntropyLoss |  5.64695120\n",
            "Step  50300: eval          Accuracy |  0.17431192\n",
            "\n",
            "Step  50400: Ran 100 train steps in 59.42 secs\n",
            "Step  50400: train CrossEntropyLoss |  5.23280096\n",
            "Step  50400: eval  CrossEntropyLoss |  4.99547100\n",
            "Step  50400: eval          Accuracy |  0.24880384\n",
            "\n",
            "Step  50500: Ran 100 train steps in 59.16 secs\n",
            "Step  50500: train CrossEntropyLoss |  5.26380157\n",
            "Step  50500: eval  CrossEntropyLoss |  5.16798353\n",
            "Step  50500: eval          Accuracy |  0.22314049\n",
            "\n",
            "Step  50600: Ran 100 train steps in 59.95 secs\n",
            "Step  50600: train CrossEntropyLoss |  5.21177053\n",
            "Step  50600: eval  CrossEntropyLoss |  5.45502138\n",
            "Step  50600: eval          Accuracy |  0.20202020\n",
            "\n",
            "Step  50700: Ran 100 train steps in 60.94 secs\n",
            "Step  50700: train CrossEntropyLoss |  5.22589874\n",
            "Step  50700: eval  CrossEntropyLoss |  4.92474127\n",
            "Step  50700: eval          Accuracy |  0.23300971\n",
            "\n",
            "Step  50800: Ran 100 train steps in 59.07 secs\n",
            "Step  50800: train CrossEntropyLoss |  5.21334743\n",
            "Step  50800: eval  CrossEntropyLoss |  5.51003981\n",
            "Step  50800: eval          Accuracy |  0.18316831\n",
            "\n",
            "Step  50900: Ran 100 train steps in 60.37 secs\n",
            "Step  50900: train CrossEntropyLoss |  5.21941519\n",
            "Step  50900: eval  CrossEntropyLoss |  5.12209940\n",
            "Step  50900: eval          Accuracy |  0.23893805\n",
            "\n",
            "Step  51000: Ran 100 train steps in 59.71 secs\n",
            "Step  51000: train CrossEntropyLoss |  5.18912792\n",
            "Step  51000: eval  CrossEntropyLoss |  5.43505335\n",
            "Step  51000: eval          Accuracy |  0.19491525\n",
            "\n",
            "Step  51100: Ran 100 train steps in 59.86 secs\n",
            "Step  51100: train CrossEntropyLoss |  5.19835663\n",
            "Step  51100: eval  CrossEntropyLoss |  5.00079679\n",
            "Step  51100: eval          Accuracy |  0.17094018\n",
            "\n",
            "Step  51200: Ran 100 train steps in 59.88 secs\n",
            "Step  51200: train CrossEntropyLoss |  5.19516182\n",
            "Step  51200: eval  CrossEntropyLoss |  5.29936981\n",
            "Step  51200: eval          Accuracy |  0.22270742\n",
            "\n",
            "Step  51300: Ran 100 train steps in 60.07 secs\n",
            "Step  51300: train CrossEntropyLoss |  5.18847322\n",
            "Step  51300: eval  CrossEntropyLoss |  4.76509905\n",
            "Step  51300: eval          Accuracy |  0.27619049\n",
            "\n",
            "Step  51400: Ran 100 train steps in 59.09 secs\n",
            "Step  51400: train CrossEntropyLoss |  5.17970085\n",
            "Step  51400: eval  CrossEntropyLoss |  4.98431444\n",
            "Step  51400: eval          Accuracy |  0.28235295\n",
            "\n",
            "Step  51500: Ran 100 train steps in 59.41 secs\n",
            "Step  51500: train CrossEntropyLoss |  5.24535751\n",
            "Step  51500: eval  CrossEntropyLoss |  5.18533945\n",
            "Step  51500: eval          Accuracy |  0.21782178\n",
            "\n",
            "Step  51600: Ran 100 train steps in 59.39 secs\n",
            "Step  51600: train CrossEntropyLoss |  5.17084694\n",
            "Step  51600: eval  CrossEntropyLoss |  5.06859636\n",
            "Step  51600: eval          Accuracy |  0.20114942\n",
            "\n",
            "Step  51700: Ran 100 train steps in 58.89 secs\n",
            "Step  51700: train CrossEntropyLoss |  5.12185574\n",
            "Step  51700: eval  CrossEntropyLoss |  5.76475716\n",
            "Step  51700: eval          Accuracy |  0.14705883\n",
            "\n",
            "Step  51800: Ran 100 train steps in 59.78 secs\n",
            "Step  51800: train CrossEntropyLoss |  5.17106390\n",
            "Step  51800: eval  CrossEntropyLoss |  5.11069822\n",
            "Step  51800: eval          Accuracy |  0.25892860\n",
            "\n",
            "Step  51900: Ran 100 train steps in 59.17 secs\n",
            "Step  51900: train CrossEntropyLoss |  5.16027451\n",
            "Step  51900: eval  CrossEntropyLoss |  4.79450655\n",
            "Step  51900: eval          Accuracy |  0.22807017\n",
            "\n",
            "Step  52000: Ran 100 train steps in 58.90 secs\n",
            "Step  52000: train CrossEntropyLoss |  5.21281004\n",
            "Step  52000: eval  CrossEntropyLoss |  5.21599579\n",
            "Step  52000: eval          Accuracy |  0.23684211\n",
            "\n",
            "Step  52100: Ran 100 train steps in 59.28 secs\n",
            "Step  52100: train CrossEntropyLoss |  5.18292093\n",
            "Step  52100: eval  CrossEntropyLoss |  5.38953114\n",
            "Step  52100: eval          Accuracy |  0.17307693\n",
            "\n",
            "Step  52200: Ran 100 train steps in 58.90 secs\n",
            "Step  52200: train CrossEntropyLoss |  5.21516705\n",
            "Step  52200: eval  CrossEntropyLoss |  5.53191948\n",
            "Step  52200: eval          Accuracy |  0.17355371\n",
            "\n",
            "Step  52300: Ran 100 train steps in 59.04 secs\n",
            "Step  52300: train CrossEntropyLoss |  5.26894093\n",
            "Step  52300: eval  CrossEntropyLoss |  5.96730518\n",
            "Step  52300: eval          Accuracy |  0.12500000\n",
            "\n",
            "Step  52400: Ran 100 train steps in 58.92 secs\n",
            "Step  52400: train CrossEntropyLoss |  5.22868967\n",
            "Step  52400: eval  CrossEntropyLoss |  4.74906206\n",
            "Step  52400: eval          Accuracy |  0.21917810\n",
            "\n",
            "Step  52500: Ran 100 train steps in 59.11 secs\n",
            "Step  52500: train CrossEntropyLoss |  5.24240589\n",
            "Step  52500: eval  CrossEntropyLoss |  5.04027748\n",
            "Step  52500: eval          Accuracy |  0.25287357\n",
            "\n",
            "Step  52600: Ran 100 train steps in 59.61 secs\n",
            "Step  52600: train CrossEntropyLoss |  5.14137983\n",
            "Step  52600: eval  CrossEntropyLoss |  5.23642397\n",
            "Step  52600: eval          Accuracy |  0.19298247\n",
            "\n",
            "Step  52700: Ran 100 train steps in 58.88 secs\n",
            "Step  52700: train CrossEntropyLoss |  5.17459631\n",
            "Step  52700: eval  CrossEntropyLoss |  5.18821001\n",
            "Step  52700: eval          Accuracy |  0.18032788\n",
            "\n",
            "Step  52800: Ran 100 train steps in 58.95 secs\n",
            "Step  52800: train CrossEntropyLoss |  5.15118933\n",
            "Step  52800: eval  CrossEntropyLoss |  4.97044182\n",
            "Step  52800: eval          Accuracy |  0.25233644\n",
            "\n",
            "Step  52900: Ran 100 train steps in 59.47 secs\n",
            "Step  52900: train CrossEntropyLoss |  5.17404413\n",
            "Step  52900: eval  CrossEntropyLoss |  4.87044716\n",
            "Step  52900: eval          Accuracy |  0.26203209\n",
            "\n",
            "Step  53000: Ran 100 train steps in 58.79 secs\n",
            "Step  53000: train CrossEntropyLoss |  5.19375467\n",
            "Step  53000: eval  CrossEntropyLoss |  4.95087719\n",
            "Step  53000: eval          Accuracy |  0.26086956\n",
            "\n",
            "Step  53100: Ran 100 train steps in 59.35 secs\n",
            "Step  53100: train CrossEntropyLoss |  5.15625286\n",
            "Step  53100: eval  CrossEntropyLoss |  5.41414881\n",
            "Step  53100: eval          Accuracy |  0.25490198\n",
            "\n",
            "Step  53200: Ran 100 train steps in 59.33 secs\n",
            "Step  53200: train CrossEntropyLoss |  5.24088717\n",
            "Step  53200: eval  CrossEntropyLoss |  5.21443129\n",
            "Step  53200: eval          Accuracy |  0.16666667\n",
            "\n",
            "Step  53300: Ran 100 train steps in 58.94 secs\n",
            "Step  53300: train CrossEntropyLoss |  5.12961102\n",
            "Step  53300: eval  CrossEntropyLoss |  5.40900803\n",
            "Step  53300: eval          Accuracy |  0.20833334\n",
            "\n",
            "Step  53400: Ran 100 train steps in 58.94 secs\n",
            "Step  53400: train CrossEntropyLoss |  5.17595243\n",
            "Step  53400: eval  CrossEntropyLoss |  5.10936356\n",
            "Step  53400: eval          Accuracy |  0.21904762\n",
            "\n",
            "Step  53500: Ran 100 train steps in 59.20 secs\n",
            "Step  53500: train CrossEntropyLoss |  5.18627024\n",
            "Step  53500: eval  CrossEntropyLoss |  4.97266722\n",
            "Step  53500: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  53600: Ran 100 train steps in 58.78 secs\n",
            "Step  53600: train CrossEntropyLoss |  5.23297310\n",
            "Step  53600: eval  CrossEntropyLoss |  5.00601864\n",
            "Step  53600: eval          Accuracy |  0.20526317\n",
            "\n",
            "Step  53700: Ran 100 train steps in 59.04 secs\n",
            "Step  53700: train CrossEntropyLoss |  5.12631035\n",
            "Step  53700: eval  CrossEntropyLoss |  5.04153872\n",
            "Step  53700: eval          Accuracy |  0.19166668\n",
            "\n",
            "Step  53800: Ran 100 train steps in 59.25 secs\n",
            "Step  53800: train CrossEntropyLoss |  5.14102364\n",
            "Step  53800: eval  CrossEntropyLoss |  4.98260498\n",
            "Step  53800: eval          Accuracy |  0.17757009\n",
            "\n",
            "Step  53900: Ran 100 train steps in 58.68 secs\n",
            "Step  53900: train CrossEntropyLoss |  5.15155888\n",
            "Step  53900: eval  CrossEntropyLoss |  4.85418701\n",
            "Step  53900: eval          Accuracy |  0.23267326\n",
            "\n",
            "Step  54000: Ran 100 train steps in 59.88 secs\n",
            "Step  54000: train CrossEntropyLoss |  5.20327950\n",
            "Step  54000: eval  CrossEntropyLoss |  5.43829823\n",
            "Step  54000: eval          Accuracy |  0.19230770\n",
            "\n",
            "Step  54100: Ran 100 train steps in 59.63 secs\n",
            "Step  54100: train CrossEntropyLoss |  5.17521572\n",
            "Step  54100: eval  CrossEntropyLoss |  5.15479565\n",
            "Step  54100: eval          Accuracy |  0.20999999\n",
            "\n",
            "Step  54200: Ran 100 train steps in 59.39 secs\n",
            "Step  54200: train CrossEntropyLoss |  5.14825487\n",
            "Step  54200: eval  CrossEntropyLoss |  5.51058054\n",
            "Step  54200: eval          Accuracy |  0.18260869\n",
            "\n",
            "Step  54300: Ran 100 train steps in 59.08 secs\n",
            "Step  54300: train CrossEntropyLoss |  5.20445156\n",
            "Step  54300: eval  CrossEntropyLoss |  4.49022484\n",
            "Step  54300: eval          Accuracy |  0.27374303\n",
            "\n",
            "Step  54400: Ran 100 train steps in 60.02 secs\n",
            "Step  54400: train CrossEntropyLoss |  5.12412214\n",
            "Step  54400: eval  CrossEntropyLoss |  4.74110460\n",
            "Step  54400: eval          Accuracy |  0.28431374\n",
            "\n",
            "Step  54500: Ran 100 train steps in 58.88 secs\n",
            "Step  54500: train CrossEntropyLoss |  5.18463421\n",
            "Step  54500: eval  CrossEntropyLoss |  5.35041714\n",
            "Step  54500: eval          Accuracy |  0.23076925\n",
            "\n",
            "Step  54600: Ran 100 train steps in 59.54 secs\n",
            "Step  54600: train CrossEntropyLoss |  5.13678074\n",
            "Step  54600: eval  CrossEntropyLoss |  4.93874884\n",
            "Step  54600: eval          Accuracy |  0.24324325\n",
            "\n",
            "Step  54700: Ran 100 train steps in 59.28 secs\n",
            "Step  54700: train CrossEntropyLoss |  5.12722301\n",
            "Step  54700: eval  CrossEntropyLoss |  5.50760078\n",
            "Step  54700: eval          Accuracy |  0.15789475\n",
            "\n",
            "Step  54800: Ran 100 train steps in 59.04 secs\n",
            "Step  54800: train CrossEntropyLoss |  5.14142561\n",
            "Step  54800: eval  CrossEntropyLoss |  5.57410955\n",
            "Step  54800: eval          Accuracy |  0.18257262\n",
            "\n",
            "Step  54900: Ran 100 train steps in 59.28 secs\n",
            "Step  54900: train CrossEntropyLoss |  5.10557461\n",
            "Step  54900: eval  CrossEntropyLoss |  4.81159449\n",
            "Step  54900: eval          Accuracy |  0.26785716\n",
            "\n",
            "Step  55000: Ran 100 train steps in 59.15 secs\n",
            "Step  55000: train CrossEntropyLoss |  5.09849405\n",
            "Step  55000: eval  CrossEntropyLoss |  5.14741421\n",
            "Step  55000: eval          Accuracy |  0.25961539\n",
            "\n",
            "Step  55100: Ran 100 train steps in 59.28 secs\n",
            "Step  55100: train CrossEntropyLoss |  5.12509584\n",
            "Step  55100: eval  CrossEntropyLoss |  5.07071352\n",
            "Step  55100: eval          Accuracy |  0.25641027\n",
            "\n",
            "Step  55200: Ran 100 train steps in 58.19 secs\n",
            "Step  55200: train CrossEntropyLoss |  5.11709642\n",
            "Step  55200: eval  CrossEntropyLoss |  5.33039045\n",
            "Step  55200: eval          Accuracy |  0.22834645\n",
            "\n",
            "Step  55300: Ran 100 train steps in 58.82 secs\n",
            "Step  55300: train CrossEntropyLoss |  5.11996651\n",
            "Step  55300: eval  CrossEntropyLoss |  5.21191311\n",
            "Step  55300: eval          Accuracy |  0.20388350\n",
            "\n",
            "Step  55400: Ran 100 train steps in 58.80 secs\n",
            "Step  55400: train CrossEntropyLoss |  5.13567019\n",
            "Step  55400: eval  CrossEntropyLoss |  4.86708832\n",
            "Step  55400: eval          Accuracy |  0.24468084\n",
            "\n",
            "Step  55500: Ran 100 train steps in 58.76 secs\n",
            "Step  55500: train CrossEntropyLoss |  5.13461733\n",
            "Step  55500: eval  CrossEntropyLoss |  5.16873837\n",
            "Step  55500: eval          Accuracy |  0.23423424\n",
            "\n",
            "Step  55600: Ran 100 train steps in 58.51 secs\n",
            "Step  55600: train CrossEntropyLoss |  5.09110928\n",
            "Step  55600: eval  CrossEntropyLoss |  5.54480314\n",
            "Step  55600: eval          Accuracy |  0.18103448\n",
            "\n",
            "Step  55700: Ran 100 train steps in 58.33 secs\n",
            "Step  55700: train CrossEntropyLoss |  5.14558506\n",
            "Step  55700: eval  CrossEntropyLoss |  4.79104662\n",
            "Step  55700: eval          Accuracy |  0.24137931\n",
            "\n",
            "Step  55800: Ran 100 train steps in 58.44 secs\n",
            "Step  55800: train CrossEntropyLoss |  5.07296228\n",
            "Step  55800: eval  CrossEntropyLoss |  5.25532913\n",
            "Step  55800: eval          Accuracy |  0.19801980\n",
            "\n",
            "Step  55900: Ran 100 train steps in 58.22 secs\n",
            "Step  55900: train CrossEntropyLoss |  5.15609789\n",
            "Step  55900: eval  CrossEntropyLoss |  4.69477415\n",
            "Step  55900: eval          Accuracy |  0.21698114\n",
            "\n",
            "Step  56000: Ran 100 train steps in 59.05 secs\n",
            "Step  56000: train CrossEntropyLoss |  5.07164001\n",
            "Step  56000: eval  CrossEntropyLoss |  5.17599487\n",
            "Step  56000: eval          Accuracy |  0.22999999\n",
            "\n",
            "Step  56100: Ran 100 train steps in 58.41 secs\n",
            "Step  56100: train CrossEntropyLoss |  5.10261202\n",
            "Step  56100: eval  CrossEntropyLoss |  4.86279774\n",
            "Step  56100: eval          Accuracy |  0.21153846\n",
            "\n",
            "Step  56200: Ran 100 train steps in 58.48 secs\n",
            "Step  56200: train CrossEntropyLoss |  5.16009045\n",
            "Step  56200: eval  CrossEntropyLoss |  4.89523840\n",
            "Step  56200: eval          Accuracy |  0.22651935\n",
            "\n",
            "Step  56300: Ran 100 train steps in 58.34 secs\n",
            "Step  56300: train CrossEntropyLoss |  5.14373875\n",
            "Step  56300: eval  CrossEntropyLoss |  4.79142857\n",
            "Step  56300: eval          Accuracy |  0.25714287\n",
            "\n",
            "Step  56400: Ran 100 train steps in 58.62 secs\n",
            "Step  56400: train CrossEntropyLoss |  5.09108829\n",
            "Step  56400: eval  CrossEntropyLoss |  5.09062243\n",
            "Step  56400: eval          Accuracy |  0.25252524\n",
            "\n",
            "Step  56500: Ran 100 train steps in 57.88 secs\n",
            "Step  56500: train CrossEntropyLoss |  5.15323925\n",
            "Step  56500: eval  CrossEntropyLoss |  4.93122387\n",
            "Step  56500: eval          Accuracy |  0.28729284\n",
            "\n",
            "Step  56600: Ran 100 train steps in 57.88 secs\n",
            "Step  56600: train CrossEntropyLoss |  5.09878492\n",
            "Step  56600: eval  CrossEntropyLoss |  5.65381384\n",
            "Step  56600: eval          Accuracy |  0.16363636\n",
            "\n",
            "Step  56700: Ran 100 train steps in 58.43 secs\n",
            "Step  56700: train CrossEntropyLoss |  5.17583656\n",
            "Step  56700: eval  CrossEntropyLoss |  5.30878592\n",
            "Step  56700: eval          Accuracy |  0.22222224\n",
            "\n",
            "Step  56800: Ran 100 train steps in 58.33 secs\n",
            "Step  56800: train CrossEntropyLoss |  5.10942078\n",
            "Step  56800: eval  CrossEntropyLoss |  4.90625906\n",
            "Step  56800: eval          Accuracy |  0.21359223\n",
            "\n",
            "Step  56900: Ran 100 train steps in 57.81 secs\n",
            "Step  56900: train CrossEntropyLoss |  5.11400032\n",
            "Step  56900: eval  CrossEntropyLoss |  4.87870216\n",
            "Step  56900: eval          Accuracy |  0.23737374\n",
            "\n",
            "Step  57000: Ran 100 train steps in 58.34 secs\n",
            "Step  57000: train CrossEntropyLoss |  5.06528234\n",
            "Step  57000: eval  CrossEntropyLoss |  4.74312305\n",
            "Step  57000: eval          Accuracy |  0.26356590\n",
            "\n",
            "Step  57100: Ran 100 train steps in 58.02 secs\n",
            "Step  57100: train CrossEntropyLoss |  5.09914351\n",
            "Step  57100: eval  CrossEntropyLoss |  5.33506775\n",
            "Step  57100: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  57200: Ran 100 train steps in 57.86 secs\n",
            "Step  57200: train CrossEntropyLoss |  5.12827826\n",
            "Step  57200: eval  CrossEntropyLoss |  5.16187000\n",
            "Step  57200: eval          Accuracy |  0.17999999\n",
            "\n",
            "Step  57300: Ran 100 train steps in 58.46 secs\n",
            "Step  57300: train CrossEntropyLoss |  5.10239744\n",
            "Step  57300: eval  CrossEntropyLoss |  5.16984367\n",
            "Step  57300: eval          Accuracy |  0.19459459\n",
            "\n",
            "Step  57400: Ran 100 train steps in 58.17 secs\n",
            "Step  57400: train CrossEntropyLoss |  5.07926798\n",
            "Step  57400: eval  CrossEntropyLoss |  5.22751904\n",
            "Step  57400: eval          Accuracy |  0.17699115\n",
            "\n",
            "Step  57500: Ran 100 train steps in 58.52 secs\n",
            "Step  57500: train CrossEntropyLoss |  5.08355427\n",
            "Step  57500: eval  CrossEntropyLoss |  4.96400404\n",
            "Step  57500: eval          Accuracy |  0.22033899\n",
            "\n",
            "Step  57600: Ran 100 train steps in 58.05 secs\n",
            "Step  57600: train CrossEntropyLoss |  5.07130766\n",
            "Step  57600: eval  CrossEntropyLoss |  4.61846924\n",
            "Step  57600: eval          Accuracy |  0.29357797\n",
            "\n",
            "Step  57700: Ran 100 train steps in 58.74 secs\n",
            "Step  57700: train CrossEntropyLoss |  5.04574823\n",
            "Step  57700: eval  CrossEntropyLoss |  5.07401037\n",
            "Step  57700: eval          Accuracy |  0.21962616\n",
            "\n",
            "Step  57800: Ran 100 train steps in 58.77 secs\n",
            "Step  57800: train CrossEntropyLoss |  5.10497665\n",
            "Step  57800: eval  CrossEntropyLoss |  5.34797668\n",
            "Step  57800: eval          Accuracy |  0.17999999\n",
            "\n",
            "Step  57900: Ran 100 train steps in 58.62 secs\n",
            "Step  57900: train CrossEntropyLoss |  5.11199665\n",
            "Step  57900: eval  CrossEntropyLoss |  4.59077835\n",
            "Step  57900: eval          Accuracy |  0.27472529\n",
            "\n",
            "Step  58000: Ran 100 train steps in 58.74 secs\n",
            "Step  58000: train CrossEntropyLoss |  5.07213306\n",
            "Step  58000: eval  CrossEntropyLoss |  4.90018082\n",
            "Step  58000: eval          Accuracy |  0.23529413\n",
            "\n",
            "Step  58100: Ran 100 train steps in 58.14 secs\n",
            "Step  58100: train CrossEntropyLoss |  5.03017712\n",
            "Step  58100: eval  CrossEntropyLoss |  5.19137049\n",
            "Step  58100: eval          Accuracy |  0.19469026\n",
            "\n",
            "Step  58200: Ran 100 train steps in 58.48 secs\n",
            "Step  58200: train CrossEntropyLoss |  5.04130125\n",
            "Step  58200: eval  CrossEntropyLoss |  4.46556377\n",
            "Step  58200: eval          Accuracy |  0.25988701\n",
            "\n",
            "Step  58300: Ran 100 train steps in 58.28 secs\n",
            "Step  58300: train CrossEntropyLoss |  5.08171415\n",
            "Step  58300: eval  CrossEntropyLoss |  4.77927113\n",
            "Step  58300: eval          Accuracy |  0.28865981\n",
            "\n",
            "Step  58400: Ran 100 train steps in 58.22 secs\n",
            "Step  58400: train CrossEntropyLoss |  5.06009388\n",
            "Step  58400: eval  CrossEntropyLoss |  4.98059559\n",
            "Step  58400: eval          Accuracy |  0.25125629\n",
            "\n",
            "Step  58500: Ran 100 train steps in 58.24 secs\n",
            "Step  58500: train CrossEntropyLoss |  5.10331774\n",
            "Step  58500: eval  CrossEntropyLoss |  5.18724775\n",
            "Step  58500: eval          Accuracy |  0.20833334\n",
            "\n",
            "Step  58600: Ran 100 train steps in 58.16 secs\n",
            "Step  58600: train CrossEntropyLoss |  5.05424070\n",
            "Step  58600: eval  CrossEntropyLoss |  5.15411377\n",
            "Step  58600: eval          Accuracy |  0.21276595\n",
            "\n",
            "Step  58700: Ran 100 train steps in 58.34 secs\n",
            "Step  58700: train CrossEntropyLoss |  5.08742857\n",
            "Step  58700: eval  CrossEntropyLoss |  5.18478823\n",
            "Step  58700: eval          Accuracy |  0.21348315\n",
            "\n",
            "Step  58800: Ran 100 train steps in 58.30 secs\n",
            "Step  58800: train CrossEntropyLoss |  5.02395821\n",
            "Step  58800: eval  CrossEntropyLoss |  5.47352505\n",
            "Step  58800: eval          Accuracy |  0.14678898\n",
            "\n",
            "Step  58900: Ran 100 train steps in 58.86 secs\n",
            "Step  58900: train CrossEntropyLoss |  5.08882427\n",
            "Step  58900: eval  CrossEntropyLoss |  5.27339172\n",
            "Step  58900: eval          Accuracy |  0.21359223\n",
            "\n",
            "Step  59000: Ran 100 train steps in 58.20 secs\n",
            "Step  59000: train CrossEntropyLoss |  5.03319407\n",
            "Step  59000: eval  CrossEntropyLoss |  4.88220644\n",
            "Step  59000: eval          Accuracy |  0.25388601\n",
            "\n",
            "Step  59100: Ran 100 train steps in 58.16 secs\n",
            "Step  59100: train CrossEntropyLoss |  5.03519344\n",
            "Step  59100: eval  CrossEntropyLoss |  5.13630962\n",
            "Step  59100: eval          Accuracy |  0.17094018\n",
            "\n",
            "Step  59200: Ran 100 train steps in 58.19 secs\n",
            "Step  59200: train CrossEntropyLoss |  5.07241774\n",
            "Step  59200: eval  CrossEntropyLoss |  4.53314829\n",
            "Step  59200: eval          Accuracy |  0.24761906\n",
            "\n",
            "Step  59300: Ran 100 train steps in 58.62 secs\n",
            "Step  59300: train CrossEntropyLoss |  5.08196354\n",
            "Step  59300: eval  CrossEntropyLoss |  4.84743357\n",
            "Step  59300: eval          Accuracy |  0.23853210\n",
            "\n",
            "Step  59400: Ran 100 train steps in 58.15 secs\n",
            "Step  59400: train CrossEntropyLoss |  5.04496288\n",
            "Step  59400: eval  CrossEntropyLoss |  4.79842091\n",
            "Step  59400: eval          Accuracy |  0.24107143\n",
            "\n",
            "Step  59500: Ran 100 train steps in 58.42 secs\n",
            "Step  59500: train CrossEntropyLoss |  5.06222630\n",
            "Step  59500: eval  CrossEntropyLoss |  5.18674517\n",
            "Step  59500: eval          Accuracy |  0.23584905\n",
            "\n",
            "Step  59600: Ran 100 train steps in 58.69 secs\n",
            "Step  59600: train CrossEntropyLoss |  5.10419941\n",
            "Step  59600: eval  CrossEntropyLoss |  4.92908239\n",
            "Step  59600: eval          Accuracy |  0.26315790\n",
            "\n",
            "Step  59700: Ran 100 train steps in 58.36 secs\n",
            "Step  59700: train CrossEntropyLoss |  5.01159954\n",
            "Step  59700: eval  CrossEntropyLoss |  5.27385044\n",
            "Step  59700: eval          Accuracy |  0.21917810\n",
            "\n",
            "Step  59800: Ran 100 train steps in 58.51 secs\n",
            "Step  59800: train CrossEntropyLoss |  5.01906252\n",
            "Step  59800: eval  CrossEntropyLoss |  5.18509007\n",
            "Step  59800: eval          Accuracy |  0.20183486\n",
            "\n",
            "Step  59900: Ran 100 train steps in 58.53 secs\n",
            "Step  59900: train CrossEntropyLoss |  5.02927446\n",
            "Step  59900: eval  CrossEntropyLoss |  4.88319349\n",
            "Step  59900: eval          Accuracy |  0.29213482\n",
            "\n",
            "Step  60000: Ran 100 train steps in 59.05 secs\n",
            "Step  60000: train CrossEntropyLoss |  5.03127718\n",
            "Step  60000: eval  CrossEntropyLoss |  5.10049057\n",
            "Step  60000: eval          Accuracy |  0.23076925\n",
            "\n",
            "Step  60100: Ran 100 train steps in 59.56 secs\n",
            "Step  60100: train CrossEntropyLoss |  5.01573324\n",
            "Step  60100: eval  CrossEntropyLoss |  4.48251247\n",
            "Step  60100: eval          Accuracy |  0.30097088\n",
            "\n",
            "Step  60200: Ran 100 train steps in 59.10 secs\n",
            "Step  60200: train CrossEntropyLoss |  5.00584173\n",
            "Step  60200: eval  CrossEntropyLoss |  5.99409008\n",
            "Step  60200: eval          Accuracy |  0.12605043\n",
            "\n",
            "Step  60300: Ran 100 train steps in 59.64 secs\n",
            "Step  60300: train CrossEntropyLoss |  4.99195528\n",
            "Step  60300: eval  CrossEntropyLoss |  5.27836800\n",
            "Step  60300: eval          Accuracy |  0.17525774\n",
            "\n",
            "Step  60400: Ran 100 train steps in 59.37 secs\n",
            "Step  60400: train CrossEntropyLoss |  5.01590490\n",
            "Step  60400: eval  CrossEntropyLoss |  5.21135712\n",
            "Step  60400: eval          Accuracy |  0.21359223\n",
            "\n",
            "Step  60500: Ran 100 train steps in 60.07 secs\n",
            "Step  60500: train CrossEntropyLoss |  5.06839275\n",
            "Step  60500: eval  CrossEntropyLoss |  5.54825163\n",
            "Step  60500: eval          Accuracy |  0.24038462\n",
            "\n",
            "Step  60600: Ran 100 train steps in 58.63 secs\n",
            "Step  60600: train CrossEntropyLoss |  5.04187107\n",
            "Step  60600: eval  CrossEntropyLoss |  5.14744520\n",
            "Step  60600: eval          Accuracy |  0.23423424\n",
            "\n",
            "Step  60700: Ran 100 train steps in 59.11 secs\n",
            "Step  60700: train CrossEntropyLoss |  4.95342779\n",
            "Step  60700: eval  CrossEntropyLoss |  4.37713146\n",
            "Step  60700: eval          Accuracy |  0.26470590\n",
            "\n",
            "Step  60800: Ran 100 train steps in 58.91 secs\n",
            "Step  60800: train CrossEntropyLoss |  5.02491093\n",
            "Step  60800: eval  CrossEntropyLoss |  4.63497829\n",
            "Step  60800: eval          Accuracy |  0.29411766\n",
            "\n",
            "Step  60900: Ran 100 train steps in 58.46 secs\n",
            "Step  60900: train CrossEntropyLoss |  4.98687840\n",
            "Step  60900: eval  CrossEntropyLoss |  4.83998108\n",
            "Step  60900: eval          Accuracy |  0.22448979\n",
            "\n",
            "Step  61000: Ran 100 train steps in 58.24 secs\n",
            "Step  61000: train CrossEntropyLoss |  5.01392937\n",
            "Step  61000: eval  CrossEntropyLoss |  5.80234623\n",
            "Step  61000: eval          Accuracy |  0.13461539\n",
            "\n",
            "Step  61100: Ran 100 train steps in 58.27 secs\n",
            "Step  61100: train CrossEntropyLoss |  4.96973991\n",
            "Step  61100: eval  CrossEntropyLoss |  5.57891464\n",
            "Step  61100: eval          Accuracy |  0.17142858\n",
            "\n",
            "Step  61200: Ran 100 train steps in 58.51 secs\n",
            "Step  61200: train CrossEntropyLoss |  5.02270365\n",
            "Step  61200: eval  CrossEntropyLoss |  4.61897564\n",
            "Step  61200: eval          Accuracy |  0.25961539\n",
            "\n",
            "Step  61300: Ran 100 train steps in 58.47 secs\n",
            "Step  61300: train CrossEntropyLoss |  5.03845453\n",
            "Step  61300: eval  CrossEntropyLoss |  4.91170073\n",
            "Step  61300: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  61400: Ran 100 train steps in 58.65 secs\n",
            "Step  61400: train CrossEntropyLoss |  5.00141907\n",
            "Step  61400: eval  CrossEntropyLoss |  5.01453686\n",
            "Step  61400: eval          Accuracy |  0.20661156\n",
            "\n",
            "Step  61500: Ran 100 train steps in 57.98 secs\n",
            "Step  61500: train CrossEntropyLoss |  4.99035645\n",
            "Step  61500: eval  CrossEntropyLoss |  5.06211185\n",
            "Step  61500: eval          Accuracy |  0.23711342\n",
            "\n",
            "Step  61600: Ran 100 train steps in 58.16 secs\n",
            "Step  61600: train CrossEntropyLoss |  4.95577765\n",
            "Step  61600: eval  CrossEntropyLoss |  5.32535124\n",
            "Step  61600: eval          Accuracy |  0.16793893\n",
            "\n",
            "Step  61700: Ran 100 train steps in 58.66 secs\n",
            "Step  61700: train CrossEntropyLoss |  5.03250504\n",
            "Step  61700: eval  CrossEntropyLoss |  4.81784964\n",
            "Step  61700: eval          Accuracy |  0.25233644\n",
            "\n",
            "Step  61800: Ran 100 train steps in 57.98 secs\n",
            "Step  61800: train CrossEntropyLoss |  4.98771620\n",
            "Step  61800: eval  CrossEntropyLoss |  4.39973974\n",
            "Step  61800: eval          Accuracy |  0.29545456\n",
            "\n",
            "Step  61900: Ran 100 train steps in 58.19 secs\n",
            "Step  61900: train CrossEntropyLoss |  4.98430681\n",
            "Step  61900: eval  CrossEntropyLoss |  5.39247799\n",
            "Step  61900: eval          Accuracy |  0.18269232\n",
            "\n",
            "Step  62000: Ran 100 train steps in 58.05 secs\n",
            "Step  62000: train CrossEntropyLoss |  4.99208212\n",
            "Step  62000: eval  CrossEntropyLoss |  4.81585932\n",
            "Step  62000: eval          Accuracy |  0.23478259\n",
            "\n",
            "Step  62100: Ran 100 train steps in 58.02 secs\n",
            "Step  62100: train CrossEntropyLoss |  4.96251106\n",
            "Step  62100: eval  CrossEntropyLoss |  4.75851679\n",
            "Step  62100: eval          Accuracy |  0.23829786\n",
            "\n",
            "Step  62200: Ran 100 train steps in 58.01 secs\n",
            "Step  62200: train CrossEntropyLoss |  5.01085901\n",
            "Step  62200: eval  CrossEntropyLoss |  5.21806574\n",
            "Step  62200: eval          Accuracy |  0.18348622\n",
            "\n",
            "Step  62300: Ran 100 train steps in 58.22 secs\n",
            "Step  62300: train CrossEntropyLoss |  5.04063272\n",
            "Step  62300: eval  CrossEntropyLoss |  4.52936935\n",
            "Step  62300: eval          Accuracy |  0.25615764\n",
            "\n",
            "Step  62400: Ran 100 train steps in 58.53 secs\n",
            "Step  62400: train CrossEntropyLoss |  5.00202036\n",
            "Step  62400: eval  CrossEntropyLoss |  4.90470982\n",
            "Step  62400: eval          Accuracy |  0.29896909\n",
            "\n",
            "Step  62500: Ran 100 train steps in 58.04 secs\n",
            "Step  62500: train CrossEntropyLoss |  5.01477909\n",
            "Step  62500: eval  CrossEntropyLoss |  5.23318577\n",
            "Step  62500: eval          Accuracy |  0.19780220\n",
            "\n",
            "Step  62600: Ran 100 train steps in 58.61 secs\n",
            "Step  62600: train CrossEntropyLoss |  4.97267771\n",
            "Step  62600: eval  CrossEntropyLoss |  4.94049644\n",
            "Step  62600: eval          Accuracy |  0.18867925\n",
            "\n",
            "Step  62700: Ran 100 train steps in 57.81 secs\n",
            "Step  62700: train CrossEntropyLoss |  4.96717072\n",
            "Step  62700: eval  CrossEntropyLoss |  4.43279457\n",
            "Step  62700: eval          Accuracy |  0.25773197\n",
            "\n",
            "Step  62800: Ran 100 train steps in 58.17 secs\n",
            "Step  62800: train CrossEntropyLoss |  4.96163845\n",
            "Step  62800: eval  CrossEntropyLoss |  4.92565298\n",
            "Step  62800: eval          Accuracy |  0.21578948\n",
            "\n",
            "Step  62900: Ran 100 train steps in 58.75 secs\n",
            "Step  62900: train CrossEntropyLoss |  5.01498604\n",
            "Step  62900: eval  CrossEntropyLoss |  5.29475260\n",
            "Step  62900: eval          Accuracy |  0.23275863\n",
            "\n",
            "Step  63000: Ran 100 train steps in 58.56 secs\n",
            "Step  63000: train CrossEntropyLoss |  4.96187210\n",
            "Step  63000: eval  CrossEntropyLoss |  5.50563383\n",
            "Step  63000: eval          Accuracy |  0.18918920\n",
            "\n",
            "Step  63100: Ran 100 train steps in 58.00 secs\n",
            "Step  63100: train CrossEntropyLoss |  4.94801855\n",
            "Step  63100: eval  CrossEntropyLoss |  5.24339247\n",
            "Step  63100: eval          Accuracy |  0.19444445\n",
            "\n",
            "Step  63200: Ran 100 train steps in 57.92 secs\n",
            "Step  63200: train CrossEntropyLoss |  4.97396088\n",
            "Step  63200: eval  CrossEntropyLoss |  5.00784588\n",
            "Step  63200: eval          Accuracy |  0.21359223\n",
            "\n",
            "Step  63300: Ran 100 train steps in 58.18 secs\n",
            "Step  63300: train CrossEntropyLoss |  4.91159678\n",
            "Step  63300: eval  CrossEntropyLoss |  5.05556870\n",
            "Step  63300: eval          Accuracy |  0.23831774\n",
            "\n",
            "Step  63400: Ran 100 train steps in 58.74 secs\n",
            "Step  63400: train CrossEntropyLoss |  4.97884560\n",
            "Step  63400: eval  CrossEntropyLoss |  5.37023211\n",
            "Step  63400: eval          Accuracy |  0.18348622\n",
            "\n",
            "Step  63500: Ran 100 train steps in 58.04 secs\n",
            "Step  63500: train CrossEntropyLoss |  4.94916010\n",
            "Step  63500: eval  CrossEntropyLoss |  4.73120499\n",
            "Step  63500: eval          Accuracy |  0.26168224\n",
            "\n",
            "Step  63600: Ran 100 train steps in 57.89 secs\n",
            "Step  63600: train CrossEntropyLoss |  4.96462917\n",
            "Step  63600: eval  CrossEntropyLoss |  5.45040560\n",
            "Step  63600: eval          Accuracy |  0.23275863\n",
            "\n",
            "Step  63700: Ran 100 train steps in 58.21 secs\n",
            "Step  63700: train CrossEntropyLoss |  4.97521305\n",
            "Step  63700: eval  CrossEntropyLoss |  5.15115118\n",
            "Step  63700: eval          Accuracy |  0.20999999\n",
            "\n",
            "Step  63800: Ran 100 train steps in 58.39 secs\n",
            "Step  63800: train CrossEntropyLoss |  4.97566891\n",
            "Step  63800: eval  CrossEntropyLoss |  5.39030933\n",
            "Step  63800: eval          Accuracy |  0.21296297\n",
            "\n",
            "Step  63900: Ran 100 train steps in 57.79 secs\n",
            "Step  63900: train CrossEntropyLoss |  4.96889162\n",
            "Step  63900: eval  CrossEntropyLoss |  5.12105036\n",
            "Step  63900: eval          Accuracy |  0.22222224\n",
            "\n",
            "Step  64000: Ran 100 train steps in 58.03 secs\n",
            "Step  64000: train CrossEntropyLoss |  4.95082617\n",
            "Step  64000: eval  CrossEntropyLoss |  5.00406218\n",
            "Step  64000: eval          Accuracy |  0.27027029\n",
            "\n",
            "Step  64100: Ran 100 train steps in 58.00 secs\n",
            "Step  64100: train CrossEntropyLoss |  4.93747139\n",
            "Step  64100: eval  CrossEntropyLoss |  4.68624258\n",
            "Step  64100: eval          Accuracy |  0.24218750\n",
            "\n",
            "Step  64200: Ran 100 train steps in 58.52 secs\n",
            "Step  64200: train CrossEntropyLoss |  4.99975109\n",
            "Step  64200: eval  CrossEntropyLoss |  5.05609751\n",
            "Step  64200: eval          Accuracy |  0.25500000\n",
            "\n",
            "Step  64300: Ran 100 train steps in 58.40 secs\n",
            "Step  64300: train CrossEntropyLoss |  4.96259689\n",
            "Step  64300: eval  CrossEntropyLoss |  5.21529913\n",
            "Step  64300: eval          Accuracy |  0.25274727\n",
            "\n",
            "Step  64400: Ran 100 train steps in 58.67 secs\n",
            "Step  64400: train CrossEntropyLoss |  4.97625399\n",
            "Step  64400: eval  CrossEntropyLoss |  4.63440561\n",
            "Step  64400: eval          Accuracy |  0.26724139\n",
            "\n",
            "Step  64500: Ran 100 train steps in 58.09 secs\n",
            "Step  64500: train CrossEntropyLoss |  4.93031073\n",
            "Step  64500: eval  CrossEntropyLoss |  5.11830616\n",
            "Step  64500: eval          Accuracy |  0.23300971\n",
            "\n",
            "Step  64600: Ran 100 train steps in 58.81 secs\n",
            "Step  64600: train CrossEntropyLoss |  4.87801123\n",
            "Step  64600: eval  CrossEntropyLoss |  4.78710556\n",
            "Step  64600: eval          Accuracy |  0.25603864\n",
            "\n",
            "Step  64700: Ran 100 train steps in 58.47 secs\n",
            "Step  64700: train CrossEntropyLoss |  4.90204525\n",
            "Step  64700: eval  CrossEntropyLoss |  5.31220007\n",
            "Step  64700: eval          Accuracy |  0.23300971\n",
            "\n",
            "Step  64800: Ran 100 train steps in 58.09 secs\n",
            "Step  64800: train CrossEntropyLoss |  4.94579363\n",
            "Step  64800: eval  CrossEntropyLoss |  5.32315874\n",
            "Step  64800: eval          Accuracy |  0.24528302\n",
            "\n",
            "Step  64900: Ran 100 train steps in 58.00 secs\n",
            "Step  64900: train CrossEntropyLoss |  4.95095730\n",
            "Step  64900: eval  CrossEntropyLoss |  4.74223518\n",
            "Step  64900: eval          Accuracy |  0.26737967\n",
            "\n",
            "Step  65000: Ran 100 train steps in 58.74 secs\n",
            "Step  65000: train CrossEntropyLoss |  4.90972567\n",
            "Step  65000: eval  CrossEntropyLoss |  5.27697659\n",
            "Step  65000: eval          Accuracy |  0.20952381\n",
            "\n",
            "Step  65100: Ran 100 train steps in 58.48 secs\n",
            "Step  65100: train CrossEntropyLoss |  4.89211988\n",
            "Step  65100: eval  CrossEntropyLoss |  4.83098745\n",
            "Step  65100: eval          Accuracy |  0.23148148\n",
            "\n",
            "Step  65200: Ran 100 train steps in 58.50 secs\n",
            "Step  65200: train CrossEntropyLoss |  4.89691639\n",
            "Step  65200: eval  CrossEntropyLoss |  5.67434072\n",
            "Step  65200: eval          Accuracy |  0.17241380\n",
            "\n",
            "Step  65300: Ran 100 train steps in 58.47 secs\n",
            "Step  65300: train CrossEntropyLoss |  4.95958042\n",
            "Step  65300: eval  CrossEntropyLoss |  4.63666105\n",
            "Step  65300: eval          Accuracy |  0.25954199\n",
            "\n",
            "Step  65400: Ran 100 train steps in 58.44 secs\n",
            "Step  65400: train CrossEntropyLoss |  4.93806267\n",
            "Step  65400: eval  CrossEntropyLoss |  4.47130823\n",
            "Step  65400: eval          Accuracy |  0.25128207\n",
            "\n",
            "Step  65500: Ran 100 train steps in 58.29 secs\n",
            "Step  65500: train CrossEntropyLoss |  4.88592196\n",
            "Step  65500: eval  CrossEntropyLoss |  4.74311876\n",
            "Step  65500: eval          Accuracy |  0.30088496\n",
            "\n",
            "Step  65600: Ran 100 train steps in 57.88 secs\n",
            "Step  65600: train CrossEntropyLoss |  4.92422199\n",
            "Step  65600: eval  CrossEntropyLoss |  4.86556816\n",
            "Step  65600: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  65700: Ran 100 train steps in 58.35 secs\n",
            "Step  65700: train CrossEntropyLoss |  4.93331099\n",
            "Step  65700: eval  CrossEntropyLoss |  5.43914032\n",
            "Step  65700: eval          Accuracy |  0.14782608\n",
            "\n",
            "Step  65800: Ran 100 train steps in 57.59 secs\n",
            "Step  65800: train CrossEntropyLoss |  4.93141365\n",
            "Step  65800: eval  CrossEntropyLoss |  4.94438076\n",
            "Step  65800: eval          Accuracy |  0.17460318\n",
            "\n",
            "Step  65900: Ran 100 train steps in 58.26 secs\n",
            "Step  65900: train CrossEntropyLoss |  4.97520733\n",
            "Step  65900: eval  CrossEntropyLoss |  5.15163422\n",
            "Step  65900: eval          Accuracy |  0.23584905\n",
            "\n",
            "Step  66000: Ran 100 train steps in 57.84 secs\n",
            "Step  66000: train CrossEntropyLoss |  4.89228678\n",
            "Step  66000: eval  CrossEntropyLoss |  5.21858311\n",
            "Step  66000: eval          Accuracy |  0.20297030\n",
            "\n",
            "Step  66100: Ran 100 train steps in 57.88 secs\n",
            "Step  66100: train CrossEntropyLoss |  4.94307423\n",
            "Step  66100: eval  CrossEntropyLoss |  4.72865438\n",
            "Step  66100: eval          Accuracy |  0.33707866\n",
            "\n",
            "Step  66200: Ran 100 train steps in 58.35 secs\n",
            "Step  66200: train CrossEntropyLoss |  4.89603424\n",
            "Step  66200: eval  CrossEntropyLoss |  5.17529345\n",
            "Step  66200: eval          Accuracy |  0.23423424\n",
            "\n",
            "Step  66300: Ran 100 train steps in 57.85 secs\n",
            "Step  66300: train CrossEntropyLoss |  4.93398571\n",
            "Step  66300: eval  CrossEntropyLoss |  4.99236774\n",
            "Step  66300: eval          Accuracy |  0.21428573\n",
            "\n",
            "Step  66400: Ran 100 train steps in 58.37 secs\n",
            "Step  66400: train CrossEntropyLoss |  4.90552711\n",
            "Step  66400: eval  CrossEntropyLoss |  4.69209242\n",
            "Step  66400: eval          Accuracy |  0.22429906\n",
            "\n",
            "Step  66500: Ran 100 train steps in 58.43 secs\n",
            "Step  66500: train CrossEntropyLoss |  4.83801699\n",
            "Step  66500: eval  CrossEntropyLoss |  5.05234051\n",
            "Step  66500: eval          Accuracy |  0.14953271\n",
            "\n",
            "Step  66600: Ran 100 train steps in 59.29 secs\n",
            "Step  66600: train CrossEntropyLoss |  4.92522335\n",
            "Step  66600: eval  CrossEntropyLoss |  5.18837357\n",
            "Step  66600: eval          Accuracy |  0.25961539\n",
            "\n",
            "Step  66700: Ran 100 train steps in 60.79 secs\n",
            "Step  66700: train CrossEntropyLoss |  4.90792513\n",
            "Step  66700: eval  CrossEntropyLoss |  4.88026953\n",
            "Step  66700: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  66800: Ran 100 train steps in 60.53 secs\n",
            "Step  66800: train CrossEntropyLoss |  4.84703732\n",
            "Step  66800: eval  CrossEntropyLoss |  4.33471632\n",
            "Step  66800: eval          Accuracy |  0.24242425\n",
            "\n",
            "Step  66900: Ran 100 train steps in 59.86 secs\n",
            "Step  66900: train CrossEntropyLoss |  4.87589550\n",
            "Step  66900: eval  CrossEntropyLoss |  4.36112165\n",
            "Step  66900: eval          Accuracy |  0.29670331\n",
            "\n",
            "Step  67000: Ran 100 train steps in 60.29 secs\n",
            "Step  67000: train CrossEntropyLoss |  4.87348795\n",
            "Step  67000: eval  CrossEntropyLoss |  4.77035999\n",
            "Step  67000: eval          Accuracy |  0.21495326\n",
            "\n",
            "Step  67100: Ran 100 train steps in 60.63 secs\n",
            "Step  67100: train CrossEntropyLoss |  4.86301422\n",
            "Step  67100: eval  CrossEntropyLoss |  4.67343760\n",
            "Step  67100: eval          Accuracy |  0.32352942\n",
            "\n",
            "Step  67200: Ran 100 train steps in 60.47 secs\n",
            "Step  67200: train CrossEntropyLoss |  4.88400030\n",
            "Step  67200: eval  CrossEntropyLoss |  4.76852036\n",
            "Step  67200: eval          Accuracy |  0.24528302\n",
            "\n",
            "Step  67300: Ran 100 train steps in 59.99 secs\n",
            "Step  67300: train CrossEntropyLoss |  4.90466595\n",
            "Step  67300: eval  CrossEntropyLoss |  4.89880466\n",
            "Step  67300: eval          Accuracy |  0.19689120\n",
            "\n",
            "Step  67400: Ran 100 train steps in 59.56 secs\n",
            "Step  67400: train CrossEntropyLoss |  4.89009476\n",
            "Step  67400: eval  CrossEntropyLoss |  4.61344862\n",
            "Step  67400: eval          Accuracy |  0.28695652\n",
            "\n",
            "Step  67500: Ran 100 train steps in 61.12 secs\n",
            "Step  67500: train CrossEntropyLoss |  4.93342018\n",
            "Step  67500: eval  CrossEntropyLoss |  5.25841618\n",
            "Step  67500: eval          Accuracy |  0.18750000\n",
            "\n",
            "Step  67600: Ran 100 train steps in 60.09 secs\n",
            "Step  67600: train CrossEntropyLoss |  4.93110847\n",
            "Step  67600: eval  CrossEntropyLoss |  5.18562031\n",
            "Step  67600: eval          Accuracy |  0.25609756\n",
            "\n",
            "Step  67700: Ran 100 train steps in 60.27 secs\n",
            "Step  67700: train CrossEntropyLoss |  4.91760254\n",
            "Step  67700: eval  CrossEntropyLoss |  4.97831345\n",
            "Step  67700: eval          Accuracy |  0.25233644\n",
            "\n",
            "Step  67800: Ran 100 train steps in 60.10 secs\n",
            "Step  67800: train CrossEntropyLoss |  4.84179068\n",
            "Step  67800: eval  CrossEntropyLoss |  5.10864973\n",
            "Step  67800: eval          Accuracy |  0.23618090\n",
            "\n",
            "Step  67900: Ran 100 train steps in 59.89 secs\n",
            "Step  67900: train CrossEntropyLoss |  4.83141613\n",
            "Step  67900: eval  CrossEntropyLoss |  4.97413254\n",
            "Step  67900: eval          Accuracy |  0.16513760\n",
            "\n",
            "Step  68000: Ran 100 train steps in 59.74 secs\n",
            "Step  68000: train CrossEntropyLoss |  4.94826365\n",
            "Step  68000: eval  CrossEntropyLoss |  5.02114105\n",
            "Step  68000: eval          Accuracy |  0.21259843\n",
            "\n",
            "Step  68100: Ran 100 train steps in 60.12 secs\n",
            "Step  68100: train CrossEntropyLoss |  4.88630772\n",
            "Step  68100: eval  CrossEntropyLoss |  5.22993946\n",
            "Step  68100: eval          Accuracy |  0.22448979\n",
            "\n",
            "Step  68200: Ran 100 train steps in 59.69 secs\n",
            "Step  68200: train CrossEntropyLoss |  4.89241219\n",
            "Step  68200: eval  CrossEntropyLoss |  4.75686789\n",
            "Step  68200: eval          Accuracy |  0.22093023\n",
            "\n",
            "Step  68300: Ran 100 train steps in 60.31 secs\n",
            "Step  68300: train CrossEntropyLoss |  4.88111258\n",
            "Step  68300: eval  CrossEntropyLoss |  4.21112585\n",
            "Step  68300: eval          Accuracy |  0.29357797\n",
            "\n",
            "Step  68400: Ran 100 train steps in 59.71 secs\n",
            "Step  68400: train CrossEntropyLoss |  4.84779978\n",
            "Step  68400: eval  CrossEntropyLoss |  4.66002083\n",
            "Step  68400: eval          Accuracy |  0.18750000\n",
            "\n",
            "Step  68500: Ran 100 train steps in 60.07 secs\n",
            "Step  68500: train CrossEntropyLoss |  4.86755371\n",
            "Step  68500: eval  CrossEntropyLoss |  4.81095695\n",
            "Step  68500: eval          Accuracy |  0.23423424\n",
            "\n",
            "Step  68600: Ran 100 train steps in 59.68 secs\n",
            "Step  68600: train CrossEntropyLoss |  4.84987926\n",
            "Step  68600: eval  CrossEntropyLoss |  5.01778793\n",
            "Step  68600: eval          Accuracy |  0.22580644\n",
            "\n",
            "Step  68700: Ran 100 train steps in 59.68 secs\n",
            "Step  68700: train CrossEntropyLoss |  4.89177608\n",
            "Step  68700: eval  CrossEntropyLoss |  4.82390404\n",
            "Step  68700: eval          Accuracy |  0.26960784\n",
            "\n",
            "Step  68800: Ran 100 train steps in 59.42 secs\n",
            "Step  68800: train CrossEntropyLoss |  4.84057188\n",
            "Step  68800: eval  CrossEntropyLoss |  5.16884756\n",
            "Step  68800: eval          Accuracy |  0.24545453\n",
            "\n",
            "Step  68900: Ran 100 train steps in 59.52 secs\n",
            "Step  68900: train CrossEntropyLoss |  4.86978436\n",
            "Step  68900: eval  CrossEntropyLoss |  4.87057352\n",
            "Step  68900: eval          Accuracy |  0.21698114\n",
            "\n",
            "Step  69000: Ran 100 train steps in 59.43 secs\n",
            "Step  69000: train CrossEntropyLoss |  4.85607290\n",
            "Step  69000: eval  CrossEntropyLoss |  4.97479105\n",
            "Step  69000: eval          Accuracy |  0.23636363\n",
            "\n",
            "Step  69100: Ran 100 train steps in 59.64 secs\n",
            "Step  69100: train CrossEntropyLoss |  4.84646797\n",
            "Step  69100: eval  CrossEntropyLoss |  5.01584339\n",
            "Step  69100: eval          Accuracy |  0.22916667\n",
            "\n",
            "Step  69200: Ran 100 train steps in 60.15 secs\n",
            "Step  69200: train CrossEntropyLoss |  4.81534672\n",
            "Step  69200: eval  CrossEntropyLoss |  4.41871119\n",
            "Step  69200: eval          Accuracy |  0.26126125\n",
            "\n",
            "Step  69300: Ran 100 train steps in 59.96 secs\n",
            "Step  69300: train CrossEntropyLoss |  4.84916353\n",
            "Step  69300: eval  CrossEntropyLoss |  4.48944521\n",
            "Step  69300: eval          Accuracy |  0.26388890\n",
            "\n",
            "Step  69400: Ran 100 train steps in 59.97 secs\n",
            "Step  69400: train CrossEntropyLoss |  4.89555693\n",
            "Step  69400: eval  CrossEntropyLoss |  5.02263165\n",
            "Step  69400: eval          Accuracy |  0.20588236\n",
            "\n",
            "Step  69500: Ran 100 train steps in 59.96 secs\n",
            "Step  69500: train CrossEntropyLoss |  4.86746883\n",
            "Step  69500: eval  CrossEntropyLoss |  5.31950426\n",
            "Step  69500: eval          Accuracy |  0.24324325\n",
            "\n",
            "Step  69600: Ran 100 train steps in 59.48 secs\n",
            "Step  69600: train CrossEntropyLoss |  4.86338902\n",
            "Step  69600: eval  CrossEntropyLoss |  4.85287619\n",
            "Step  69600: eval          Accuracy |  0.26881722\n",
            "\n",
            "Step  69700: Ran 100 train steps in 59.79 secs\n",
            "Step  69700: train CrossEntropyLoss |  4.88259029\n",
            "Step  69700: eval  CrossEntropyLoss |  4.69354105\n",
            "Step  69700: eval          Accuracy |  0.29885057\n",
            "\n",
            "Step  69800: Ran 100 train steps in 59.83 secs\n",
            "Step  69800: train CrossEntropyLoss |  4.84625101\n",
            "Step  69800: eval  CrossEntropyLoss |  5.02736282\n",
            "Step  69800: eval          Accuracy |  0.22222224\n",
            "\n",
            "Step  69900: Ran 100 train steps in 60.00 secs\n",
            "Step  69900: train CrossEntropyLoss |  4.82550049\n",
            "Step  69900: eval  CrossEntropyLoss |  5.22682524\n",
            "Step  69900: eval          Accuracy |  0.21052632\n",
            "\n",
            "Step  70000: Ran 100 train steps in 60.15 secs\n",
            "Step  70000: train CrossEntropyLoss |  4.83845282\n",
            "Step  70000: eval  CrossEntropyLoss |  4.53237820\n",
            "Step  70000: eval          Accuracy |  0.29245284\n",
            "\n",
            "Step  70100: Ran 100 train steps in 59.64 secs\n",
            "Step  70100: train CrossEntropyLoss |  4.86745024\n",
            "Step  70100: eval  CrossEntropyLoss |  5.23923302\n",
            "Step  70100: eval          Accuracy |  0.19130434\n",
            "\n",
            "Step  70200: Ran 100 train steps in 60.27 secs\n",
            "Step  70200: train CrossEntropyLoss |  4.85375834\n",
            "Step  70200: eval  CrossEntropyLoss |  4.69035482\n",
            "Step  70200: eval          Accuracy |  0.25396827\n",
            "\n",
            "Step  70300: Ran 100 train steps in 59.80 secs\n",
            "Step  70300: train CrossEntropyLoss |  4.81606197\n",
            "Step  70300: eval  CrossEntropyLoss |  5.28570461\n",
            "Step  70300: eval          Accuracy |  0.19148935\n",
            "\n",
            "Step  70400: Ran 100 train steps in 59.79 secs\n",
            "Step  70400: train CrossEntropyLoss |  4.87765598\n",
            "Step  70400: eval  CrossEntropyLoss |  5.02162933\n",
            "Step  70400: eval          Accuracy |  0.22105265\n",
            "\n",
            "Step  70500: Ran 100 train steps in 59.60 secs\n",
            "Step  70500: train CrossEntropyLoss |  4.82124710\n",
            "Step  70500: eval  CrossEntropyLoss |  4.51316118\n",
            "Step  70500: eval          Accuracy |  0.31034482\n",
            "\n",
            "Step  70600: Ran 100 train steps in 60.13 secs\n",
            "Step  70600: train CrossEntropyLoss |  4.93571138\n",
            "Step  70600: eval  CrossEntropyLoss |  5.29101181\n",
            "Step  70600: eval          Accuracy |  0.18269232\n",
            "\n",
            "Step  70700: Ran 100 train steps in 59.65 secs\n",
            "Step  70700: train CrossEntropyLoss |  4.88809204\n",
            "Step  70700: eval  CrossEntropyLoss |  5.21002722\n",
            "Step  70700: eval          Accuracy |  0.15841584\n",
            "\n",
            "Step  70800: Ran 100 train steps in 59.45 secs\n",
            "Step  70800: train CrossEntropyLoss |  4.82579231\n",
            "Step  70800: eval  CrossEntropyLoss |  4.92546034\n",
            "Step  70800: eval          Accuracy |  0.25373134\n",
            "\n",
            "Step  70900: Ran 100 train steps in 59.91 secs\n",
            "Step  70900: train CrossEntropyLoss |  4.81551027\n",
            "Step  70900: eval  CrossEntropyLoss |  4.19024181\n",
            "Step  70900: eval          Accuracy |  0.30526316\n",
            "\n",
            "Step  71000: Ran 100 train steps in 60.11 secs\n",
            "Step  71000: train CrossEntropyLoss |  4.85984802\n",
            "Step  71000: eval  CrossEntropyLoss |  5.24215364\n",
            "Step  71000: eval          Accuracy |  0.23140495\n",
            "\n",
            "Step  71100: Ran 100 train steps in 59.49 secs\n",
            "Step  71100: train CrossEntropyLoss |  4.87579775\n",
            "Step  71100: eval  CrossEntropyLoss |  4.74742460\n",
            "Step  71100: eval          Accuracy |  0.25225225\n",
            "\n",
            "Step  71200: Ran 100 train steps in 59.49 secs\n",
            "Step  71200: train CrossEntropyLoss |  4.86644268\n",
            "Step  71200: eval  CrossEntropyLoss |  4.43458271\n",
            "Step  71200: eval          Accuracy |  0.25125629\n",
            "\n",
            "Step  71300: Ran 100 train steps in 59.51 secs\n",
            "Step  71300: train CrossEntropyLoss |  4.83157778\n",
            "Step  71300: eval  CrossEntropyLoss |  4.73814392\n",
            "Step  71300: eval          Accuracy |  0.21848741\n",
            "\n",
            "Step  71400: Ran 100 train steps in 60.09 secs\n",
            "Step  71400: train CrossEntropyLoss |  4.79741192\n",
            "Step  71400: eval  CrossEntropyLoss |  4.52634192\n",
            "Step  71400: eval          Accuracy |  0.22222222\n",
            "\n",
            "Step  71500: Ran 100 train steps in 59.49 secs\n",
            "Step  71500: train CrossEntropyLoss |  4.79893923\n",
            "Step  71500: eval  CrossEntropyLoss |  5.16438198\n",
            "Step  71500: eval          Accuracy |  0.24017468\n",
            "\n",
            "Step  71600: Ran 100 train steps in 59.57 secs\n",
            "Step  71600: train CrossEntropyLoss |  4.78992605\n",
            "Step  71600: eval  CrossEntropyLoss |  4.77933121\n",
            "Step  71600: eval          Accuracy |  0.22018348\n",
            "\n",
            "Step  71700: Ran 100 train steps in 59.56 secs\n",
            "Step  71700: train CrossEntropyLoss |  4.81725979\n",
            "Step  71700: eval  CrossEntropyLoss |  5.11857891\n",
            "Step  71700: eval          Accuracy |  0.29357797\n",
            "\n",
            "Step  71800: Ran 100 train steps in 60.04 secs\n",
            "Step  71800: train CrossEntropyLoss |  4.82976055\n",
            "Step  71800: eval  CrossEntropyLoss |  4.79288244\n",
            "Step  71800: eval          Accuracy |  0.22429906\n",
            "\n",
            "Step  71900: Ran 100 train steps in 59.84 secs\n",
            "Step  71900: train CrossEntropyLoss |  4.75787258\n",
            "Step  71900: eval  CrossEntropyLoss |  5.23475552\n",
            "Step  71900: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  72000: Ran 100 train steps in 59.57 secs\n",
            "Step  72000: train CrossEntropyLoss |  4.82022285\n",
            "Step  72000: eval  CrossEntropyLoss |  4.72784662\n",
            "Step  72000: eval          Accuracy |  0.26881722\n",
            "\n",
            "Step  72100: Ran 100 train steps in 59.47 secs\n",
            "Step  72100: train CrossEntropyLoss |  4.79868984\n",
            "Step  72100: eval  CrossEntropyLoss |  4.62722015\n",
            "Step  72100: eval          Accuracy |  0.26605505\n",
            "\n",
            "Step  72200: Ran 100 train steps in 60.24 secs\n",
            "Step  72200: train CrossEntropyLoss |  4.79023790\n",
            "Step  72200: eval  CrossEntropyLoss |  5.40396786\n",
            "Step  72200: eval          Accuracy |  0.22549020\n",
            "\n",
            "Step  72300: Ran 100 train steps in 60.15 secs\n",
            "Step  72300: train CrossEntropyLoss |  4.75313854\n",
            "Step  72300: eval  CrossEntropyLoss |  4.74364424\n",
            "Step  72300: eval          Accuracy |  0.24880384\n",
            "\n",
            "Step  72400: Ran 100 train steps in 59.48 secs\n",
            "Step  72400: train CrossEntropyLoss |  4.73372412\n",
            "Step  72400: eval  CrossEntropyLoss |  4.67158413\n",
            "Step  72400: eval          Accuracy |  0.25619835\n",
            "\n",
            "Step  72500: Ran 100 train steps in 59.48 secs\n",
            "Step  72500: train CrossEntropyLoss |  4.81693602\n",
            "Step  72500: eval  CrossEntropyLoss |  5.32246780\n",
            "Step  72500: eval          Accuracy |  0.21052632\n",
            "\n",
            "Step  72600: Ran 100 train steps in 60.27 secs\n",
            "Step  72600: train CrossEntropyLoss |  4.74349976\n",
            "Step  72600: eval  CrossEntropyLoss |  4.42213488\n",
            "Step  72600: eval          Accuracy |  0.22105265\n",
            "\n",
            "Step  72700: Ran 100 train steps in 59.54 secs\n",
            "Step  72700: train CrossEntropyLoss |  4.78923893\n",
            "Step  72700: eval  CrossEntropyLoss |  4.64580297\n",
            "Step  72700: eval          Accuracy |  0.29411766\n",
            "\n",
            "Step  72800: Ran 100 train steps in 60.32 secs\n",
            "Step  72800: train CrossEntropyLoss |  4.84830236\n",
            "Step  72800: eval  CrossEntropyLoss |  4.74608088\n",
            "Step  72800: eval          Accuracy |  0.25490198\n",
            "\n",
            "Step  72900: Ran 100 train steps in 59.62 secs\n",
            "Step  72900: train CrossEntropyLoss |  4.80569887\n",
            "Step  72900: eval  CrossEntropyLoss |  4.98992682\n",
            "Step  72900: eval          Accuracy |  0.26213592\n",
            "\n",
            "Step  73000: Ran 100 train steps in 59.59 secs\n",
            "Step  73000: train CrossEntropyLoss |  4.80048466\n",
            "Step  73000: eval  CrossEntropyLoss |  5.70576000\n",
            "Step  73000: eval          Accuracy |  0.19565217\n",
            "\n",
            "Step  73100: Ran 100 train steps in 59.66 secs\n",
            "Step  73100: train CrossEntropyLoss |  4.79248714\n",
            "Step  73100: eval  CrossEntropyLoss |  4.54565001\n",
            "Step  73100: eval          Accuracy |  0.29729730\n",
            "\n",
            "Step  73200: Ran 100 train steps in 59.56 secs\n",
            "Step  73200: train CrossEntropyLoss |  4.74732876\n",
            "Step  73200: eval  CrossEntropyLoss |  4.80714273\n",
            "Step  73200: eval          Accuracy |  0.20370370\n",
            "\n",
            "Step  73300: Ran 100 train steps in 59.86 secs\n",
            "Step  73300: train CrossEntropyLoss |  4.81879997\n",
            "Step  73300: eval  CrossEntropyLoss |  4.54212523\n",
            "Step  73300: eval          Accuracy |  0.23655914\n",
            "\n",
            "Step  73400: Ran 100 train steps in 59.77 secs\n",
            "Step  73400: train CrossEntropyLoss |  4.75613689\n",
            "Step  73400: eval  CrossEntropyLoss |  4.16596317\n",
            "Step  73400: eval          Accuracy |  0.27927929\n",
            "\n",
            "Step  73500: Ran 100 train steps in 60.08 secs\n",
            "Step  73500: train CrossEntropyLoss |  4.80846214\n",
            "Step  73500: eval  CrossEntropyLoss |  4.85339451\n",
            "Step  73500: eval          Accuracy |  0.24038462\n",
            "\n",
            "Step  73600: Ran 100 train steps in 59.81 secs\n",
            "Step  73600: train CrossEntropyLoss |  4.78437090\n",
            "Step  73600: eval  CrossEntropyLoss |  4.79874277\n",
            "Step  73600: eval          Accuracy |  0.28333333\n",
            "\n",
            "Step  73700: Ran 100 train steps in 59.55 secs\n",
            "Step  73700: train CrossEntropyLoss |  4.79559946\n",
            "Step  73700: eval  CrossEntropyLoss |  4.91045570\n",
            "Step  73700: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  73800: Ran 100 train steps in 60.28 secs\n",
            "Step  73800: train CrossEntropyLoss |  4.79548740\n",
            "Step  73800: eval  CrossEntropyLoss |  5.33923769\n",
            "Step  73800: eval          Accuracy |  0.12222222\n",
            "\n",
            "Step  73900: Ran 100 train steps in 59.98 secs\n",
            "Step  73900: train CrossEntropyLoss |  4.81128836\n",
            "Step  73900: eval  CrossEntropyLoss |  4.51782084\n",
            "Step  73900: eval          Accuracy |  0.23831774\n",
            "\n",
            "Step  74000: Ran 100 train steps in 59.46 secs\n",
            "Step  74000: train CrossEntropyLoss |  4.73908806\n",
            "Step  74000: eval  CrossEntropyLoss |  4.42286777\n",
            "Step  74000: eval          Accuracy |  0.28846154\n",
            "\n",
            "Step  74100: Ran 100 train steps in 59.51 secs\n",
            "Step  74100: train CrossEntropyLoss |  4.79363155\n",
            "Step  74100: eval  CrossEntropyLoss |  5.03202248\n",
            "Step  74100: eval          Accuracy |  0.22500001\n",
            "\n",
            "Step  74200: Ran 100 train steps in 59.99 secs\n",
            "Step  74200: train CrossEntropyLoss |  4.78737020\n",
            "Step  74200: eval  CrossEntropyLoss |  5.00800228\n",
            "Step  74200: eval          Accuracy |  0.20224719\n",
            "\n",
            "Step  74300: Ran 100 train steps in 60.11 secs\n",
            "Step  74300: train CrossEntropyLoss |  4.78544807\n",
            "Step  74300: eval  CrossEntropyLoss |  5.04127026\n",
            "Step  74300: eval          Accuracy |  0.24210528\n",
            "\n",
            "Step  74400: Ran 100 train steps in 59.42 secs\n",
            "Step  74400: train CrossEntropyLoss |  4.74065733\n",
            "Step  74400: eval  CrossEntropyLoss |  4.91644669\n",
            "Step  74400: eval          Accuracy |  0.20704845\n",
            "\n",
            "Step  74500: Ran 100 train steps in 59.82 secs\n",
            "Step  74500: train CrossEntropyLoss |  4.79311228\n",
            "Step  74500: eval  CrossEntropyLoss |  4.78163433\n",
            "Step  74500: eval          Accuracy |  0.32673267\n",
            "\n",
            "Step  74600: Ran 100 train steps in 60.12 secs\n",
            "Step  74600: train CrossEntropyLoss |  4.83621311\n",
            "Step  74600: eval  CrossEntropyLoss |  4.91986656\n",
            "Step  74600: eval          Accuracy |  0.23364486\n",
            "\n",
            "Step  74700: Ran 100 train steps in 59.64 secs\n",
            "Step  74700: train CrossEntropyLoss |  4.76750803\n",
            "Step  74700: eval  CrossEntropyLoss |  4.50524092\n",
            "Step  74700: eval          Accuracy |  0.33333334\n",
            "\n",
            "Step  74800: Ran 100 train steps in 59.46 secs\n",
            "Step  74800: train CrossEntropyLoss |  4.73765469\n",
            "Step  74800: eval  CrossEntropyLoss |  4.64001751\n",
            "Step  74800: eval          Accuracy |  0.28571427\n",
            "\n",
            "Step  74900: Ran 100 train steps in 60.24 secs\n",
            "Step  74900: train CrossEntropyLoss |  4.69391108\n",
            "Step  74900: eval  CrossEntropyLoss |  4.77734995\n",
            "Step  74900: eval          Accuracy |  0.23577237\n",
            "\n",
            "Step  75000: Ran 100 train steps in 60.23 secs\n",
            "Step  75000: train CrossEntropyLoss |  4.78137732\n",
            "Step  75000: eval  CrossEntropyLoss |  4.98541927\n",
            "Step  75000: eval          Accuracy |  0.23999999\n",
            "\n",
            "Step  75100: Ran 100 train steps in 59.66 secs\n",
            "Step  75100: train CrossEntropyLoss |  4.75298786\n",
            "Step  75100: eval  CrossEntropyLoss |  4.49504757\n",
            "Step  75100: eval          Accuracy |  0.28272250\n",
            "\n",
            "Step  75200: Ran 100 train steps in 59.92 secs\n",
            "Step  75200: train CrossEntropyLoss |  4.69737959\n",
            "Step  75200: eval  CrossEntropyLoss |  4.67584610\n",
            "Step  75200: eval          Accuracy |  0.26999998\n",
            "\n",
            "Step  75300: Ran 100 train steps in 59.72 secs\n",
            "Step  75300: train CrossEntropyLoss |  4.75481606\n",
            "Step  75300: eval  CrossEntropyLoss |  4.54459476\n",
            "Step  75300: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  75400: Ran 100 train steps in 60.13 secs\n",
            "Step  75400: train CrossEntropyLoss |  4.73010778\n",
            "Step  75400: eval  CrossEntropyLoss |  5.37862778\n",
            "Step  75400: eval          Accuracy |  0.21890546\n",
            "\n",
            "Step  75500: Ran 100 train steps in 60.13 secs\n",
            "Step  75500: train CrossEntropyLoss |  4.78568697\n",
            "Step  75500: eval  CrossEntropyLoss |  4.83570147\n",
            "Step  75500: eval          Accuracy |  0.22580644\n",
            "\n",
            "Step  75600: Ran 100 train steps in 59.43 secs\n",
            "Step  75600: train CrossEntropyLoss |  4.75081825\n",
            "Step  75600: eval  CrossEntropyLoss |  5.37286568\n",
            "Step  75600: eval          Accuracy |  0.18803419\n",
            "\n",
            "Step  75700: Ran 100 train steps in 60.18 secs\n",
            "Step  75700: train CrossEntropyLoss |  4.73680639\n",
            "Step  75700: eval  CrossEntropyLoss |  4.78460503\n",
            "Step  75700: eval          Accuracy |  0.22826087\n",
            "\n",
            "Step  75800: Ran 100 train steps in 60.20 secs\n",
            "Step  75800: train CrossEntropyLoss |  4.76561403\n",
            "Step  75800: eval  CrossEntropyLoss |  5.03259277\n",
            "Step  75800: eval          Accuracy |  0.19999999\n",
            "\n",
            "Step  75900: Ran 100 train steps in 60.27 secs\n",
            "Step  75900: train CrossEntropyLoss |  4.74116564\n",
            "Step  75900: eval  CrossEntropyLoss |  4.83209848\n",
            "Step  75900: eval          Accuracy |  0.26442307\n",
            "\n",
            "Step  76000: Ran 100 train steps in 60.02 secs\n",
            "Step  76000: train CrossEntropyLoss |  4.72218847\n",
            "Step  76000: eval  CrossEntropyLoss |  4.66330433\n",
            "Step  76000: eval          Accuracy |  0.24299064\n",
            "\n",
            "Step  76100: Ran 100 train steps in 60.22 secs\n",
            "Step  76100: train CrossEntropyLoss |  4.77069807\n",
            "Step  76100: eval  CrossEntropyLoss |  4.78926325\n",
            "Step  76100: eval          Accuracy |  0.27500001\n",
            "\n",
            "Step  76200: Ran 100 train steps in 60.09 secs\n",
            "Step  76200: train CrossEntropyLoss |  4.75371885\n",
            "Step  76200: eval  CrossEntropyLoss |  4.46281338\n",
            "Step  76200: eval          Accuracy |  0.31683168\n",
            "\n",
            "Step  76300: Ran 100 train steps in 59.71 secs\n",
            "Step  76300: train CrossEntropyLoss |  4.76566029\n",
            "Step  76300: eval  CrossEntropyLoss |  5.32829952\n",
            "Step  76300: eval          Accuracy |  0.16666667\n",
            "\n",
            "Step  76400: Ran 100 train steps in 59.74 secs\n",
            "Step  76400: train CrossEntropyLoss |  4.76353502\n",
            "Step  76400: eval  CrossEntropyLoss |  4.67567778\n",
            "Step  76400: eval          Accuracy |  0.22448979\n",
            "\n",
            "Step  76500: Ran 100 train steps in 60.22 secs\n",
            "Step  76500: train CrossEntropyLoss |  4.73704147\n",
            "Step  76500: eval  CrossEntropyLoss |  4.64480066\n",
            "Step  76500: eval          Accuracy |  0.27433628\n",
            "\n",
            "Step  76600: Ran 100 train steps in 59.83 secs\n",
            "Step  76600: train CrossEntropyLoss |  4.75998402\n",
            "Step  76600: eval  CrossEntropyLoss |  4.63278532\n",
            "Step  76600: eval          Accuracy |  0.24576271\n",
            "\n",
            "Step  76700: Ran 100 train steps in 59.20 secs\n",
            "Step  76700: train CrossEntropyLoss |  4.69586134\n",
            "Step  76700: eval  CrossEntropyLoss |  4.33422232\n",
            "Step  76700: eval          Accuracy |  0.29756099\n",
            "\n",
            "Step  76800: Ran 100 train steps in 59.63 secs\n",
            "Step  76800: train CrossEntropyLoss |  4.67600489\n",
            "Step  76800: eval  CrossEntropyLoss |  4.85700178\n",
            "Step  76800: eval          Accuracy |  0.23931625\n",
            "\n",
            "Step  76900: Ran 100 train steps in 59.60 secs\n",
            "Step  76900: train CrossEntropyLoss |  4.68945646\n",
            "Step  76900: eval  CrossEntropyLoss |  4.30652523\n",
            "Step  76900: eval          Accuracy |  0.27551019\n",
            "\n",
            "Step  77000: Ran 100 train steps in 60.03 secs\n",
            "Step  77000: train CrossEntropyLoss |  4.71232748\n",
            "Step  77000: eval  CrossEntropyLoss |  4.78063011\n",
            "Step  77000: eval          Accuracy |  0.23076925\n",
            "\n",
            "Step  77100: Ran 100 train steps in 59.82 secs\n",
            "Step  77100: train CrossEntropyLoss |  4.72886324\n",
            "Step  77100: eval  CrossEntropyLoss |  4.63217402\n",
            "Step  77100: eval          Accuracy |  0.23902439\n",
            "\n",
            "Step  77200: Ran 100 train steps in 59.92 secs\n",
            "Step  77200: train CrossEntropyLoss |  4.72202969\n",
            "Step  77200: eval  CrossEntropyLoss |  5.02870607\n",
            "Step  77200: eval          Accuracy |  0.20689654\n",
            "\n",
            "Step  77300: Ran 100 train steps in 59.43 secs\n",
            "Step  77300: train CrossEntropyLoss |  4.65017319\n",
            "Step  77300: eval  CrossEntropyLoss |  4.87091208\n",
            "Step  77300: eval          Accuracy |  0.22727272\n",
            "\n",
            "Step  77400: Ran 100 train steps in 59.67 secs\n",
            "Step  77400: train CrossEntropyLoss |  4.70162201\n",
            "Step  77400: eval  CrossEntropyLoss |  4.82516146\n",
            "Step  77400: eval          Accuracy |  0.24537037\n",
            "\n",
            "Step  77500: Ran 100 train steps in 59.83 secs\n",
            "Step  77500: train CrossEntropyLoss |  4.70603323\n",
            "Step  77500: eval  CrossEntropyLoss |  4.97898960\n",
            "Step  77500: eval          Accuracy |  0.23999999\n",
            "\n",
            "Step  77600: Ran 100 train steps in 59.63 secs\n",
            "Step  77600: train CrossEntropyLoss |  4.71363592\n",
            "Step  77600: eval  CrossEntropyLoss |  4.40070581\n",
            "Step  77600: eval          Accuracy |  0.28318584\n",
            "\n",
            "Step  77700: Ran 100 train steps in 59.77 secs\n",
            "Step  77700: train CrossEntropyLoss |  4.67419434\n",
            "Step  77700: eval  CrossEntropyLoss |  5.16620398\n",
            "Step  77700: eval          Accuracy |  0.24770641\n",
            "\n",
            "Step  77800: Ran 100 train steps in 59.52 secs\n",
            "Step  77800: train CrossEntropyLoss |  4.70081902\n",
            "Step  77800: eval  CrossEntropyLoss |  4.58303738\n",
            "Step  77800: eval          Accuracy |  0.26130652\n",
            "\n",
            "Step  77900: Ran 100 train steps in 59.81 secs\n",
            "Step  77900: train CrossEntropyLoss |  4.72399044\n",
            "Step  77900: eval  CrossEntropyLoss |  5.26049614\n",
            "Step  77900: eval          Accuracy |  0.21782178\n",
            "\n",
            "Step  78000: Ran 100 train steps in 59.67 secs\n",
            "Step  78000: train CrossEntropyLoss |  4.67390871\n",
            "Step  78000: eval  CrossEntropyLoss |  4.38456917\n",
            "Step  78000: eval          Accuracy |  0.25773197\n",
            "\n",
            "Step  78100: Ran 100 train steps in 59.48 secs\n",
            "Step  78100: train CrossEntropyLoss |  4.70097446\n",
            "Step  78100: eval  CrossEntropyLoss |  4.34957552\n",
            "Step  78100: eval          Accuracy |  0.29064038\n",
            "\n",
            "Step  78200: Ran 100 train steps in 59.94 secs\n",
            "Step  78200: train CrossEntropyLoss |  4.70978832\n",
            "Step  78200: eval  CrossEntropyLoss |  4.97966623\n",
            "Step  78200: eval          Accuracy |  0.24409449\n",
            "\n",
            "Step  78300: Ran 100 train steps in 59.38 secs\n",
            "Step  78300: train CrossEntropyLoss |  4.70319414\n",
            "Step  78300: eval  CrossEntropyLoss |  5.12049580\n",
            "Step  78300: eval          Accuracy |  0.16216217\n",
            "\n",
            "Step  78400: Ran 100 train steps in 59.53 secs\n",
            "Step  78400: train CrossEntropyLoss |  4.70009327\n",
            "Step  78400: eval  CrossEntropyLoss |  4.37457705\n",
            "Step  78400: eval          Accuracy |  0.27040815\n",
            "\n",
            "Step  78500: Ran 100 train steps in 59.69 secs\n",
            "Step  78500: train CrossEntropyLoss |  4.76369715\n",
            "Step  78500: eval  CrossEntropyLoss |  4.81621552\n",
            "Step  78500: eval          Accuracy |  0.16346155\n",
            "\n",
            "Step  78600: Ran 100 train steps in 59.27 secs\n",
            "Step  78600: train CrossEntropyLoss |  4.68893576\n",
            "Step  78600: eval  CrossEntropyLoss |  5.61348391\n",
            "Step  78600: eval          Accuracy |  0.21359223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dyYwnk_1i56",
        "outputId": "179e1926-7d75-403c-86af-5f7f7e9efdac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# copy the model to Google Drive\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/root/model/model.pkl.gz': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPWbvRXFyAC8"
      },
      "source": [
        "# sync Google Drive dir with the train dir\r\n",
        "!rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "outputId": "7067530c-31d1-4c81-a936-c537c93d93ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_text_pairs[0]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('подразделения мчс россии завершили ликвидацию последствий пожаров в населенных пунктах хакасии, сообщает в четверг, 23 апреля, риа новости со ссылкой на республиканское управление ведомства. «мчс россии начинает поэтапный вывод группировки сил, привлеченных в хакасию для оказания помощи населению при разборе завалов и сгоревших конструкций», — говорится в сообщении главного управления мчс по республике хакасия. первая группа спасателей из 120 человек улетает ведомственным самолетом. на борт загружен аварийно-спасательный инструмент. до конца недели все подразделения мчс россии из других регионов покинут хакасию и вернутся к местам постоянной дислокации. 12 апреля лесные пожары, распространявшиеся на территории хакасии, перекинулись на населенные пункты. пострадало более 40 населенных пунктов, около 5 тысяч человек остались без жилья. уже 15 апреля в мчс сообщили о ликвидации всех пожаров в населенных пунктах хакасии, красноярского края и забайкалья. по последним данным, от пожаров погибли 34 человека, более 600\\xa0обратились за медицинской помощью. основными причинами распространения огня называют неконтролируемый пал травы и сильные порывы ветра. следственный комитет рф после пожаров в хакасии возбудил пять уголовных дел. предварительно ущерб от огня оценен в 7\\xa0миллиардов рублей.',\n",
              " 'мчс завершило ликвидацию последствий пожаров в\\xa0хакасии')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "outputId": "34600992-051b-4a19-b680-efec7d323b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "sentence_test_nxt_symbl = \"подразделения мчс россии завершили ликвидацию последствий пожаров в населенных пунктах хакасии, сообщает в четверг, 23 апреля, риа новости.\"\r\n",
        "detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[0], model)])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "26\n",
            "[ 5317  3090   278  2130    18 14638   408  9828 12479     5  9291 15468\n",
            "   710    26  7054 15945   258     5  1801 15945  1628  1215 15945   785\n",
            "  1006 15949]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'в'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "        if len(generated_output) >= 20:\r\n",
        "            print(detokenize(generated_output))\r\n",
        "            break\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "    \r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "outputId": "37f4d576-752b-4c46-9410-9582e7384309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_sentence = \"подразделения мчс россии завершили ликвидацию последствий пожаров в населенных пунктах хакасии, сообщает в четверг, 23 апреля, риа новости.\"\r\n",
        "print(wrapper.fill(test_sentence), '\\n')\r\n",
        "print(greedy_decode(test_sentence, model))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "подразделения мчс россии завершили ликвидацию последствий пожаров в\n",
            "населенных пунктах хакасии, сообщает в четверг, 23 апреля, риа\n",
            "новости. \n",
            "\n",
            "32\n",
            "26\n",
            "[ 5317  3090   278  2130    18 14638   408  9828 12479     5  9291 15468\n",
            "   710    26  7054 15945   258     5  1801 15945  1628  1215 15945   785\n",
            "  1006 15949]\n",
            "в\n",
            "32\n",
            "27\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5]\n",
            "в результате\n",
            "32\n",
            "28\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818]\n",
            "в результате результате\n",
            "32\n",
            "29\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818]\n",
            "в результате результате результате\n",
            "32\n",
            "30\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818]\n",
            "в результате результате результате результате\n",
            "32\n",
            "31\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818]\n",
            "в результате результате результате результате в\n",
            "64\n",
            "32\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5]\n",
            "в результате результате результате результате в результате\n",
            "64\n",
            "33\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818]\n",
            "в результате результате результате результате в результате в\n",
            "64\n",
            "34\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5]\n",
            "в результате результате результате результате в результате в результате\n",
            "64\n",
            "35\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва\n",
            "64\n",
            "36\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869]\n",
            "в результате результате результате результате в результате в результате взрыва в\n",
            "64\n",
            "37\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате\n",
            "64\n",
            "38\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в\n",
            "64\n",
            "39\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате\n",
            "64\n",
            "40\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли\n",
            "64\n",
            "41\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли\n",
            "64\n",
            "42\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли\n",
            "64\n",
            "43\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли\n",
            "64\n",
            "44\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли\n",
            "64\n",
            "45\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли\n",
            "64\n",
            "46\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли\n",
            "64\n",
            "47\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли\n",
            "64\n",
            "48\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в\n",
            "64\n",
            "49\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате\n",
            "64\n",
            "50\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва\n",
            "64\n",
            "51\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в\n",
            "64\n",
            "52\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате\n",
            "64\n",
            "53\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва\n",
            "64\n",
            "54\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в\n",
            "64\n",
            "55\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате\n",
            "64\n",
            "56\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва\n",
            "64\n",
            "57\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в\n",
            "64\n",
            "58\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате\n",
            "64\n",
            "59\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли\n",
            "64\n",
            "60\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли\n",
            "64\n",
            "61\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли\n",
            "64\n",
            "62\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в\n",
            "64\n",
            "63\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате\n",
            "128\n",
            "64\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли\n",
            "128\n",
            "65\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли\n",
            "128\n",
            "66\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли\n",
            "128\n",
            "67\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли\n",
            "128\n",
            "68\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли\n",
            "128\n",
            "69\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли\n",
            "128\n",
            "70\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в\n",
            "128\n",
            "71\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате\n",
            "128\n",
            "72\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли\n",
            "128\n",
            "73\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в\n",
            "128\n",
            "74\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате\n",
            "128\n",
            "75\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли\n",
            "128\n",
            "76\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в\n",
            "128\n",
            "77\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "78\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "79\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "80\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "81\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "82\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "83\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "84\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "85\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "86\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "87\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "88\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "89\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "90\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "91\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "92\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "93\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "94\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "95\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "96\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "97\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-28fe3024bd30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"подразделения мчс россии завершили ликвидацию последствий пожаров в населенных пунктах хакасии, сообщает в четверг, 23 апреля, риа новости.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-ed2cb67223e2>\u001b[0m in \u001b[0;36mgreedy_decode\u001b[0;34m(input_sentence, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcur_output\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEOS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Get next symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mcur_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_symbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_output_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Append next symbol to original sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcur_output_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_output_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-58ee115bfca9>\u001b[0m in \u001b[0;36mnext_symbol\u001b[0;34m(cur_output_tokens, model)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# model expects a tuple containing two padded tensors (with batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_with_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_with_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m# HINT: output has shape (1, padded_length, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# To get log_probs you need to index output with 0 in the first dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, weights, state, rng)\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m  \u001b[0;31m# Needed if the model wasn't fully initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;31m# Note that zip silently truncates its result if lengths don't match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m       \u001b[0msub_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_out\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/assert_shape.py\u001b[0m in \u001b[0;36mforward_wrapper\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_assert_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_assert_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/core.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m                          f'instead got: {self.weights}')\n\u001b[1;32m     94\u001b[0m       \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m  \u001b[0;31m# Affine map.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[0mcontract_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb_ndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3510\u001b[0m   \u001b[0mbatch_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3511\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontract_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mdot_general\u001b[0;34m(lhs, rhs, dimension_numbers, precision)\u001b[0m\n\u001b[1;32m    668\u001b[0m   return dot_general_p.bind(lhs, rhs,\n\u001b[1;32m    669\u001b[0m                             \u001b[0mdimension_numbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontract_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                             precision=_canonicalize_precision(precision))\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mtop_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    352\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_nans\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNMgNbMifDtf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILBTGs9HfDx2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6loRlXfYezPe"
      },
      "source": [
        "test_inputs = tokenize(inputs) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW"
      },
      "source": [
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(test_inputs)[None, :],\r\n",
        "                                        temperature=0.0, max_length=5) # originally max_length=10\r\n",
        "print(wrapper.fill(pretty_decode(output[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yXlt4_BezWi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgy_oXOlezZq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}