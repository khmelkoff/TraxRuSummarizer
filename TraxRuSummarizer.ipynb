{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM4mzQ966zPv84nB8Bwa9G2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21fa1270-a823-47cc-818e-5b7fc7ee68a5"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 17.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 50.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 64.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 46.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 26.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 56.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 59.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 52.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.2MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "51826f64-64c1-4e32-e010-81981eab4747"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "# data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "# data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "# data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "# data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "# text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "# for i in tqdm(range(data.shape[0])):\r\n",
        "    # if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        # text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafd90d3-592e-44a8-d602-30424903033f"
      },
      "source": [
        "# text_pairs[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "9a443a1c-929d-46b7-a380-16c307c65131"
      },
      "source": [
        "# s0 = text_pairs[10][0]\r\n",
        "# text_list = wrapper.wrap(s0[:300])\r\n",
        "# for line in text_list:\r\n",
        "#     print(line)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qLsMmUFFnHA"
      },
      "source": [
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "07be832a-8ea5-4c16-8104-0f9a40209e61"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boC79r_7EPy6",
        "outputId": "81143b8e-2eb8-49c0-8e9b-ef69c787c05a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('телеканал lifetime объявил о намерении снять фильм о жизни певицы уитни хьюстон. съемками будет руководить американская актриса анджела бассетт, для нее работа над картиной в качестве режиссера станет дебютной, сообщает издание variety. фильм может выйти на широкий экран уже в 2015 году. сценарий фильма написал сим биттерман. предполагается, что сюжет будет сосредоточен на отношениях певицы с ее бывшим мужем, r&b-исполнителем, рэпером и танцором бобби брауном — с момента их первой встречи до обретения обоими мировой популярности. их брак длился 15 лет, и распался, когда хьюстон было 48 лет. исполнители главных ролей в будущем фильме пока не известны. сама бассетт в одном из интервью упомянула, что брауна могут сыграть дональд гловер, тай диггс, майкл джордан или энтони маки, а хьюстон — фотомодель йайя дакоста. анджела бассетт — американская актриса, наиболее известная своими ролями в биографических фильмах. за роль тины тернер в ленте «на что способна любовь» (1993), бассетт удостоилась премии «золотой глобус» и номинации на премию «оскар». другими ее актерскими работами стали образы бетти шабазз, розы паркс, а также матери майкла джексона.',\n",
              " 'анджела бассетт снимет фильм о\\xa0уитни хьюстон')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7429b8d-4efb-49fa-e838-1198d4b18394"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64cb6bec-6eea-49a7-bad5-6df9626e881a"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5ce38e-f60e-4493-ac1a-379ea04f7f8a"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa51916-995c-4d61-99b7-aced32814b8f"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized:\n",
            "[3044, 11, 1627, 1080, 25, 1445, 288, 4205, 5442, 15945, 939, 11463, 1410, 164, 13393, 164, 8476, 1]\n",
            "len= 18\n",
            "detokenized:\n",
            "сведения о пассажирах на всех видах транспорта, где используются\n",
            "именные проездные билеты\n",
            "len= 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "66962d3f-6b25-45ed-e8b0-ddab035d60bf"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "ce6539e3-ec3b-4a3b-ede0-d71ebb9ca89c"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2648,  3508,  1183,   369,  4753,  1249, 15938,    25, 11813,\n",
              "       10231, 11587,   110,   138,  4257,   470, 12999,   291,   533,\n",
              "        5082,     5,  2031,  1337, 10618,     5,  2386, 15960,  6600,\n",
              "         462,  7040,  8282,   941,   147,   110, 15954,   169,   223,\n",
              "         163,   371,    63,   226,   968,    25,  3552,   999,  1790,\n",
              "        4921,  2127,    17, 14980,    46,  5622,   385,  6456,    17,\n",
              "        2386, 15960,  2282,   279,    16, 11291,   884, 10097,   110,\n",
              "          10,  3522,   917,   351,    52,  2399,    30,   824,   250,\n",
              "          17,  5532,     5,  5590,  1168,  6456, 12231,  1175,  8010,\n",
              "          81,   138,  4257,   188,  3492,   294,   593,  2656,    43,\n",
              "        2366,   973,  9806,  2645,     4,  3980, 13865,  1208,    81,\n",
              "        2673,   135,   125,  9800,  2648,   594,  1497, 13444, 10836,\n",
              "         210,  1421,     5,  2507, 15949,   769,   459, 15945,    46,\n",
              "       12588,  6456,   452,  3631,  1659,   534, 15949,   110,   315,\n",
              "         529,    32,   987,   430,  1976,   249, 12414,  2648,  6052,\n",
              "        2418, 15949,  2253,  4535,    57, 14875,    25,  7414, 11101,\n",
              "       15949,   203,  8339, 15945,    79,    25,  1776,  2883,   356,\n",
              "        2296, 11802,    16,  3929,  4571,   372,  1878,   175,  3535,\n",
              "        2112,   291,   210,   901,   639,   158,  7708,   585,  6398,\n",
              "        4422,  2127,  4376,  6385,  4968,  5739,  6690, 13478,   219,\n",
              "       15929,  2430,  5963,  1129, 15949,  8660,   110,   138,  4257,\n",
              "         470, 12999,   291,  2160,  7033,    53,  2951,   207,  8843,\n",
              "          23,  2044, 15960, 15929, 15960, 15941,  2894, 15931,  8660,\n",
              "        1193, 15945,  4267,    86,  9017,   106,  5754, 13097,   110,\n",
              "       10317,  3126,   371,  2235,  3394,  7017,  1787,  9200,  2443,\n",
              "        5716,     5,  2386, 15960,  4522,     5,  3757,   316, 15949,\n",
              "           4,   916,  1974,  7978,  6307,  6130,     5,  4622,  5544,\n",
              "         278, 15945,   861,    16,  7384, 15949,     1,     0,   953,\n",
              "          10,  1902,  7165,    30, 12547,  1249, 15938,    25, 11813,\n",
              "       10231, 11587,     1,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "b88afe50-b0e0-413a-bbef-a788bcd86c1e"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.00015)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.00015), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "0be3fd9d-9a4c-4cb7-cf28-190361ccffc4"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  40100: Ran 100 train steps in 85.59 secs\n",
            "Step  40100: train CrossEntropyLoss |  6.15448999\n",
            "Step  40100: eval  CrossEntropyLoss |  6.28171492\n",
            "Step  40100: eval          Accuracy |  0.17307693\n",
            "\n",
            "Step  40200: Ran 100 train steps in 63.85 secs\n",
            "Step  40200: train CrossEntropyLoss |  6.06502533\n",
            "Step  40200: eval  CrossEntropyLoss |  5.90793419\n",
            "Step  40200: eval          Accuracy |  0.18260869\n",
            "\n",
            "Step  40300: Ran 100 train steps in 46.75 secs\n",
            "Step  40300: train CrossEntropyLoss |  6.07245588\n",
            "Step  40300: eval  CrossEntropyLoss |  6.06538582\n",
            "Step  40300: eval          Accuracy |  0.19473685\n",
            "\n",
            "Step  40400: Ran 100 train steps in 47.35 secs\n",
            "Step  40400: train CrossEntropyLoss |  6.06374454\n",
            "Step  40400: eval  CrossEntropyLoss |  5.75971127\n",
            "Step  40400: eval          Accuracy |  0.18446602\n",
            "\n",
            "Step  40500: Ran 100 train steps in 47.26 secs\n",
            "Step  40500: train CrossEntropyLoss |  5.96124935\n",
            "Step  40500: eval  CrossEntropyLoss |  5.36516476\n",
            "Step  40500: eval          Accuracy |  0.19230770\n",
            "\n",
            "Step  40600: Ran 100 train steps in 47.72 secs\n",
            "Step  40600: train CrossEntropyLoss |  5.90989065\n",
            "Step  40600: eval  CrossEntropyLoss |  5.75021744\n",
            "Step  40600: eval          Accuracy |  0.14207649\n",
            "\n",
            "Step  40700: Ran 100 train steps in 47.67 secs\n",
            "Step  40700: train CrossEntropyLoss |  5.91631651\n",
            "Step  40700: eval  CrossEntropyLoss |  5.68759632\n",
            "Step  40700: eval          Accuracy |  0.16814159\n",
            "\n",
            "Step  40800: Ran 100 train steps in 47.96 secs\n",
            "Step  40800: train CrossEntropyLoss |  5.81280375\n",
            "Step  40800: eval  CrossEntropyLoss |  5.64321089\n",
            "Step  40800: eval          Accuracy |  0.18181819\n",
            "\n",
            "Step  40900: Ran 100 train steps in 47.78 secs\n",
            "Step  40900: train CrossEntropyLoss |  5.84556055\n",
            "Step  40900: eval  CrossEntropyLoss |  5.46037436\n",
            "Step  40900: eval          Accuracy |  0.20540541\n",
            "\n",
            "Step  41000: Ran 100 train steps in 47.93 secs\n",
            "Step  41000: train CrossEntropyLoss |  5.79434109\n",
            "Step  41000: eval  CrossEntropyLoss |  6.37084389\n",
            "Step  41000: eval          Accuracy |  0.17241380\n",
            "\n",
            "Step  41100: Ran 100 train steps in 48.38 secs\n",
            "Step  41100: train CrossEntropyLoss |  5.80823040\n",
            "Step  41100: eval  CrossEntropyLoss |  5.76863766\n",
            "Step  41100: eval          Accuracy |  0.19318183\n",
            "\n",
            "Step  41200: Ran 100 train steps in 47.84 secs\n",
            "Step  41200: train CrossEntropyLoss |  5.78369665\n",
            "Step  41200: eval  CrossEntropyLoss |  6.02579975\n",
            "Step  41200: eval          Accuracy |  0.10784314\n",
            "\n",
            "Step  41300: Ran 100 train steps in 47.99 secs\n",
            "Step  41300: train CrossEntropyLoss |  5.74792290\n",
            "Step  41300: eval  CrossEntropyLoss |  5.41237020\n",
            "Step  41300: eval          Accuracy |  0.19900498\n",
            "\n",
            "Step  41400: Ran 100 train steps in 47.80 secs\n",
            "Step  41400: train CrossEntropyLoss |  5.70388174\n",
            "Step  41400: eval  CrossEntropyLoss |  6.05817175\n",
            "Step  41400: eval          Accuracy |  0.20388350\n",
            "\n",
            "Step  41500: Ran 100 train steps in 48.06 secs\n",
            "Step  41500: train CrossEntropyLoss |  5.73536539\n",
            "Step  41500: eval  CrossEntropyLoss |  5.82820654\n",
            "Step  41500: eval          Accuracy |  0.13829787\n",
            "\n",
            "Step  41600: Ran 100 train steps in 47.83 secs\n",
            "Step  41600: train CrossEntropyLoss |  5.70982647\n",
            "Step  41600: eval  CrossEntropyLoss |  5.51964521\n",
            "Step  41600: eval          Accuracy |  0.17961165\n",
            "\n",
            "Step  41700: Ran 100 train steps in 48.00 secs\n",
            "Step  41700: train CrossEntropyLoss |  5.66378832\n",
            "Step  41700: eval  CrossEntropyLoss |  6.35122871\n",
            "Step  41700: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step  41800: Ran 100 train steps in 48.15 secs\n",
            "Step  41800: train CrossEntropyLoss |  5.63804007\n",
            "Step  41800: eval  CrossEntropyLoss |  5.37860203\n",
            "Step  41800: eval          Accuracy |  0.17431192\n",
            "\n",
            "Step  41900: Ran 100 train steps in 48.14 secs\n",
            "Step  41900: train CrossEntropyLoss |  5.69363403\n",
            "Step  41900: eval  CrossEntropyLoss |  5.73737288\n",
            "Step  41900: eval          Accuracy |  0.14432991\n",
            "\n",
            "Step  42000: Ran 100 train steps in 48.02 secs\n",
            "Step  42000: train CrossEntropyLoss |  5.67165899\n",
            "Step  42000: eval  CrossEntropyLoss |  5.75918102\n",
            "Step  42000: eval          Accuracy |  0.16410257\n",
            "\n",
            "Step  42100: Ran 100 train steps in 47.94 secs\n",
            "Step  42100: train CrossEntropyLoss |  5.63003159\n",
            "Step  42100: eval  CrossEntropyLoss |  4.93831253\n",
            "Step  42100: eval          Accuracy |  0.22727273\n",
            "\n",
            "Step  42200: Ran 100 train steps in 48.27 secs\n",
            "Step  42200: train CrossEntropyLoss |  5.61875343\n",
            "Step  42200: eval  CrossEntropyLoss |  5.44444036\n",
            "Step  42200: eval          Accuracy |  0.19000000\n",
            "\n",
            "Step  42300: Ran 100 train steps in 48.05 secs\n",
            "Step  42300: train CrossEntropyLoss |  5.63258219\n",
            "Step  42300: eval  CrossEntropyLoss |  5.24068594\n",
            "Step  42300: eval          Accuracy |  0.25510204\n",
            "\n",
            "Step  42400: Ran 100 train steps in 48.48 secs\n",
            "Step  42400: train CrossEntropyLoss |  5.65166903\n",
            "Step  42400: eval  CrossEntropyLoss |  5.23085785\n",
            "Step  42400: eval          Accuracy |  0.19354838\n",
            "\n",
            "Step  42500: Ran 100 train steps in 48.26 secs\n",
            "Step  42500: train CrossEntropyLoss |  5.61163425\n",
            "Step  42500: eval  CrossEntropyLoss |  5.21554518\n",
            "Step  42500: eval          Accuracy |  0.21649486\n",
            "\n",
            "Step  42600: Ran 100 train steps in 48.51 secs\n",
            "Step  42600: train CrossEntropyLoss |  5.60927820\n",
            "Step  42600: eval  CrossEntropyLoss |  5.24766016\n",
            "Step  42600: eval          Accuracy |  0.17475729\n",
            "\n",
            "Step  42700: Ran 100 train steps in 48.40 secs\n",
            "Step  42700: train CrossEntropyLoss |  5.60965395\n",
            "Step  42700: eval  CrossEntropyLoss |  5.44535112\n",
            "Step  42700: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  42800: Ran 100 train steps in 48.27 secs\n",
            "Step  42800: train CrossEntropyLoss |  5.59970379\n",
            "Step  42800: eval  CrossEntropyLoss |  5.84827232\n",
            "Step  42800: eval          Accuracy |  0.15384616\n",
            "\n",
            "Step  42900: Ran 100 train steps in 48.10 secs\n",
            "Step  42900: train CrossEntropyLoss |  5.59038782\n",
            "Step  42900: eval  CrossEntropyLoss |  5.46988153\n",
            "Step  42900: eval          Accuracy |  0.15833335\n",
            "\n",
            "Step  43000: Ran 100 train steps in 48.30 secs\n",
            "Step  43000: train CrossEntropyLoss |  5.56791544\n",
            "Step  43000: eval  CrossEntropyLoss |  5.89216185\n",
            "Step  43000: eval          Accuracy |  0.13008131\n",
            "\n",
            "Step  43100: Ran 100 train steps in 48.19 secs\n",
            "Step  43100: train CrossEntropyLoss |  5.57963896\n",
            "Step  43100: eval  CrossEntropyLoss |  5.29832315\n",
            "Step  43100: eval          Accuracy |  0.19806764\n",
            "\n",
            "Step  43200: Ran 100 train steps in 48.21 secs\n",
            "Step  43200: train CrossEntropyLoss |  5.58527899\n",
            "Step  43200: eval  CrossEntropyLoss |  5.62608719\n",
            "Step  43200: eval          Accuracy |  0.14150944\n",
            "\n",
            "Step  43300: Ran 100 train steps in 48.20 secs\n",
            "Step  43300: train CrossEntropyLoss |  5.55253696\n",
            "Step  43300: eval  CrossEntropyLoss |  5.48574400\n",
            "Step  43300: eval          Accuracy |  0.14563107\n",
            "\n",
            "Step  43400: Ran 100 train steps in 48.31 secs\n",
            "Step  43400: train CrossEntropyLoss |  5.58936262\n",
            "Step  43400: eval  CrossEntropyLoss |  5.83703947\n",
            "Step  43400: eval          Accuracy |  0.17948718\n",
            "\n",
            "Step  43500: Ran 100 train steps in 48.39 secs\n",
            "Step  43500: train CrossEntropyLoss |  5.62763166\n",
            "Step  43500: eval  CrossEntropyLoss |  5.21809006\n",
            "Step  43500: eval          Accuracy |  0.20192309\n",
            "\n",
            "Step  43600: Ran 100 train steps in 48.46 secs\n",
            "Step  43600: train CrossEntropyLoss |  5.54537153\n",
            "Step  43600: eval  CrossEntropyLoss |  6.02776861\n",
            "Step  43600: eval          Accuracy |  0.16239317\n",
            "\n",
            "Step  43700: Ran 100 train steps in 48.02 secs\n",
            "Step  43700: train CrossEntropyLoss |  5.59007931\n",
            "Step  43700: eval  CrossEntropyLoss |  6.07430315\n",
            "Step  43700: eval          Accuracy |  0.15929204\n",
            "\n",
            "Step  43800: Ran 100 train steps in 48.38 secs\n",
            "Step  43800: train CrossEntropyLoss |  5.52175951\n",
            "Step  43800: eval  CrossEntropyLoss |  5.28696108\n",
            "Step  43800: eval          Accuracy |  0.20873787\n",
            "\n",
            "Step  43900: Ran 100 train steps in 48.09 secs\n",
            "Step  43900: train CrossEntropyLoss |  5.56276798\n",
            "Step  43900: eval  CrossEntropyLoss |  5.63159227\n",
            "Step  43900: eval          Accuracy |  0.19658120\n",
            "\n",
            "Step  44000: Ran 100 train steps in 48.32 secs\n",
            "Step  44000: train CrossEntropyLoss |  5.52223015\n",
            "Step  44000: eval  CrossEntropyLoss |  5.90228653\n",
            "Step  44000: eval          Accuracy |  0.15126051\n",
            "\n",
            "Step  44100: Ran 100 train steps in 47.99 secs\n",
            "Step  44100: train CrossEntropyLoss |  5.49857950\n",
            "Step  44100: eval  CrossEntropyLoss |  5.73635721\n",
            "Step  44100: eval          Accuracy |  0.18644068\n",
            "\n",
            "Step  44200: Ran 100 train steps in 48.33 secs\n",
            "Step  44200: train CrossEntropyLoss |  5.51822519\n",
            "Step  44200: eval  CrossEntropyLoss |  5.15550423\n",
            "Step  44200: eval          Accuracy |  0.22448979\n",
            "\n",
            "Step  44300: Ran 100 train steps in 48.37 secs\n",
            "Step  44300: train CrossEntropyLoss |  5.54016638\n",
            "Step  44300: eval  CrossEntropyLoss |  5.29010630\n",
            "Step  44300: eval          Accuracy |  0.21568628\n",
            "\n",
            "Step  44400: Ran 100 train steps in 48.02 secs\n",
            "Step  44400: train CrossEntropyLoss |  5.51737833\n",
            "Step  44400: eval  CrossEntropyLoss |  5.66295004\n",
            "Step  44400: eval          Accuracy |  0.20183486\n",
            "\n",
            "Step  44500: Ran 100 train steps in 48.16 secs\n",
            "Step  44500: train CrossEntropyLoss |  5.54275703\n",
            "Step  44500: eval  CrossEntropyLoss |  5.66452932\n",
            "Step  44500: eval          Accuracy |  0.20370370\n",
            "\n",
            "Step  44600: Ran 100 train steps in 48.57 secs\n",
            "Step  44600: train CrossEntropyLoss |  5.49103832\n",
            "Step  44600: eval  CrossEntropyLoss |  5.67777157\n",
            "Step  44600: eval          Accuracy |  0.17475729\n",
            "\n",
            "Step  44700: Ran 100 train steps in 49.66 secs\n",
            "Step  44700: train CrossEntropyLoss |  5.50567532\n",
            "Step  44700: eval  CrossEntropyLoss |  4.89211130\n",
            "Step  44700: eval          Accuracy |  0.21925133\n",
            "\n",
            "Step  44800: Ran 100 train steps in 49.75 secs\n",
            "Step  44800: train CrossEntropyLoss |  5.51775455\n",
            "Step  44800: eval  CrossEntropyLoss |  5.19918013\n",
            "Step  44800: eval          Accuracy |  0.17821781\n",
            "\n",
            "Step  44900: Ran 100 train steps in 49.63 secs\n",
            "Step  44900: train CrossEntropyLoss |  5.52383232\n",
            "Step  44900: eval  CrossEntropyLoss |  5.38603354\n",
            "Step  44900: eval          Accuracy |  0.20430107\n",
            "\n",
            "Step  45000: Ran 100 train steps in 49.51 secs\n",
            "Step  45000: train CrossEntropyLoss |  5.44825649\n",
            "Step  45000: eval  CrossEntropyLoss |  5.49684334\n",
            "Step  45000: eval          Accuracy |  0.14529915\n",
            "\n",
            "Step  45100: Ran 100 train steps in 49.25 secs\n",
            "Step  45100: train CrossEntropyLoss |  5.53121090\n",
            "Step  45100: eval  CrossEntropyLoss |  4.60484886\n",
            "Step  45100: eval          Accuracy |  0.22471911\n",
            "\n",
            "Step  45200: Ran 100 train steps in 49.09 secs\n",
            "Step  45200: train CrossEntropyLoss |  5.52566719\n",
            "Step  45200: eval  CrossEntropyLoss |  5.65459538\n",
            "Step  45200: eval          Accuracy |  0.15463918\n",
            "\n",
            "Step  45300: Ran 100 train steps in 49.10 secs\n",
            "Step  45300: train CrossEntropyLoss |  5.47586584\n",
            "Step  45300: eval  CrossEntropyLoss |  5.42250395\n",
            "Step  45300: eval          Accuracy |  0.19148937\n",
            "\n",
            "Step  45400: Ran 100 train steps in 49.25 secs\n",
            "Step  45400: train CrossEntropyLoss |  5.44470882\n",
            "Step  45400: eval  CrossEntropyLoss |  5.92500305\n",
            "Step  45400: eval          Accuracy |  0.16037735\n",
            "\n",
            "Step  45500: Ran 100 train steps in 49.03 secs\n",
            "Step  45500: train CrossEntropyLoss |  5.40316200\n",
            "Step  45500: eval  CrossEntropyLoss |  5.46532822\n",
            "Step  45500: eval          Accuracy |  0.23076923\n",
            "\n",
            "Step  45600: Ran 100 train steps in 49.49 secs\n",
            "Step  45600: train CrossEntropyLoss |  5.44457817\n",
            "Step  45600: eval  CrossEntropyLoss |  5.25320196\n",
            "Step  45600: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  45700: Ran 100 train steps in 49.28 secs\n",
            "Step  45700: train CrossEntropyLoss |  5.50357533\n",
            "Step  45700: eval  CrossEntropyLoss |  5.06585884\n",
            "Step  45700: eval          Accuracy |  0.22222224\n",
            "\n",
            "Step  45800: Ran 100 train steps in 49.33 secs\n",
            "Step  45800: train CrossEntropyLoss |  5.46452999\n",
            "Step  45800: eval  CrossEntropyLoss |  4.92266226\n",
            "Step  45800: eval          Accuracy |  0.21568628\n",
            "\n",
            "Step  45900: Ran 100 train steps in 49.29 secs\n",
            "Step  45900: train CrossEntropyLoss |  5.43241262\n",
            "Step  45900: eval  CrossEntropyLoss |  5.36457062\n",
            "Step  45900: eval          Accuracy |  0.19555557\n",
            "\n",
            "Step  46000: Ran 100 train steps in 49.49 secs\n",
            "Step  46000: train CrossEntropyLoss |  5.50553942\n",
            "Step  46000: eval  CrossEntropyLoss |  5.14940786\n",
            "Step  46000: eval          Accuracy |  0.18627451\n",
            "\n",
            "Step  46100: Ran 100 train steps in 49.67 secs\n",
            "Step  46100: train CrossEntropyLoss |  5.49603033\n",
            "Step  46100: eval  CrossEntropyLoss |  5.41021252\n",
            "Step  46100: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  46200: Ran 100 train steps in 49.32 secs\n",
            "Step  46200: train CrossEntropyLoss |  5.42138529\n",
            "Step  46200: eval  CrossEntropyLoss |  5.60375023\n",
            "Step  46200: eval          Accuracy |  0.14814815\n",
            "\n",
            "Step  46300: Ran 100 train steps in 49.19 secs\n",
            "Step  46300: train CrossEntropyLoss |  5.47888279\n",
            "Step  46300: eval  CrossEntropyLoss |  5.80873203\n",
            "Step  46300: eval          Accuracy |  0.15789473\n",
            "\n",
            "Step  46400: Ran 100 train steps in 50.00 secs\n",
            "Step  46400: train CrossEntropyLoss |  5.46253109\n",
            "Step  46400: eval  CrossEntropyLoss |  5.16748571\n",
            "Step  46400: eval          Accuracy |  0.19459459\n",
            "\n",
            "Step  46500: Ran 100 train steps in 49.43 secs\n",
            "Step  46500: train CrossEntropyLoss |  5.53186703\n",
            "Step  46500: eval  CrossEntropyLoss |  5.63850880\n",
            "Step  46500: eval          Accuracy |  0.15384616\n",
            "\n",
            "Step  46600: Ran 100 train steps in 49.45 secs\n",
            "Step  46600: train CrossEntropyLoss |  5.45378733\n",
            "Step  46600: eval  CrossEntropyLoss |  5.15662718\n",
            "Step  46600: eval          Accuracy |  0.15702479\n",
            "\n",
            "Step  46700: Ran 100 train steps in 48.97 secs\n",
            "Step  46700: train CrossEntropyLoss |  5.47341156\n",
            "Step  46700: eval  CrossEntropyLoss |  5.14782953\n",
            "Step  46700: eval          Accuracy |  0.17500001\n",
            "\n",
            "Step  46800: Ran 100 train steps in 49.02 secs\n",
            "Step  46800: train CrossEntropyLoss |  5.41297531\n",
            "Step  46800: eval  CrossEntropyLoss |  5.19644785\n",
            "Step  46800: eval          Accuracy |  0.19801980\n",
            "\n",
            "Step  46900: Ran 100 train steps in 49.01 secs\n",
            "Step  46900: train CrossEntropyLoss |  5.46448898\n",
            "Step  46900: eval  CrossEntropyLoss |  6.01263714\n",
            "Step  46900: eval          Accuracy |  0.14953271\n",
            "\n",
            "Step  47000: Ran 100 train steps in 48.68 secs\n",
            "Step  47000: train CrossEntropyLoss |  5.41524410\n",
            "Step  47000: eval  CrossEntropyLoss |  5.24451160\n",
            "Step  47000: eval          Accuracy |  0.22772276\n",
            "\n",
            "Step  47100: Ran 100 train steps in 48.84 secs\n",
            "Step  47100: train CrossEntropyLoss |  5.40062428\n",
            "Step  47100: eval  CrossEntropyLoss |  5.04259634\n",
            "Step  47100: eval          Accuracy |  0.19827586\n",
            "\n",
            "Step  47200: Ran 100 train steps in 48.27 secs\n",
            "Step  47200: train CrossEntropyLoss |  5.42516851\n",
            "Step  47200: eval  CrossEntropyLoss |  5.35520601\n",
            "Step  47200: eval          Accuracy |  0.20555556\n",
            "\n",
            "Step  47300: Ran 100 train steps in 48.45 secs\n",
            "Step  47300: train CrossEntropyLoss |  5.48862982\n",
            "Step  47300: eval  CrossEntropyLoss |  5.27735901\n",
            "Step  47300: eval          Accuracy |  0.20353982\n",
            "\n",
            "Step  47400: Ran 100 train steps in 48.43 secs\n",
            "Step  47400: train CrossEntropyLoss |  5.39044905\n",
            "Step  47400: eval  CrossEntropyLoss |  5.46940947\n",
            "Step  47400: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  47500: Ran 100 train steps in 48.31 secs\n",
            "Step  47500: train CrossEntropyLoss |  5.37473154\n",
            "Step  47500: eval  CrossEntropyLoss |  5.32564259\n",
            "Step  47500: eval          Accuracy |  0.18316831\n",
            "\n",
            "Step  47600: Ran 100 train steps in 48.98 secs\n",
            "Step  47600: train CrossEntropyLoss |  5.36532021\n",
            "Step  47600: eval  CrossEntropyLoss |  5.18728924\n",
            "Step  47600: eval          Accuracy |  0.23232323\n",
            "\n",
            "Step  47700: Ran 100 train steps in 48.84 secs\n",
            "Step  47700: train CrossEntropyLoss |  5.42324495\n",
            "Step  47700: eval  CrossEntropyLoss |  5.76275015\n",
            "Step  47700: eval          Accuracy |  0.14678898\n",
            "\n",
            "Step  47800: Ran 100 train steps in 48.77 secs\n",
            "Step  47800: train CrossEntropyLoss |  5.39401865\n",
            "Step  47800: eval  CrossEntropyLoss |  5.32632256\n",
            "Step  47800: eval          Accuracy |  0.18750001\n",
            "\n",
            "Step  47900: Ran 100 train steps in 48.85 secs\n",
            "Step  47900: train CrossEntropyLoss |  5.38364983\n",
            "Step  47900: eval  CrossEntropyLoss |  5.27090263\n",
            "Step  47900: eval          Accuracy |  0.23008850\n",
            "\n",
            "Step  48000: Ran 100 train steps in 48.69 secs\n",
            "Step  48000: train CrossEntropyLoss |  5.39674187\n",
            "Step  48000: eval  CrossEntropyLoss |  5.59748411\n",
            "Step  48000: eval          Accuracy |  0.16346155\n",
            "\n",
            "Step  48100: Ran 100 train steps in 48.42 secs\n",
            "Step  48100: train CrossEntropyLoss |  5.37755871\n",
            "Step  48100: eval  CrossEntropyLoss |  5.18287039\n",
            "Step  48100: eval          Accuracy |  0.17241380\n",
            "\n",
            "Step  48200: Ran 100 train steps in 48.61 secs\n",
            "Step  48200: train CrossEntropyLoss |  5.36197948\n",
            "Step  48200: eval  CrossEntropyLoss |  5.21427536\n",
            "Step  48200: eval          Accuracy |  0.17543860\n",
            "\n",
            "Step  48300: Ran 100 train steps in 48.59 secs\n",
            "Step  48300: train CrossEntropyLoss |  5.39364862\n",
            "Step  48300: eval  CrossEntropyLoss |  5.68480921\n",
            "Step  48300: eval          Accuracy |  0.15789473\n",
            "\n",
            "Step  48400: Ran 100 train steps in 48.63 secs\n",
            "Step  48400: train CrossEntropyLoss |  5.41972876\n",
            "Step  48400: eval  CrossEntropyLoss |  5.06801271\n",
            "Step  48400: eval          Accuracy |  0.23232323\n",
            "\n",
            "Step  48500: Ran 100 train steps in 48.33 secs\n",
            "Step  48500: train CrossEntropyLoss |  5.32189178\n",
            "Step  48500: eval  CrossEntropyLoss |  4.70723629\n",
            "Step  48500: eval          Accuracy |  0.28571430\n",
            "\n",
            "Step  48600: Ran 100 train steps in 48.56 secs\n",
            "Step  48600: train CrossEntropyLoss |  5.35243034\n",
            "Step  48600: eval  CrossEntropyLoss |  5.43379164\n",
            "Step  48600: eval          Accuracy |  0.19907407\n",
            "\n",
            "Step  48700: Ran 100 train steps in 48.84 secs\n",
            "Step  48700: train CrossEntropyLoss |  5.38472176\n",
            "Step  48700: eval  CrossEntropyLoss |  5.35358906\n",
            "Step  48700: eval          Accuracy |  0.22772276\n",
            "\n",
            "Step  48800: Ran 100 train steps in 48.48 secs\n",
            "Step  48800: train CrossEntropyLoss |  5.37825346\n",
            "Step  48800: eval  CrossEntropyLoss |  5.48270607\n",
            "Step  48800: eval          Accuracy |  0.17592593\n",
            "\n",
            "Step  48900: Ran 100 train steps in 48.66 secs\n",
            "Step  48900: train CrossEntropyLoss |  5.38351679\n",
            "Step  48900: eval  CrossEntropyLoss |  5.23054314\n",
            "Step  48900: eval          Accuracy |  0.21739130\n",
            "\n",
            "Step  49000: Ran 100 train steps in 48.78 secs\n",
            "Step  49000: train CrossEntropyLoss |  5.43456602\n",
            "Step  49000: eval  CrossEntropyLoss |  5.05995512\n",
            "Step  49000: eval          Accuracy |  0.20192309\n",
            "\n",
            "Step  49100: Ran 100 train steps in 48.73 secs\n",
            "Step  49100: train CrossEntropyLoss |  5.31007433\n",
            "Step  49100: eval  CrossEntropyLoss |  5.28544092\n",
            "Step  49100: eval          Accuracy |  0.14678898\n",
            "\n",
            "Step  49200: Ran 100 train steps in 48.67 secs\n",
            "Step  49200: train CrossEntropyLoss |  5.33783627\n",
            "Step  49200: eval  CrossEntropyLoss |  5.14440775\n",
            "Step  49200: eval          Accuracy |  0.19999999\n",
            "\n",
            "Step  49300: Ran 100 train steps in 48.72 secs\n",
            "Step  49300: train CrossEntropyLoss |  5.32456875\n",
            "Step  49300: eval  CrossEntropyLoss |  5.31612301\n",
            "Step  49300: eval          Accuracy |  0.19534883\n",
            "\n",
            "Step  49400: Ran 100 train steps in 48.69 secs\n",
            "Step  49400: train CrossEntropyLoss |  5.37172174\n",
            "Step  49400: eval  CrossEntropyLoss |  5.53976727\n",
            "Step  49400: eval          Accuracy |  0.16981132\n",
            "\n",
            "Step  49500: Ran 100 train steps in 48.49 secs\n",
            "Step  49500: train CrossEntropyLoss |  5.34096670\n",
            "Step  49500: eval  CrossEntropyLoss |  5.24357557\n",
            "Step  49500: eval          Accuracy |  0.20744680\n",
            "\n",
            "Step  49600: Ran 100 train steps in 48.90 secs\n",
            "Step  49600: train CrossEntropyLoss |  5.33810186\n",
            "Step  49600: eval  CrossEntropyLoss |  5.55803442\n",
            "Step  49600: eval          Accuracy |  0.16504854\n",
            "\n",
            "Step  49700: Ran 100 train steps in 48.77 secs\n",
            "Step  49700: train CrossEntropyLoss |  5.35192537\n",
            "Step  49700: eval  CrossEntropyLoss |  5.41527128\n",
            "Step  49700: eval          Accuracy |  0.18095239\n",
            "\n",
            "Step  49800: Ran 100 train steps in 48.68 secs\n",
            "Step  49800: train CrossEntropyLoss |  5.30921125\n",
            "Step  49800: eval  CrossEntropyLoss |  5.54313755\n",
            "Step  49800: eval          Accuracy |  0.17010310\n",
            "\n",
            "Step  49900: Ran 100 train steps in 48.75 secs\n",
            "Step  49900: train CrossEntropyLoss |  5.35502052\n",
            "Step  49900: eval  CrossEntropyLoss |  5.58711720\n",
            "Step  49900: eval          Accuracy |  0.18095239\n",
            "\n",
            "Step  50000: Ran 100 train steps in 48.61 secs\n",
            "Step  50000: train CrossEntropyLoss |  5.34508419\n",
            "Step  50000: eval  CrossEntropyLoss |  5.35002804\n",
            "Step  50000: eval          Accuracy |  0.18181819\n",
            "\n",
            "Step  50100: Ran 100 train steps in 48.78 secs\n",
            "Step  50100: train CrossEntropyLoss |  5.33312798\n",
            "Step  50100: eval  CrossEntropyLoss |  5.85925388\n",
            "Step  50100: eval          Accuracy |  0.16822429\n",
            "\n",
            "Step  50200: Ran 100 train steps in 48.59 secs\n",
            "Step  50200: train CrossEntropyLoss |  5.32366610\n",
            "Step  50200: eval  CrossEntropyLoss |  5.17966175\n",
            "Step  50200: eval          Accuracy |  0.21428572\n",
            "\n",
            "Step  50300: Ran 100 train steps in 48.69 secs\n",
            "Step  50300: train CrossEntropyLoss |  5.27193785\n",
            "Step  50300: eval  CrossEntropyLoss |  4.95343685\n",
            "Step  50300: eval          Accuracy |  0.16071430\n",
            "\n",
            "Step  50400: Ran 100 train steps in 48.66 secs\n",
            "Step  50400: train CrossEntropyLoss |  5.35250521\n",
            "Step  50400: eval  CrossEntropyLoss |  5.09563398\n",
            "Step  50400: eval          Accuracy |  0.19587630\n",
            "\n",
            "Step  50500: Ran 100 train steps in 48.33 secs\n",
            "Step  50500: train CrossEntropyLoss |  5.32096195\n",
            "Step  50500: eval  CrossEntropyLoss |  5.04182100\n",
            "Step  50500: eval          Accuracy |  0.20634922\n",
            "\n",
            "Step  50600: Ran 100 train steps in 48.47 secs\n",
            "Step  50600: train CrossEntropyLoss |  5.32285070\n",
            "Step  50600: eval  CrossEntropyLoss |  5.84449434\n",
            "Step  50600: eval          Accuracy |  0.11428572\n",
            "\n",
            "Step  50700: Ran 100 train steps in 48.59 secs\n",
            "Step  50700: train CrossEntropyLoss |  5.31544590\n",
            "Step  50700: eval  CrossEntropyLoss |  5.06066751\n",
            "Step  50700: eval          Accuracy |  0.20218578\n",
            "\n",
            "Step  50800: Ran 100 train steps in 48.36 secs\n",
            "Step  50800: train CrossEntropyLoss |  5.30639505\n",
            "Step  50800: eval  CrossEntropyLoss |  5.52601290\n",
            "Step  50800: eval          Accuracy |  0.19047619\n",
            "\n",
            "Step  50900: Ran 100 train steps in 48.54 secs\n",
            "Step  50900: train CrossEntropyLoss |  5.35793018\n",
            "Step  50900: eval  CrossEntropyLoss |  5.26235914\n",
            "Step  50900: eval          Accuracy |  0.12711865\n",
            "\n",
            "Step  51000: Ran 100 train steps in 48.26 secs\n",
            "Step  51000: train CrossEntropyLoss |  5.29672527\n",
            "Step  51000: eval  CrossEntropyLoss |  5.66087675\n",
            "Step  51000: eval          Accuracy |  0.20707071\n",
            "\n",
            "Step  51100: Ran 100 train steps in 48.41 secs\n",
            "Step  51100: train CrossEntropyLoss |  5.24834585\n",
            "Step  51100: eval  CrossEntropyLoss |  5.33489704\n",
            "Step  51100: eval          Accuracy |  0.25423729\n",
            "\n",
            "Step  51200: Ran 100 train steps in 48.27 secs\n",
            "Step  51200: train CrossEntropyLoss |  5.27633476\n",
            "Step  51200: eval  CrossEntropyLoss |  5.24370193\n",
            "Step  51200: eval          Accuracy |  0.15966387\n",
            "\n",
            "Step  51300: Ran 100 train steps in 48.53 secs\n",
            "Step  51300: train CrossEntropyLoss |  5.30149603\n",
            "Step  51300: eval  CrossEntropyLoss |  5.27229309\n",
            "Step  51300: eval          Accuracy |  0.19587630\n",
            "\n",
            "Step  51400: Ran 100 train steps in 48.31 secs\n",
            "Step  51400: train CrossEntropyLoss |  5.31724834\n",
            "Step  51400: eval  CrossEntropyLoss |  4.85399818\n",
            "Step  51400: eval          Accuracy |  0.26804125\n",
            "\n",
            "Step  51500: Ran 100 train steps in 48.35 secs\n",
            "Step  51500: train CrossEntropyLoss |  5.34728527\n",
            "Step  51500: eval  CrossEntropyLoss |  5.26634121\n",
            "Step  51500: eval          Accuracy |  0.17391303\n",
            "\n",
            "Step  51600: Ran 100 train steps in 48.48 secs\n",
            "Step  51600: train CrossEntropyLoss |  5.26394653\n",
            "Step  51600: eval  CrossEntropyLoss |  5.15398598\n",
            "Step  51600: eval          Accuracy |  0.19791667\n",
            "\n",
            "Step  51700: Ran 100 train steps in 48.67 secs\n",
            "Step  51700: train CrossEntropyLoss |  5.29532719\n",
            "Step  51700: eval  CrossEntropyLoss |  4.96986485\n",
            "Step  51700: eval          Accuracy |  0.20754717\n",
            "\n",
            "Step  51800: Ran 100 train steps in 48.10 secs\n",
            "Step  51800: train CrossEntropyLoss |  5.25775146\n",
            "Step  51800: eval  CrossEntropyLoss |  5.79061413\n",
            "Step  51800: eval          Accuracy |  0.16521738\n",
            "\n",
            "Step  51900: Ran 100 train steps in 48.61 secs\n",
            "Step  51900: train CrossEntropyLoss |  5.30626202\n",
            "Step  51900: eval  CrossEntropyLoss |  5.39721489\n",
            "Step  51900: eval          Accuracy |  0.19000000\n",
            "\n",
            "Step  52000: Ran 100 train steps in 48.15 secs\n",
            "Step  52000: train CrossEntropyLoss |  5.24130106\n",
            "Step  52000: eval  CrossEntropyLoss |  5.50598574\n",
            "Step  52000: eval          Accuracy |  0.17391303\n",
            "\n",
            "Step  52100: Ran 100 train steps in 48.49 secs\n",
            "Step  52100: train CrossEntropyLoss |  5.27772427\n",
            "Step  52100: eval  CrossEntropyLoss |  5.35898829\n",
            "Step  52100: eval          Accuracy |  0.20952381\n",
            "\n",
            "Step  52200: Ran 100 train steps in 48.32 secs\n",
            "Step  52200: train CrossEntropyLoss |  5.23881054\n",
            "Step  52200: eval  CrossEntropyLoss |  5.52655125\n",
            "Step  52200: eval          Accuracy |  0.15306123\n",
            "\n",
            "Step  52300: Ran 100 train steps in 48.90 secs\n",
            "Step  52300: train CrossEntropyLoss |  5.28912258\n",
            "Step  52300: eval  CrossEntropyLoss |  5.47094393\n",
            "Step  52300: eval          Accuracy |  0.15384616\n",
            "\n",
            "Step  52400: Ran 100 train steps in 48.33 secs\n",
            "Step  52400: train CrossEntropyLoss |  5.30080843\n",
            "Step  52400: eval  CrossEntropyLoss |  5.80938578\n",
            "Step  52400: eval          Accuracy |  0.17511521\n",
            "\n",
            "Step  52500: Ran 100 train steps in 48.13 secs\n",
            "Step  52500: train CrossEntropyLoss |  5.28226995\n",
            "Step  52500: eval  CrossEntropyLoss |  4.98804045\n",
            "Step  52500: eval          Accuracy |  0.17582418\n",
            "\n",
            "Step  52600: Ran 100 train steps in 48.87 secs\n",
            "Step  52600: train CrossEntropyLoss |  5.30743456\n",
            "Step  52600: eval  CrossEntropyLoss |  4.54946566\n",
            "Step  52600: eval          Accuracy |  0.29999998\n",
            "\n",
            "Step  52700: Ran 100 train steps in 48.44 secs\n",
            "Step  52700: train CrossEntropyLoss |  5.23915339\n",
            "Step  52700: eval  CrossEntropyLoss |  5.05271196\n",
            "Step  52700: eval          Accuracy |  0.16666667\n",
            "\n",
            "Step  52800: Ran 100 train steps in 48.85 secs\n",
            "Step  52800: train CrossEntropyLoss |  5.24853182\n",
            "Step  52800: eval  CrossEntropyLoss |  5.58392715\n",
            "Step  52800: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  52900: Ran 100 train steps in 48.41 secs\n",
            "Step  52900: train CrossEntropyLoss |  5.25819397\n",
            "Step  52900: eval  CrossEntropyLoss |  5.60912132\n",
            "Step  52900: eval          Accuracy |  0.18421052\n",
            "\n",
            "Step  53000: Ran 100 train steps in 48.47 secs\n",
            "Step  53000: train CrossEntropyLoss |  5.23237944\n",
            "Step  53000: eval  CrossEntropyLoss |  5.11351490\n",
            "Step  53000: eval          Accuracy |  0.25438598\n",
            "\n",
            "Step  53100: Ran 100 train steps in 48.10 secs\n",
            "Step  53100: train CrossEntropyLoss |  5.27960634\n",
            "Step  53100: eval  CrossEntropyLoss |  4.76332569\n",
            "Step  53100: eval          Accuracy |  0.25263160\n",
            "\n",
            "Step  53200: Ran 100 train steps in 48.28 secs\n",
            "Step  53200: train CrossEntropyLoss |  5.21632481\n",
            "Step  53200: eval  CrossEntropyLoss |  5.46890306\n",
            "Step  53200: eval          Accuracy |  0.19587630\n",
            "\n",
            "Step  53300: Ran 100 train steps in 48.13 secs\n",
            "Step  53300: train CrossEntropyLoss |  5.26003361\n",
            "Step  53300: eval  CrossEntropyLoss |  5.45618963\n",
            "Step  53300: eval          Accuracy |  0.21848741\n",
            "\n",
            "Step  53400: Ran 100 train steps in 48.35 secs\n",
            "Step  53400: train CrossEntropyLoss |  5.26587391\n",
            "Step  53400: eval  CrossEntropyLoss |  5.06070995\n",
            "Step  53400: eval          Accuracy |  0.22164950\n",
            "\n",
            "Step  53500: Ran 100 train steps in 48.22 secs\n",
            "Step  53500: train CrossEntropyLoss |  5.28413820\n",
            "Step  53500: eval  CrossEntropyLoss |  5.60550690\n",
            "Step  53500: eval          Accuracy |  0.16831683\n",
            "\n",
            "Step  53600: Ran 100 train steps in 48.09 secs\n",
            "Step  53600: train CrossEntropyLoss |  5.23634863\n",
            "Step  53600: eval  CrossEntropyLoss |  4.78330564\n",
            "Step  53600: eval          Accuracy |  0.26415095\n",
            "\n",
            "Step  53700: Ran 100 train steps in 47.99 secs\n",
            "Step  53700: train CrossEntropyLoss |  5.22554350\n",
            "Step  53700: eval  CrossEntropyLoss |  5.13786697\n",
            "Step  53700: eval          Accuracy |  0.20192309\n",
            "\n",
            "Step  53800: Ran 100 train steps in 48.59 secs\n",
            "Step  53800: train CrossEntropyLoss |  5.23219395\n",
            "Step  53800: eval  CrossEntropyLoss |  5.36587095\n",
            "Step  53800: eval          Accuracy |  0.15447156\n",
            "\n",
            "Step  53900: Ran 100 train steps in 48.07 secs\n",
            "Step  53900: train CrossEntropyLoss |  5.23062849\n",
            "Step  53900: eval  CrossEntropyLoss |  4.87951517\n",
            "Step  53900: eval          Accuracy |  0.23809524\n",
            "\n",
            "Step  54000: Ran 100 train steps in 48.53 secs\n",
            "Step  54000: train CrossEntropyLoss |  5.21265507\n",
            "Step  54000: eval  CrossEntropyLoss |  5.28675079\n",
            "Step  54000: eval          Accuracy |  0.19354838\n",
            "\n",
            "Step  54100: Ran 100 train steps in 48.20 secs\n",
            "Step  54100: train CrossEntropyLoss |  5.19932079\n",
            "Step  54100: eval  CrossEntropyLoss |  5.29705667\n",
            "Step  54100: eval          Accuracy |  0.20512821\n",
            "\n",
            "Step  54200: Ran 100 train steps in 48.28 secs\n",
            "Step  54200: train CrossEntropyLoss |  5.23237801\n",
            "Step  54200: eval  CrossEntropyLoss |  5.01870918\n",
            "Step  54200: eval          Accuracy |  0.18518519\n",
            "\n",
            "Step  54300: Ran 100 train steps in 48.40 secs\n",
            "Step  54300: train CrossEntropyLoss |  5.22957754\n",
            "Step  54300: eval  CrossEntropyLoss |  5.64496946\n",
            "Step  54300: eval          Accuracy |  0.12631580\n",
            "\n",
            "Step  54400: Ran 100 train steps in 48.41 secs\n",
            "Step  54400: train CrossEntropyLoss |  5.22653532\n",
            "Step  54400: eval  CrossEntropyLoss |  4.86634493\n",
            "Step  54400: eval          Accuracy |  0.21296297\n",
            "\n",
            "Step  54500: Ran 100 train steps in 48.30 secs\n",
            "Step  54500: train CrossEntropyLoss |  5.24829292\n",
            "Step  54500: eval  CrossEntropyLoss |  5.33315277\n",
            "Step  54500: eval          Accuracy |  0.21319796\n",
            "\n",
            "Step  54600: Ran 100 train steps in 48.23 secs\n",
            "Step  54600: train CrossEntropyLoss |  5.21527386\n",
            "Step  54600: eval  CrossEntropyLoss |  4.66399288\n",
            "Step  54600: eval          Accuracy |  0.23577237\n",
            "\n",
            "Step  54700: Ran 100 train steps in 48.39 secs\n",
            "Step  54700: train CrossEntropyLoss |  5.24172401\n",
            "Step  54700: eval  CrossEntropyLoss |  5.29446030\n",
            "Step  54700: eval          Accuracy |  0.21052632\n",
            "\n",
            "Step  54800: Ran 100 train steps in 48.52 secs\n",
            "Step  54800: train CrossEntropyLoss |  5.25557995\n",
            "Step  54800: eval  CrossEntropyLoss |  5.13864851\n",
            "Step  54800: eval          Accuracy |  0.23008850\n",
            "\n",
            "Step  54900: Ran 100 train steps in 48.49 secs\n",
            "Step  54900: train CrossEntropyLoss |  5.19999266\n",
            "Step  54900: eval  CrossEntropyLoss |  5.49298525\n",
            "Step  54900: eval          Accuracy |  0.16129032\n",
            "\n",
            "Step  55000: Ran 100 train steps in 48.02 secs\n",
            "Step  55000: train CrossEntropyLoss |  5.23050642\n",
            "Step  55000: eval  CrossEntropyLoss |  4.96554089\n",
            "Step  55000: eval          Accuracy |  0.24271844\n",
            "\n",
            "Step  55100: Ran 100 train steps in 48.40 secs\n",
            "Step  55100: train CrossEntropyLoss |  5.21959877\n",
            "Step  55100: eval  CrossEntropyLoss |  5.08540106\n",
            "Step  55100: eval          Accuracy |  0.18867925\n",
            "\n",
            "Step  55200: Ran 100 train steps in 47.95 secs\n",
            "Step  55200: train CrossEntropyLoss |  5.25802803\n",
            "Step  55200: eval  CrossEntropyLoss |  5.50314713\n",
            "Step  55200: eval          Accuracy |  0.19130434\n",
            "\n",
            "Step  55300: Ran 100 train steps in 48.25 secs\n",
            "Step  55300: train CrossEntropyLoss |  5.21134806\n",
            "Step  55300: eval  CrossEntropyLoss |  4.81471634\n",
            "Step  55300: eval          Accuracy |  0.23834199\n",
            "\n",
            "Step  55400: Ran 100 train steps in 48.31 secs\n",
            "Step  55400: train CrossEntropyLoss |  5.21804190\n",
            "Step  55400: eval  CrossEntropyLoss |  5.22125340\n",
            "Step  55400: eval          Accuracy |  0.19298247\n",
            "\n",
            "Step  55500: Ran 100 train steps in 48.30 secs\n",
            "Step  55500: train CrossEntropyLoss |  5.21455479\n",
            "Step  55500: eval  CrossEntropyLoss |  5.51643896\n",
            "Step  55500: eval          Accuracy |  0.19565217\n",
            "\n",
            "Step  55600: Ran 100 train steps in 48.24 secs\n",
            "Step  55600: train CrossEntropyLoss |  5.17371750\n",
            "Step  55600: eval  CrossEntropyLoss |  5.87563324\n",
            "Step  55600: eval          Accuracy |  0.14782608\n",
            "\n",
            "Step  55700: Ran 100 train steps in 48.36 secs\n",
            "Step  55700: train CrossEntropyLoss |  5.23340750\n",
            "Step  55700: eval  CrossEntropyLoss |  5.03236580\n",
            "Step  55700: eval          Accuracy |  0.28888890\n",
            "\n",
            "Step  55800: Ran 100 train steps in 48.45 secs\n",
            "Step  55800: train CrossEntropyLoss |  5.24489498\n",
            "Step  55800: eval  CrossEntropyLoss |  5.20087862\n",
            "Step  55800: eval          Accuracy |  0.20909090\n",
            "\n",
            "Step  55900: Ran 100 train steps in 48.42 secs\n",
            "Step  55900: train CrossEntropyLoss |  5.14929056\n",
            "Step  55900: eval  CrossEntropyLoss |  5.79257345\n",
            "Step  55900: eval          Accuracy |  0.09803922\n",
            "\n",
            "Step  56000: Ran 100 train steps in 48.50 secs\n",
            "Step  56000: train CrossEntropyLoss |  5.22809172\n",
            "Step  56000: eval  CrossEntropyLoss |  5.23721838\n",
            "Step  56000: eval          Accuracy |  0.17999999\n",
            "\n",
            "Step  56100: Ran 100 train steps in 48.36 secs\n",
            "Step  56100: train CrossEntropyLoss |  5.15723324\n",
            "Step  56100: eval  CrossEntropyLoss |  5.67317963\n",
            "Step  56100: eval          Accuracy |  0.15642458\n",
            "\n",
            "Step  56200: Ran 100 train steps in 48.57 secs\n",
            "Step  56200: train CrossEntropyLoss |  5.19987297\n",
            "Step  56200: eval  CrossEntropyLoss |  5.46534538\n",
            "Step  56200: eval          Accuracy |  0.21100916\n",
            "\n",
            "Step  56300: Ran 100 train steps in 48.22 secs\n",
            "Step  56300: train CrossEntropyLoss |  5.17510176\n",
            "Step  56300: eval  CrossEntropyLoss |  4.83386230\n",
            "Step  56300: eval          Accuracy |  0.27551019\n",
            "\n",
            "Step  56400: Ran 100 train steps in 48.48 secs\n",
            "Step  56400: train CrossEntropyLoss |  5.11939764\n",
            "Step  56400: eval  CrossEntropyLoss |  5.32250977\n",
            "Step  56400: eval          Accuracy |  0.20476191\n",
            "\n",
            "Step  56500: Ran 100 train steps in 48.53 secs\n",
            "Step  56500: train CrossEntropyLoss |  5.16619778\n",
            "Step  56500: eval  CrossEntropyLoss |  5.87809896\n",
            "Step  56500: eval          Accuracy |  0.20909090\n",
            "\n",
            "Step  56600: Ran 100 train steps in 48.35 secs\n",
            "Step  56600: train CrossEntropyLoss |  5.22241020\n",
            "Step  56600: eval  CrossEntropyLoss |  4.98789358\n",
            "Step  56600: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  56700: Ran 100 train steps in 48.42 secs\n",
            "Step  56700: train CrossEntropyLoss |  5.22201967\n",
            "Step  56700: eval  CrossEntropyLoss |  5.46360970\n",
            "Step  56700: eval          Accuracy |  0.24257426\n",
            "\n",
            "Step  56800: Ran 100 train steps in 48.47 secs\n",
            "Step  56800: train CrossEntropyLoss |  5.10951090\n",
            "Step  56800: eval  CrossEntropyLoss |  5.41113997\n",
            "Step  56800: eval          Accuracy |  0.17757009\n",
            "\n",
            "Step  56900: Ran 100 train steps in 48.44 secs\n",
            "Step  56900: train CrossEntropyLoss |  5.24343920\n",
            "Step  56900: eval  CrossEntropyLoss |  5.27775812\n",
            "Step  56900: eval          Accuracy |  0.18103448\n",
            "\n",
            "Step  57000: Ran 100 train steps in 48.29 secs\n",
            "Step  57000: train CrossEntropyLoss |  5.15761709\n",
            "Step  57000: eval  CrossEntropyLoss |  5.78307629\n",
            "Step  57000: eval          Accuracy |  0.16666667\n",
            "\n",
            "Step  57100: Ran 100 train steps in 48.58 secs\n",
            "Step  57100: train CrossEntropyLoss |  5.17937756\n",
            "Step  57100: eval  CrossEntropyLoss |  5.37685871\n",
            "Step  57100: eval          Accuracy |  0.18181819\n",
            "\n",
            "Step  57200: Ran 100 train steps in 48.53 secs\n",
            "Step  57200: train CrossEntropyLoss |  5.15568542\n",
            "Step  57200: eval  CrossEntropyLoss |  5.24011183\n",
            "Step  57200: eval          Accuracy |  0.23300971\n",
            "\n",
            "Step  57300: Ran 100 train steps in 48.22 secs\n",
            "Step  57300: train CrossEntropyLoss |  5.17348576\n",
            "Step  57300: eval  CrossEntropyLoss |  5.36653423\n",
            "Step  57300: eval          Accuracy |  0.20879121\n",
            "\n",
            "Step  57400: Ran 100 train steps in 48.45 secs\n",
            "Step  57400: train CrossEntropyLoss |  5.19112825\n",
            "Step  57400: eval  CrossEntropyLoss |  5.11066723\n",
            "Step  57400: eval          Accuracy |  0.22872339\n",
            "\n",
            "Step  57500: Ran 100 train steps in 48.40 secs\n",
            "Step  57500: train CrossEntropyLoss |  5.14261580\n",
            "Step  57500: eval  CrossEntropyLoss |  5.00083876\n",
            "Step  57500: eval          Accuracy |  0.23853210\n",
            "\n",
            "Step  57600: Ran 100 train steps in 48.53 secs\n",
            "Step  57600: train CrossEntropyLoss |  5.20030022\n",
            "Step  57600: eval  CrossEntropyLoss |  5.36482573\n",
            "Step  57600: eval          Accuracy |  0.20560747\n",
            "\n",
            "Step  57700: Ran 100 train steps in 48.40 secs\n",
            "Step  57700: train CrossEntropyLoss |  5.15667248\n",
            "Step  57700: eval  CrossEntropyLoss |  5.15210438\n",
            "Step  57700: eval          Accuracy |  0.22680414\n",
            "\n",
            "Step  57800: Ran 100 train steps in 48.53 secs\n",
            "Step  57800: train CrossEntropyLoss |  5.14842892\n",
            "Step  57800: eval  CrossEntropyLoss |  5.26498413\n",
            "Step  57800: eval          Accuracy |  0.19999999\n",
            "\n",
            "Step  57900: Ran 100 train steps in 48.31 secs\n",
            "Step  57900: train CrossEntropyLoss |  5.14967203\n",
            "Step  57900: eval  CrossEntropyLoss |  5.67095709\n",
            "Step  57900: eval          Accuracy |  0.15887851\n",
            "\n",
            "Step  58000: Ran 100 train steps in 48.65 secs\n",
            "Step  58000: train CrossEntropyLoss |  5.08294058\n",
            "Step  58000: eval  CrossEntropyLoss |  5.00236464\n",
            "Step  58000: eval          Accuracy |  0.20833334\n",
            "\n",
            "Step  58100: Ran 100 train steps in 48.31 secs\n",
            "Step  58100: train CrossEntropyLoss |  5.10019588\n",
            "Step  58100: eval  CrossEntropyLoss |  5.37585688\n",
            "Step  58100: eval          Accuracy |  0.19999999\n",
            "\n",
            "Step  58200: Ran 100 train steps in 48.53 secs\n",
            "Step  58200: train CrossEntropyLoss |  5.10747385\n",
            "Step  58200: eval  CrossEntropyLoss |  5.36893368\n",
            "Step  58200: eval          Accuracy |  0.14893617\n",
            "\n",
            "Step  58300: Ran 100 train steps in 48.49 secs\n",
            "Step  58300: train CrossEntropyLoss |  5.11562347\n",
            "Step  58300: eval  CrossEntropyLoss |  4.95891142\n",
            "Step  58300: eval          Accuracy |  0.19327731\n",
            "\n",
            "Step  58400: Ran 100 train steps in 48.35 secs\n",
            "Step  58400: train CrossEntropyLoss |  5.05391216\n",
            "Step  58400: eval  CrossEntropyLoss |  4.65905762\n",
            "Step  58400: eval          Accuracy |  0.24203822\n",
            "\n",
            "Step  58500: Ran 100 train steps in 48.65 secs\n",
            "Step  58500: train CrossEntropyLoss |  5.14299345\n",
            "Step  58500: eval  CrossEntropyLoss |  4.35325480\n",
            "Step  58500: eval          Accuracy |  0.26315790\n",
            "\n",
            "Step  58600: Ran 100 train steps in 48.29 secs\n",
            "Step  58600: train CrossEntropyLoss |  5.10715342\n",
            "Step  58600: eval  CrossEntropyLoss |  4.95227194\n",
            "Step  58600: eval          Accuracy |  0.27619049\n",
            "\n",
            "Step  58700: Ran 100 train steps in 48.66 secs\n",
            "Step  58700: train CrossEntropyLoss |  5.10236979\n",
            "Step  58700: eval  CrossEntropyLoss |  5.26083136\n",
            "Step  58700: eval          Accuracy |  0.18357487\n",
            "\n",
            "Step  58800: Ran 100 train steps in 48.29 secs\n",
            "Step  58800: train CrossEntropyLoss |  5.11275387\n",
            "Step  58800: eval  CrossEntropyLoss |  5.11474037\n",
            "Step  58800: eval          Accuracy |  0.16346155\n",
            "\n",
            "Step  58900: Ran 100 train steps in 48.43 secs\n",
            "Step  58900: train CrossEntropyLoss |  5.10445547\n",
            "Step  58900: eval  CrossEntropyLoss |  5.07853556\n",
            "Step  58900: eval          Accuracy |  0.17796610\n",
            "\n",
            "Step  59000: Ran 100 train steps in 48.46 secs\n",
            "Step  59000: train CrossEntropyLoss |  5.10617352\n",
            "Step  59000: eval  CrossEntropyLoss |  4.66746521\n",
            "Step  59000: eval          Accuracy |  0.23999999\n",
            "\n",
            "Step  59100: Ran 100 train steps in 48.58 secs\n",
            "Step  59100: train CrossEntropyLoss |  5.11909389\n",
            "Step  59100: eval  CrossEntropyLoss |  5.08290052\n",
            "Step  59100: eval          Accuracy |  0.19704433\n",
            "\n",
            "Step  59200: Ran 100 train steps in 48.65 secs\n",
            "Step  59200: train CrossEntropyLoss |  5.09257412\n",
            "Step  59200: eval  CrossEntropyLoss |  5.51074553\n",
            "Step  59200: eval          Accuracy |  0.19047619\n",
            "\n",
            "Step  59300: Ran 100 train steps in 48.16 secs\n",
            "Step  59300: train CrossEntropyLoss |  5.16777134\n",
            "Step  59300: eval  CrossEntropyLoss |  4.62310934\n",
            "Step  59300: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  59400: Ran 100 train steps in 48.34 secs\n",
            "Step  59400: train CrossEntropyLoss |  5.07156992\n",
            "Step  59400: eval  CrossEntropyLoss |  4.94284153\n",
            "Step  59400: eval          Accuracy |  0.18367347\n",
            "\n",
            "Step  59500: Ran 100 train steps in 48.46 secs\n",
            "Step  59500: train CrossEntropyLoss |  5.11687660\n",
            "Step  59500: eval  CrossEntropyLoss |  5.08692217\n",
            "Step  59500: eval          Accuracy |  0.22999999\n",
            "\n",
            "Step  59600: Ran 100 train steps in 48.44 secs\n",
            "Step  59600: train CrossEntropyLoss |  5.10617590\n",
            "Step  59600: eval  CrossEntropyLoss |  4.85458755\n",
            "Step  59600: eval          Accuracy |  0.26237622\n",
            "\n",
            "Step  59700: Ran 100 train steps in 48.45 secs\n",
            "Step  59700: train CrossEntropyLoss |  5.12403297\n",
            "Step  59700: eval  CrossEntropyLoss |  5.48284864\n",
            "Step  59700: eval          Accuracy |  0.13131313\n",
            "\n",
            "Step  59800: Ran 100 train steps in 48.62 secs\n",
            "Step  59800: train CrossEntropyLoss |  5.10190392\n",
            "Step  59800: eval  CrossEntropyLoss |  5.58633375\n",
            "Step  59800: eval          Accuracy |  0.16101696\n",
            "\n",
            "Step  59900: Ran 100 train steps in 48.39 secs\n",
            "Step  59900: train CrossEntropyLoss |  5.08754063\n",
            "Step  59900: eval  CrossEntropyLoss |  5.40864801\n",
            "Step  59900: eval          Accuracy |  0.23008850\n",
            "\n",
            "Step  60000: Ran 100 train steps in 48.67 secs\n",
            "Step  60000: train CrossEntropyLoss |  5.13446140\n",
            "Step  60000: eval  CrossEntropyLoss |  5.41837072\n",
            "Step  60000: eval          Accuracy |  0.18055555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "# !rsync -a /content/drive/MyDrive/model2/ ~/\r\n",
        "\r\n",
        "# copy the model to Google Drive\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model/\r\n",
        "\r\n",
        "# sync Google Drive dir with the train dir\r\n",
        "# !rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e771de8b-9d1d-4f07-9c75-6a608bbe91c0"
      },
      "source": [
        "train_text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('свидетелям обвинения по делу об убийстве оппозиционера бориса немцова поступали угрозы во время разбирательства в московском окружном военном суде. об этом агентству тасс рассказала прокурор по уголовному делу мария семененко. «да, было. по крайней мере, нам достоверно известно о нескольких таких случаях в отношении свидетелей», — отметила она. семененко не стала отвечать на вопрос, находится ли кто-то из свидетелей под государственной защитой. по ее словам, такая информация является закрытой. в четверг, 13 июля, суд приговорил заура дадаева, признанного виновным в убийстве политика, к 20 годам лишения свободы в колонии строгого режима. помимо дадаева, по делу проходят темирлан эскерханов, а также братья анзор и шадид губашевы и хамзат бахаев. соучастники преступления получили сроки от 11 до 19 лет. немцова застрелили на большом москворецком мосту в центре столицы 27 февраля 2015 года. по версии следствия, руслан мухудинов, предполагаемый организатор преступления, предложил исполнителям 15 миллионов рублей за убийство, он объявлен в международный розыск. киллером следствие и суд сочли дадаева.',\n",
              " 'прокурор рассказала про угрозы свидетелям по\\xa0делу об\\xa0убийстве немцова')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYS0jHDJkAd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166f1956-081e-46fb-bc93-e59e6c884b33"
      },
      "source": [
        "eval_text_pairs[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('московский «локомотив» сыграл вничью с тульским «арсеналом» в матче 9-го тура российской футбольной премьер-лиги (рфпл). об этом в субботу, 1 октября, сообщается на сайте турнира. встреча, прошедшая на стадионе «локомотив» в москве, завершилась со счетом 1:1. гости вышли вперед на 44-й минуте, отличился фелицио браун форбс. железнодорожники ушли от поражения на последней минуте матча — александр самедов реализовал пенальти на ведране чорлуке. железнодорожники набрали  8-е очко и остались на 13-м месте в турнирной таблице. «арсенал» имеет на одно очко меньше и располагается строчкой ниже. подопечные юрия семина прервали серию из трех поражений в рфпл подряд. ранее «локомотив» последовательно уступал «спартаку», «уфе» и «ростову» с одинаковым счетом 0:1. 29 сентября на встрече с болельщиками столичного клуба семин извинился перед поклонниками за неудовлетворительный результат команды в последнее время. семин был назначен главным тренером «локомотива» 26 августа. с тех пор клуб одержал одну победу и уступил трижды при одной ничьей.',\n",
              " '«локомотив» сыграл вничью с «арсеналом» и\\xa0прервал серию из\\xa0трех поражений кряду')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU9OvGMPiVLR"
      },
      "source": [
        "# fixed input for comparison purposes\r\n",
        "train_input = \"свидетелям обвинения по делу об убийстве оппозиционера бориса немцова поступали угрозы во время разбирательства в московском окружном военном суде. об этом агентству тасс рассказала прокурор по уголовному делу мария семененко. «да, было. по крайней мере, нам достоверно известно о нескольких таких случаях в отношении свидетелей», — отметила она. семененко не стала отвечать на вопрос, находится ли кто-то из свидетелей под государственной защитой. по ее словам, такая информация является закрытой. в четверг, 13 июля, суд приговорил заура дадаева, признанного виновным в убийстве политика, к 20 годам лишения свободы в колонии строгого режима. помимо дадаева, по делу проходят темирлан эскерханов, а также братья анзор и шадид губашевы и хамзат бахаев. соучастники преступления получили сроки от 11 до 19 лет. немцова застрелили на большом москворецком мосту в центре столицы 27 февраля 2015 года. по версии следствия, руслан мухудинов, предполагаемый организатор преступления, предложил исполнителям 15 миллионов рублей за убийство, он объявлен в международный розыск. киллером следствие и суд сочли дадаева.\"\r\n",
        "eval_input = \"московский «локомотив» сыграл вничью с тульским «арсеналом» в матче 9-го тура российской футбольной премьер-лиги (рфпл). об этом в субботу, 1 октября, сообщается на сайте турнира. встреча, прошедшая на стадионе «локомотив» в москве, завершилась со счетом 1:1. гости вышли вперед на 44-й минуте, отличился фелицио браун форбс. железнодорожники ушли от поражения на последней минуте матча — александр самедов реализовал пенальти на ведране чорлуке. железнодорожники набрали  8-е очко и остались на 13-м месте в турнирной таблице. «арсенал» имеет на одно очко меньше и располагается строчкой ниже. подопечные юрия семина прервали серию из трех поражений в рфпл подряд. ранее «локомотив» последовательно уступал «спартаку», «уфе» и «ростову» с одинаковым счетом 0:1. 29 сентября на встрече с болельщиками столичного клуба семин извинился перед поклонниками за неудовлетворительный результат команды в последнее время. семин был назначен главным тренером «локомотива» 26 августа. с тех пор клуб одержал одну победу и уступил трижды при одной ничьей.\""
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV-2j7AKT1pT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d2ca85-14a2-4f8c-dd88-854c90361fa9"
      },
      "source": [
        "print('прокурор рассказала про угрозы свидетелям по\\xa0делу об\\xa0убийстве немцова')\r\n",
        "print('')\r\n",
        "print(wrapper.fill(train_input))\r\n",
        "print('')\r\n",
        "print('«локомотив» сыграл вничью с «арсеналом» и\\xa0прервал серию из\\xa0трех поражений кряду')\r\n",
        "print('')\r\n",
        "print(wrapper.fill(eval_input))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "прокурор рассказала про угрозы свидетелям по делу об убийстве немцова\n",
            "\n",
            "свидетелям обвинения по делу об убийстве оппозиционера бориса немцова\n",
            "поступали угрозы во время разбирательства в московском окружном\n",
            "военном суде. об этом агентству тасс рассказала прокурор по уголовному\n",
            "делу мария семененко. «да, было. по крайней мере, нам достоверно\n",
            "известно о нескольких таких случаях в отношении свидетелей», —\n",
            "отметила она. семененко не стала отвечать на вопрос, находится ли кто-\n",
            "то из свидетелей под государственной защитой. по ее словам, такая\n",
            "информация является закрытой. в четверг, 13 июля, суд приговорил заура\n",
            "дадаева, признанного виновным в убийстве политика, к 20 годам лишения\n",
            "свободы в колонии строгого режима. помимо дадаева, по делу проходят\n",
            "темирлан эскерханов, а также братья анзор и шадид губашевы и хамзат\n",
            "бахаев. соучастники преступления получили сроки от 11 до 19 лет.\n",
            "немцова застрелили на большом москворецком мосту в центре столицы 27\n",
            "февраля 2015 года. по версии следствия, руслан мухудинов,\n",
            "предполагаемый организатор преступления, предложил исполнителям 15\n",
            "миллионов рублей за убийство, он объявлен в международный розыск.\n",
            "киллером следствие и суд сочли дадаева.\n",
            "\n",
            "«локомотив» сыграл вничью с «арсеналом» и прервал серию из трех поражений кряду\n",
            "\n",
            "московский «локомотив» сыграл вничью с тульским «арсеналом» в матче\n",
            "9-го тура российской футбольной премьер-лиги (рфпл). об этом в\n",
            "субботу, 1 октября, сообщается на сайте турнира. встреча, прошедшая на\n",
            "стадионе «локомотив» в москве, завершилась со счетом 1:1. гости вышли\n",
            "вперед на 44-й минуте, отличился фелицио браун форбс. железнодорожники\n",
            "ушли от поражения на последней минуте матча — александр самедов\n",
            "реализовал пенальти на ведране чорлуке. железнодорожники набрали  8-е\n",
            "очко и остались на 13-м месте в турнирной таблице. «арсенал» имеет на\n",
            "одно очко меньше и располагается строчкой ниже. подопечные юрия семина\n",
            "прервали серию из трех поражений в рфпл подряд. ранее «локомотив»\n",
            "последовательно уступал «спартаку», «уфе» и «ростову» с одинаковым\n",
            "счетом 0:1. 29 сентября на встрече с болельщиками столичного клуба\n",
            "семин извинился перед поклонниками за неудовлетворительный результат\n",
            "команды в последнее время. семин был назначен главным тренером\n",
            "«локомотива» 26 августа. с тех пор клуб одержал одну победу и уступил\n",
            "трижды при одной ничьей.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "526e6113-b168-49df-f4c3-46fdea4d2c5f"
      },
      "source": [
        "print(detokenize([next_symbol(tokenize(train_input)+[0], model)]))\r\n",
        "print(detokenize([next_symbol(tokenize(eval_input)+[0], model)]))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "суд\n",
            "«\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "        if len(generated_output) >= 20:\r\n",
        "            print(detokenize(generated_output))\r\n",
        "            break\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "    \r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb93057-af00-4400-a08a-a45bea57f815"
      },
      "source": [
        "print(greedy_decode(train_input, model))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "суд\n",
            "суд признал\n",
            "суд признал приговор\n",
            "суд признал приговор по\n",
            "суд признал приговор по делу\n",
            "суд признал приговор по делу о\n",
            "суд признал приговор по делу о коррупции\n",
            "суд признал приговор по делу о коррупции в\n",
            "суд признал приговор по делу о коррупции в сизо\n",
            "суд признал приговор по делу о коррупции в сизо\n",
            "суд признал приговор по делу о коррупции в сизо\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037cce04-db6d-424c-ba51-c82a8619588e"
      },
      "source": [
        "print(greedy_decode(eval_input, model))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "«\n",
            "«зенит\n",
            "«зенит»\n",
            "«зенит» проиграл\n",
            "«зенит» проиграл «\n",
            "«зенит» проиграл «динамо\n",
            "«зенит» проиграл «динамо»\n",
            "«зенит» проиграл «динамо» в\n",
            "«зенит» проиграл «динамо» в матче\n",
            "«зенит» проиграл «динамо» в матче «\n",
            "«зенит» проиграл «динамо» в матче «динамо\n",
            "«зенит» проиграл «динамо» в матче «динамо»\n",
            "«зенит» проиграл «динамо» в матче «динамо»\n",
            "«зенит» проиграл «динамо» в матче «динамо»\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7c94ce-537c-4c1a-8f2f-88ced7a47c66"
      },
      "source": [
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(tokenize(eval_input))[None, :],\r\n",
        "                                        temperature=0.5, max_length=20)\r\n",
        "print(wrapper.fill(detokenize(output[0])))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ку под вный туристов медведев один военный стра евро казахстан\n",
            "полицейский лидер индий вный гол обстрелялиный у\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}