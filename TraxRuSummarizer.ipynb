{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOCXjmJUTyf6jr70VsDwySB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5684f59-3ad5-4d06-aa1a-f1c69be7644e"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: trax in /usr/local/lib/python3.6/dist-packages (1.3.7)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from trax) (0.2.9)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.6/dist-packages (from trax) (2.4.3)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: t5 in /usr/local/lib/python3.6/dist-packages (from trax) (0.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trax) (0.10.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax) (0.1.60+cuda101)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from trax) (0.4.0)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.6/dist-packages (from trax) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trax) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (5.1.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.1.5)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (20.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.27.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax) (0.11.0)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Requirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from t5->trax) (4.3.2)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax) (3.2.5)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.5.0)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.6/dist-packages (from t5->trax) (4.2.0.dev202102170106)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.7.0+cu101)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.1.95)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.1.5)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax) (2.9.0)\n",
            "Requirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.1.18)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.6/dist-packages (from jaxlib->trax) (1.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.52.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (53.0.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->t5->trax) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2018.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.25.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "04b06e99-f792-45a0-cb58-fbd35203ea26"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "# data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "# data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "# data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "# data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "# text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "# for i in tqdm(range(data.shape[0])):\r\n",
        "    # if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        # text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafd90d3-592e-44a8-d602-30424903033f"
      },
      "source": [
        "# text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "9a443a1c-929d-46b7-a380-16c307c65131"
      },
      "source": [
        "# s0 = text_pairs[10][0]\r\n",
        "# text_list = wrapper.wrap(s0[:300])\r\n",
        "# for line in text_list:\r\n",
        "#     print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLsMmUFFnHA",
        "outputId": "d93e6f44-9835-4054-ee04-623f81c09d10"
      },
      "source": [
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n",
            "в киеве исключили новый раунд минских переговоров\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "c10aada5-9f34-4224-fbe3-458f6b279d70"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC79r_7EPy6",
        "outputId": "f9964884-b0ed-4faf-97e3-3725b2db0c58"
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e3b950-3a58-452b-99cb-dcb97fd957ab"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1656cb93-3586-41cb-d1ab-938660393b90"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b59cafe-b249-42e4-c977-488e1c060f89"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        "    trax.data.FilterByLength(2048)\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eda1e43-cbb5-4542-bfc1-799fbfc20b76"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 6277  2286 15934    46  4302  1378    29 10589   153    83   110   124\n",
            "    12  9142 15945  2870  9642   471 15972     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [512, 1024]\r\n",
        "batch_sizes = [8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "b68e2d0a-1677-4266-e5bb-79ff078a0179"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "7dc0aa73-1eef-47d7-88a6-ee02f2dcfb51"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15754,  2798,  4150,  2728, 15930, 11128,    17,  3137,  1187,\n",
              "       12357,    25,  1789,  8611,  5287,  5330,    70,   645,  2550,\n",
              "         672, 15949,    63,   226,   258,   993,  3528, 11352, 15949,\n",
              "        1206,   945,  9307,   719,  4806,  1299,   653,  6513,    81,\n",
              "       15484,  1787,    16,  5757,   113,  7542, 15947,  2865, 15957,\n",
              "        4027,   719, 15949,   945,    63,  2232,   262,  6045, 13632,\n",
              "        7989,     4,  1238,  1056, 15949,   498, 15754,  2798,  4150,\n",
              "        2370,   177,  5450,  1966,  6735,  2841,   358, 11128, 15949,\n",
              "        3239,   177,  9168,   657,  1966,  6205,  1528,  1141,  1802,\n",
              "         173,    70,   645,  2595, 15985,   672, 15945,    79,  1248,\n",
              "        9913,  1597,    46,  4130,   544, 15949,  9168,    17, 11412,\n",
              "        1864, 11279,    25,  7124,  1276,  3983,   245,   441,  2990,\n",
              "       15980,   672, 15949, 15754,  2798,  4150,   277,  2728, 15934,\n",
              "        4263,    64,  4298,    45, 15937,  2691, 15945,   769,  2018,\n",
              "        4283, 15945,    25,   759,  4379, 15949,  6090,    59,  4963,\n",
              "         141,  2418, 15945,   570,  5130, 15960,  1320,    61,  1021,\n",
              "        1605,  4082,  2743,     5,  4298,    97, 15945,  2454,  2235,\n",
              "           4,  5780,   173, 15949,     4,  2074,   173,  2798,  1966,\n",
              "        6735,  4263,    64,  4298,    45, 15937,  2691,  1175,   177,\n",
              "       15949,  4354,  4150,  9650,    22,  6045,    57,    25,  3717,\n",
              "       11665,    16, 12805,   672, 15945,   207,     5,  6076,  4279,\n",
              "       15945,    41,    25,  4379, 15945,   715,  3238,  5956, 15949,\n",
              "           5,  6107,  4623, 15945,    79,  3023,  4937,    91, 11918,\n",
              "        5327,     4, 12384, 15927, 15949,  3086,  8189,  1829,   917,\n",
              "        1791, 14668,  2582,  2707,    25,  5288,  8637,  4425,  4027,\n",
              "         719, 15949,    46,  3372,  2370,  2670,  4080,   173,  2865,\n",
              "        4027, 11338,  7938,    70,   741,  2595,   672, 15945,     5,\n",
              "         128,   368,   207,    17,  2293,  1802,   173,  3612,   964,\n",
              "        2826,   672, 15949,  9260,  4150,  4623,  3866,  4032,  1367,\n",
              "        1445, 14192,  4544,  1510,  1193, 15949,   554,   799,   544,\n",
              "       12928,  4425,  4928,     5,   226, 10632,   462,   608,   584,\n",
              "        2110,  7907,   589,   118,     5,   741,   617, 15949,     1,\n",
              "           0,  9307,  4150,  2728, 15930,  2841,   358, 11128,  2235,\n",
              "          46,  1036,     1,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "cc710452-fb09-46f3-ec99-a1c18f867eb6"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.0001)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.0001), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "5d792407-c5fe-4010-b3a3-81e5309014b1"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  200100: Ran 100 train steps in 67.63 secs\n",
            "Step  200100: train CrossEntropyLoss |  3.69951987\n",
            "Step  200100: eval  CrossEntropyLoss |  4.54895258\n",
            "Step  200100: eval          Accuracy |  0.32673267\n",
            "\n",
            "Step  200200: Ran 100 train steps in 46.81 secs\n",
            "Step  200200: train CrossEntropyLoss |  3.59910941\n",
            "Step  200200: eval  CrossEntropyLoss |  4.03400993\n",
            "Step  200200: eval          Accuracy |  0.33035716\n",
            "\n",
            "Step  200300: Ran 100 train steps in 47.15 secs\n",
            "Step  200300: train CrossEntropyLoss |  3.55021024\n",
            "Step  200300: eval  CrossEntropyLoss |  2.62068462\n",
            "Step  200300: eval          Accuracy |  0.52475244\n",
            "\n",
            "Step  200400: Ran 100 train steps in 64.77 secs\n",
            "Step  200400: train CrossEntropyLoss |  3.56078911\n",
            "Step  200400: eval  CrossEntropyLoss |  3.79611635\n",
            "Step  200400: eval          Accuracy |  0.38181818\n",
            "\n",
            "Step  200500: Ran 100 train steps in 47.87 secs\n",
            "Step  200500: train CrossEntropyLoss |  3.54658675\n",
            "Step  200500: eval  CrossEntropyLoss |  3.01793671\n",
            "Step  200500: eval          Accuracy |  0.49473685\n",
            "\n",
            "Step  200600: Ran 100 train steps in 47.98 secs\n",
            "Step  200600: train CrossEntropyLoss |  3.37228775\n",
            "Step  200600: eval  CrossEntropyLoss |  3.95324802\n",
            "Step  200600: eval          Accuracy |  0.39823008\n",
            "\n",
            "Step  200700: Ran 100 train steps in 47.95 secs\n",
            "Step  200700: train CrossEntropyLoss |  3.37907243\n",
            "Step  200700: eval  CrossEntropyLoss |  3.34492970\n",
            "Step  200700: eval          Accuracy |  0.47422683\n",
            "\n",
            "Step  200800: Ran 100 train steps in 48.08 secs\n",
            "Step  200800: train CrossEntropyLoss |  3.37106538\n",
            "Step  200800: eval  CrossEntropyLoss |  3.22847891\n",
            "Step  200800: eval          Accuracy |  0.54081631\n",
            "\n",
            "Step  200900: Ran 100 train steps in 47.94 secs\n",
            "Step  200900: train CrossEntropyLoss |  3.18852186\n",
            "Step  200900: eval  CrossEntropyLoss |  3.96683002\n",
            "Step  200900: eval          Accuracy |  0.37000000\n",
            "\n",
            "Step  201000: Ran 100 train steps in 48.27 secs\n",
            "Step  201000: train CrossEntropyLoss |  3.26757908\n",
            "Step  201000: eval  CrossEntropyLoss |  3.12695146\n",
            "Step  201000: eval          Accuracy |  0.51063830\n",
            "\n",
            "Step  201100: Ran 100 train steps in 48.09 secs\n",
            "Step  201100: train CrossEntropyLoss |  3.31978631\n",
            "Step  201100: eval  CrossEntropyLoss |  3.49130177\n",
            "Step  201100: eval          Accuracy |  0.46236560\n",
            "\n",
            "Step  201200: Ran 100 train steps in 48.24 secs\n",
            "Step  201200: train CrossEntropyLoss |  3.22868443\n",
            "Step  201200: eval  CrossEntropyLoss |  3.92514968\n",
            "Step  201200: eval          Accuracy |  0.37962964\n",
            "\n",
            "Step  201300: Ran 100 train steps in 48.09 secs\n",
            "Step  201300: train CrossEntropyLoss |  3.20825243\n",
            "Step  201300: eval  CrossEntropyLoss |  3.57404160\n",
            "Step  201300: eval          Accuracy |  0.39090908\n",
            "\n",
            "Step  201400: Ran 100 train steps in 48.73 secs\n",
            "Step  201400: train CrossEntropyLoss |  3.21302509\n",
            "Step  201400: eval  CrossEntropyLoss |  3.98854327\n",
            "Step  201400: eval          Accuracy |  0.37864077\n",
            "\n",
            "Step  201500: Ran 100 train steps in 48.12 secs\n",
            "Step  201500: train CrossEntropyLoss |  3.15991163\n",
            "Step  201500: eval  CrossEntropyLoss |  3.86380172\n",
            "Step  201500: eval          Accuracy |  0.39252335\n",
            "\n",
            "Step  201600: Ran 100 train steps in 48.04 secs\n",
            "Step  201600: train CrossEntropyLoss |  3.17534304\n",
            "Step  201600: eval  CrossEntropyLoss |  3.60061026\n",
            "Step  201600: eval          Accuracy |  0.42553189\n",
            "\n",
            "Step  201700: Ran 100 train steps in 48.23 secs\n",
            "Step  201700: train CrossEntropyLoss |  3.13962483\n",
            "Step  201700: eval  CrossEntropyLoss |  4.33381462\n",
            "Step  201700: eval          Accuracy |  0.37606838\n",
            "\n",
            "Step  201800: Ran 100 train steps in 48.03 secs\n",
            "Step  201800: train CrossEntropyLoss |  3.16725254\n",
            "Step  201800: eval  CrossEntropyLoss |  3.06743550\n",
            "Step  201800: eval          Accuracy |  0.44859812\n",
            "\n",
            "Step  201900: Ran 100 train steps in 48.35 secs\n",
            "Step  201900: train CrossEntropyLoss |  3.18406582\n",
            "Step  201900: eval  CrossEntropyLoss |  3.58959699\n",
            "Step  201900: eval          Accuracy |  0.47368422\n",
            "\n",
            "Step  202000: Ran 100 train steps in 48.28 secs\n",
            "Step  202000: train CrossEntropyLoss |  3.11069512\n",
            "Step  202000: eval  CrossEntropyLoss |  3.24920535\n",
            "Step  202000: eval          Accuracy |  0.52727270\n",
            "\n",
            "Step  202100: Ran 100 train steps in 48.34 secs\n",
            "Step  202100: train CrossEntropyLoss |  3.11763334\n",
            "Step  202100: eval  CrossEntropyLoss |  2.95918536\n",
            "Step  202100: eval          Accuracy |  0.50892860\n",
            "\n",
            "Step  202200: Ran 100 train steps in 48.60 secs\n",
            "Step  202200: train CrossEntropyLoss |  3.16263723\n",
            "Step  202200: eval  CrossEntropyLoss |  3.66478848\n",
            "Step  202200: eval          Accuracy |  0.44000000\n",
            "\n",
            "Step  202300: Ran 100 train steps in 48.38 secs\n",
            "Step  202300: train CrossEntropyLoss |  3.07523251\n",
            "Step  202300: eval  CrossEntropyLoss |  3.84138703\n",
            "Step  202300: eval          Accuracy |  0.42372882\n",
            "\n",
            "Step  202400: Ran 100 train steps in 48.23 secs\n",
            "Step  202400: train CrossEntropyLoss |  3.09701681\n",
            "Step  202400: eval  CrossEntropyLoss |  3.28689766\n",
            "Step  202400: eval          Accuracy |  0.46902654\n",
            "\n",
            "Step  202500: Ran 100 train steps in 48.43 secs\n",
            "Step  202500: train CrossEntropyLoss |  3.07273126\n",
            "Step  202500: eval  CrossEntropyLoss |  3.12223554\n",
            "Step  202500: eval          Accuracy |  0.50495046\n",
            "\n",
            "Step  202600: Ran 100 train steps in 48.20 secs\n",
            "Step  202600: train CrossEntropyLoss |  3.07067132\n",
            "Step  202600: eval  CrossEntropyLoss |  2.81987762\n",
            "Step  202600: eval          Accuracy |  0.53043479\n",
            "\n",
            "Step  202700: Ran 100 train steps in 48.36 secs\n",
            "Step  202700: train CrossEntropyLoss |  3.07741642\n",
            "Step  202700: eval  CrossEntropyLoss |  3.02749991\n",
            "Step  202700: eval          Accuracy |  0.51401865\n",
            "\n",
            "Step  202800: Ran 100 train steps in 48.11 secs\n",
            "Step  202800: train CrossEntropyLoss |  3.10818648\n",
            "Step  202800: eval  CrossEntropyLoss |  2.91531467\n",
            "Step  202800: eval          Accuracy |  0.51612902\n",
            "\n",
            "Step  202900: Ran 100 train steps in 48.20 secs\n",
            "Step  202900: train CrossEntropyLoss |  3.04112887\n",
            "Step  202900: eval  CrossEntropyLoss |  2.96100569\n",
            "Step  202900: eval          Accuracy |  0.55769235\n",
            "\n",
            "Step  203000: Ran 100 train steps in 48.30 secs\n",
            "Step  203000: train CrossEntropyLoss |  2.99841857\n",
            "Step  203000: eval  CrossEntropyLoss |  2.62136292\n",
            "Step  203000: eval          Accuracy |  0.56435645\n",
            "\n",
            "Step  203100: Ran 100 train steps in 48.19 secs\n",
            "Step  203100: train CrossEntropyLoss |  3.05008793\n",
            "Step  203100: eval  CrossEntropyLoss |  2.75354671\n",
            "Step  203100: eval          Accuracy |  0.51685393\n",
            "\n",
            "Step  203200: Ran 100 train steps in 48.18 secs\n",
            "Step  203200: train CrossEntropyLoss |  3.03258467\n",
            "Step  203200: eval  CrossEntropyLoss |  3.51776218\n",
            "Step  203200: eval          Accuracy |  0.44859812\n",
            "\n",
            "Step  203300: Ran 100 train steps in 48.27 secs\n",
            "Step  203300: train CrossEntropyLoss |  3.07463408\n",
            "Step  203300: eval  CrossEntropyLoss |  3.53694916\n",
            "Step  203300: eval          Accuracy |  0.42574257\n",
            "\n",
            "Step  203400: Ran 100 train steps in 48.22 secs\n",
            "Step  203400: train CrossEntropyLoss |  3.04804921\n",
            "Step  203400: eval  CrossEntropyLoss |  3.19770575\n",
            "Step  203400: eval          Accuracy |  0.48421055\n",
            "\n",
            "Step  203500: Ran 100 train steps in 48.34 secs\n",
            "Step  203500: train CrossEntropyLoss |  3.00923109\n",
            "Step  203500: eval  CrossEntropyLoss |  3.15682244\n",
            "Step  203500: eval          Accuracy |  0.48514852\n",
            "\n",
            "Step  203600: Ran 100 train steps in 48.15 secs\n",
            "Step  203600: train CrossEntropyLoss |  3.07738376\n",
            "Step  203600: eval  CrossEntropyLoss |  2.89160824\n",
            "Step  203600: eval          Accuracy |  0.53409094\n",
            "\n",
            "Step  203700: Ran 100 train steps in 48.16 secs\n",
            "Step  203700: train CrossEntropyLoss |  3.05736566\n",
            "Step  203700: eval  CrossEntropyLoss |  3.49822807\n",
            "Step  203700: eval          Accuracy |  0.38613862\n",
            "\n",
            "Step  203800: Ran 100 train steps in 48.41 secs\n",
            "Step  203800: train CrossEntropyLoss |  3.02510929\n",
            "Step  203800: eval  CrossEntropyLoss |  3.01972437\n",
            "Step  203800: eval          Accuracy |  0.48913044\n",
            "\n",
            "Step  203900: Ran 100 train steps in 48.22 secs\n",
            "Step  203900: train CrossEntropyLoss |  3.00095201\n",
            "Step  203900: eval  CrossEntropyLoss |  3.37982535\n",
            "Step  203900: eval          Accuracy |  0.44230771\n",
            "\n",
            "Step  204000: Ran 100 train steps in 48.41 secs\n",
            "Step  204000: train CrossEntropyLoss |  3.07593489\n",
            "Step  204000: eval  CrossEntropyLoss |  3.29365301\n",
            "Step  204000: eval          Accuracy |  0.49019611\n",
            "\n",
            "Step  204100: Ran 100 train steps in 48.18 secs\n",
            "Step  204100: train CrossEntropyLoss |  3.01504445\n",
            "Step  204100: eval  CrossEntropyLoss |  3.83444619\n",
            "Step  204100: eval          Accuracy |  0.37500003\n",
            "\n",
            "Step  204200: Ran 100 train steps in 48.29 secs\n",
            "Step  204200: train CrossEntropyLoss |  2.99116397\n",
            "Step  204200: eval  CrossEntropyLoss |  3.01856661\n",
            "Step  204200: eval          Accuracy |  0.49532709\n",
            "\n",
            "Step  204300: Ran 100 train steps in 48.22 secs\n",
            "Step  204300: train CrossEntropyLoss |  2.99367189\n",
            "Step  204300: eval  CrossEntropyLoss |  2.99779558\n",
            "Step  204300: eval          Accuracy |  0.47747749\n",
            "\n",
            "Step  204400: Ran 100 train steps in 48.18 secs\n",
            "Step  204400: train CrossEntropyLoss |  2.97184110\n",
            "Step  204400: eval  CrossEntropyLoss |  3.24455547\n",
            "Step  204400: eval          Accuracy |  0.44186047\n",
            "\n",
            "Step  204500: Ran 100 train steps in 48.12 secs\n",
            "Step  204500: train CrossEntropyLoss |  2.96508574\n",
            "Step  204500: eval  CrossEntropyLoss |  2.91092038\n",
            "Step  204500: eval          Accuracy |  0.50434780\n",
            "\n",
            "Step  204600: Ran 100 train steps in 48.22 secs\n",
            "Step  204600: train CrossEntropyLoss |  3.05888057\n",
            "Step  204600: eval  CrossEntropyLoss |  3.60399103\n",
            "Step  204600: eval          Accuracy |  0.37078652\n",
            "\n",
            "Step  204700: Ran 100 train steps in 48.16 secs\n",
            "Step  204700: train CrossEntropyLoss |  3.01383042\n",
            "Step  204700: eval  CrossEntropyLoss |  2.67805409\n",
            "Step  204700: eval          Accuracy |  0.55670106\n",
            "\n",
            "Step  204800: Ran 100 train steps in 66.22 secs\n",
            "Step  204800: train CrossEntropyLoss |  2.96417546\n",
            "Step  204800: eval  CrossEntropyLoss |  2.93326616\n",
            "Step  204800: eval          Accuracy |  0.52631581\n",
            "\n",
            "Step  204900: Ran 100 train steps in 48.27 secs\n",
            "Step  204900: train CrossEntropyLoss |  3.01372409\n",
            "Step  204900: eval  CrossEntropyLoss |  3.51320934\n",
            "Step  204900: eval          Accuracy |  0.44329900\n",
            "\n",
            "Step  205000: Ran 100 train steps in 48.25 secs\n",
            "Step  205000: train CrossEntropyLoss |  2.96782351\n",
            "Step  205000: eval  CrossEntropyLoss |  3.61334229\n",
            "Step  205000: eval          Accuracy |  0.46534652\n",
            "\n",
            "Step  205100: Ran 100 train steps in 48.18 secs\n",
            "Step  205100: train CrossEntropyLoss |  3.00075459\n",
            "Step  205100: eval  CrossEntropyLoss |  2.97887588\n",
            "Step  205100: eval          Accuracy |  0.50925928\n",
            "\n",
            "Step  205200: Ran 100 train steps in 47.86 secs\n",
            "Step  205200: train CrossEntropyLoss |  3.00557852\n",
            "Step  205200: eval  CrossEntropyLoss |  2.93709993\n",
            "Step  205200: eval          Accuracy |  0.41052634\n",
            "\n",
            "Step  205300: Ran 100 train steps in 48.25 secs\n",
            "Step  205300: train CrossEntropyLoss |  2.96399021\n",
            "Step  205300: eval  CrossEntropyLoss |  3.36692166\n",
            "Step  205300: eval          Accuracy |  0.46846849\n",
            "\n",
            "Step  205400: Ran 100 train steps in 48.09 secs\n",
            "Step  205400: train CrossEntropyLoss |  3.01687527\n",
            "Step  205400: eval  CrossEntropyLoss |  2.65655613\n",
            "Step  205400: eval          Accuracy |  0.54285717\n",
            "\n",
            "Step  205500: Ran 100 train steps in 48.28 secs\n",
            "Step  205500: train CrossEntropyLoss |  2.99221349\n",
            "Step  205500: eval  CrossEntropyLoss |  2.90126109\n",
            "Step  205500: eval          Accuracy |  0.49473685\n",
            "\n",
            "Step  205600: Ran 100 train steps in 48.26 secs\n",
            "Step  205600: train CrossEntropyLoss |  2.94516301\n",
            "Step  205600: eval  CrossEntropyLoss |  3.40345788\n",
            "Step  205600: eval          Accuracy |  0.44444445\n",
            "\n",
            "Step  205700: Ran 100 train steps in 48.30 secs\n",
            "Step  205700: train CrossEntropyLoss |  2.97849846\n",
            "Step  205700: eval  CrossEntropyLoss |  3.10040069\n",
            "Step  205700: eval          Accuracy |  0.54285717\n",
            "\n",
            "Step  205800: Ran 100 train steps in 48.42 secs\n",
            "Step  205800: train CrossEntropyLoss |  2.91294360\n",
            "Step  205800: eval  CrossEntropyLoss |  3.08030486\n",
            "Step  205800: eval          Accuracy |  0.48958334\n",
            "\n",
            "Step  205900: Ran 100 train steps in 48.05 secs\n",
            "Step  205900: train CrossEntropyLoss |  2.95080161\n",
            "Step  205900: eval  CrossEntropyLoss |  2.63604069\n",
            "Step  205900: eval          Accuracy |  0.59047621\n",
            "\n",
            "Step  206000: Ran 100 train steps in 48.34 secs\n",
            "Step  206000: train CrossEntropyLoss |  2.91745305\n",
            "Step  206000: eval  CrossEntropyLoss |  3.07720542\n",
            "Step  206000: eval          Accuracy |  0.46153849\n",
            "\n",
            "Step  206100: Ran 100 train steps in 48.12 secs\n",
            "Step  206100: train CrossEntropyLoss |  3.04049182\n",
            "Step  206100: eval  CrossEntropyLoss |  2.71481609\n",
            "Step  206100: eval          Accuracy |  0.58823532\n",
            "\n",
            "Step  206200: Ran 100 train steps in 48.19 secs\n",
            "Step  206200: train CrossEntropyLoss |  2.96976614\n",
            "Step  206200: eval  CrossEntropyLoss |  3.68345928\n",
            "Step  206200: eval          Accuracy |  0.44827586\n",
            "\n",
            "Step  206300: Ran 100 train steps in 48.15 secs\n",
            "Step  206300: train CrossEntropyLoss |  2.98490500\n",
            "Step  206300: eval  CrossEntropyLoss |  3.18274736\n",
            "Step  206300: eval          Accuracy |  0.41758242\n",
            "\n",
            "Step  206400: Ran 100 train steps in 48.17 secs\n",
            "Step  206400: train CrossEntropyLoss |  2.95088363\n",
            "Step  206400: eval  CrossEntropyLoss |  3.12230110\n",
            "Step  206400: eval          Accuracy |  0.52542371\n",
            "\n",
            "Step  206500: Ran 100 train steps in 48.36 secs\n",
            "Step  206500: train CrossEntropyLoss |  3.03423548\n",
            "Step  206500: eval  CrossEntropyLoss |  2.72241068\n",
            "Step  206500: eval          Accuracy |  0.52631581\n",
            "\n",
            "Step  206600: Ran 100 train steps in 48.19 secs\n",
            "Step  206600: train CrossEntropyLoss |  2.96187162\n",
            "Step  206600: eval  CrossEntropyLoss |  3.31358361\n",
            "Step  206600: eval          Accuracy |  0.50450450\n",
            "\n",
            "Step  206700: Ran 100 train steps in 48.14 secs\n",
            "Step  206700: train CrossEntropyLoss |  2.98474836\n",
            "Step  206700: eval  CrossEntropyLoss |  2.73528886\n",
            "Step  206700: eval          Accuracy |  0.51923078\n",
            "\n",
            "Step  206800: Ran 100 train steps in 47.97 secs\n",
            "Step  206800: train CrossEntropyLoss |  2.97545958\n",
            "Step  206800: eval  CrossEntropyLoss |  2.37192345\n",
            "Step  206800: eval          Accuracy |  0.58620691\n",
            "\n",
            "Step  206900: Ran 100 train steps in 48.25 secs\n",
            "Step  206900: train CrossEntropyLoss |  2.93232989\n",
            "Step  206900: eval  CrossEntropyLoss |  3.20932531\n",
            "Step  206900: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  207000: Ran 100 train steps in 48.19 secs\n",
            "Step  207000: train CrossEntropyLoss |  2.91429925\n",
            "Step  207000: eval  CrossEntropyLoss |  2.86887956\n",
            "Step  207000: eval          Accuracy |  0.52173913\n",
            "\n",
            "Step  207100: Ran 100 train steps in 48.32 secs\n",
            "Step  207100: train CrossEntropyLoss |  2.90156889\n",
            "Step  207100: eval  CrossEntropyLoss |  3.37118173\n",
            "Step  207100: eval          Accuracy |  0.43362832\n",
            "\n",
            "Step  207200: Ran 100 train steps in 48.30 secs\n",
            "Step  207200: train CrossEntropyLoss |  2.92759728\n",
            "Step  207200: eval  CrossEntropyLoss |  3.01814365\n",
            "Step  207200: eval          Accuracy |  0.47413793\n",
            "\n",
            "Step  207300: Ran 100 train steps in 48.31 secs\n",
            "Step  207300: train CrossEntropyLoss |  3.00194263\n",
            "Step  207300: eval  CrossEntropyLoss |  3.00644088\n",
            "Step  207300: eval          Accuracy |  0.47272727\n",
            "\n",
            "Step  207400: Ran 100 train steps in 48.27 secs\n",
            "Step  207400: train CrossEntropyLoss |  2.96709156\n",
            "Step  207400: eval  CrossEntropyLoss |  2.04168534\n",
            "Step  207400: eval          Accuracy |  0.62280703\n",
            "\n",
            "Step  207500: Ran 100 train steps in 48.35 secs\n",
            "Step  207500: train CrossEntropyLoss |  2.87059498\n",
            "Step  207500: eval  CrossEntropyLoss |  2.90224648\n",
            "Step  207500: eval          Accuracy |  0.52083337\n",
            "\n",
            "Step  207600: Ran 100 train steps in 48.42 secs\n",
            "Step  207600: train CrossEntropyLoss |  2.91971421\n",
            "Step  207600: eval  CrossEntropyLoss |  3.36070943\n",
            "Step  207600: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  207700: Ran 100 train steps in 48.42 secs\n",
            "Step  207700: train CrossEntropyLoss |  2.89351869\n",
            "Step  207700: eval  CrossEntropyLoss |  2.88702631\n",
            "Step  207700: eval          Accuracy |  0.45263159\n",
            "\n",
            "Step  207800: Ran 100 train steps in 48.37 secs\n",
            "Step  207800: train CrossEntropyLoss |  2.96297383\n",
            "Step  207800: eval  CrossEntropyLoss |  3.12524438\n",
            "Step  207800: eval          Accuracy |  0.48695651\n",
            "\n",
            "Step  207900: Ran 100 train steps in 48.31 secs\n",
            "Step  207900: train CrossEntropyLoss |  2.85106754\n",
            "Step  207900: eval  CrossEntropyLoss |  2.40068865\n",
            "Step  207900: eval          Accuracy |  0.54838711\n",
            "\n",
            "Step  208000: Ran 100 train steps in 48.42 secs\n",
            "Step  208000: train CrossEntropyLoss |  2.91340089\n",
            "Step  208000: eval  CrossEntropyLoss |  3.13779140\n",
            "Step  208000: eval          Accuracy |  0.48039219\n",
            "\n",
            "Step  208100: Ran 100 train steps in 48.20 secs\n",
            "Step  208100: train CrossEntropyLoss |  2.88195205\n",
            "Step  208100: eval  CrossEntropyLoss |  2.86172414\n",
            "Step  208100: eval          Accuracy |  0.42553189\n",
            "\n",
            "Step  208200: Ran 100 train steps in 48.51 secs\n",
            "Step  208200: train CrossEntropyLoss |  2.92796969\n",
            "Step  208200: eval  CrossEntropyLoss |  3.22809505\n",
            "Step  208200: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  208300: Ran 100 train steps in 48.36 secs\n",
            "Step  208300: train CrossEntropyLoss |  2.93759942\n",
            "Step  208300: eval  CrossEntropyLoss |  2.85392761\n",
            "Step  208300: eval          Accuracy |  0.54954958\n",
            "\n",
            "Step  208400: Ran 100 train steps in 48.35 secs\n",
            "Step  208400: train CrossEntropyLoss |  2.91230869\n",
            "Step  208400: eval  CrossEntropyLoss |  2.72048616\n",
            "Step  208400: eval          Accuracy |  0.58730161\n",
            "\n",
            "Step  208500: Ran 100 train steps in 48.54 secs\n",
            "Step  208500: train CrossEntropyLoss |  2.86322379\n",
            "Step  208500: eval  CrossEntropyLoss |  2.51988506\n",
            "Step  208500: eval          Accuracy |  0.53125000\n",
            "\n",
            "Step  208600: Ran 100 train steps in 48.28 secs\n",
            "Step  208600: train CrossEntropyLoss |  2.92611504\n",
            "Step  208600: eval  CrossEntropyLoss |  3.14772940\n",
            "Step  208600: eval          Accuracy |  0.50434780\n",
            "\n",
            "Step  208700: Ran 100 train steps in 48.24 secs\n",
            "Step  208700: train CrossEntropyLoss |  2.90572953\n",
            "Step  208700: eval  CrossEntropyLoss |  3.63989043\n",
            "Step  208700: eval          Accuracy |  0.35398230\n",
            "\n",
            "Step  208800: Ran 100 train steps in 48.50 secs\n",
            "Step  208800: train CrossEntropyLoss |  2.91255450\n",
            "Step  208800: eval  CrossEntropyLoss |  2.52207732\n",
            "Step  208800: eval          Accuracy |  0.56521738\n",
            "\n",
            "Step  208900: Ran 100 train steps in 48.31 secs\n",
            "Step  208900: train CrossEntropyLoss |  2.84326601\n",
            "Step  208900: eval  CrossEntropyLoss |  2.37651157\n",
            "Step  208900: eval          Accuracy |  0.61261261\n",
            "\n",
            "Step  209000: Ran 100 train steps in 48.51 secs\n",
            "Step  209000: train CrossEntropyLoss |  2.84500909\n",
            "Step  209000: eval  CrossEntropyLoss |  2.95506191\n",
            "Step  209000: eval          Accuracy |  0.46610171\n",
            "\n",
            "Step  209100: Ran 100 train steps in 48.19 secs\n",
            "Step  209100: train CrossEntropyLoss |  2.92426801\n",
            "Step  209100: eval  CrossEntropyLoss |  2.68017817\n",
            "Step  209100: eval          Accuracy |  0.53608251\n",
            "\n",
            "Step  209200: Ran 100 train steps in 48.26 secs\n",
            "Step  209200: train CrossEntropyLoss |  2.88350129\n",
            "Step  209200: eval  CrossEntropyLoss |  3.15390110\n",
            "Step  209200: eval          Accuracy |  0.42553189\n",
            "\n",
            "Step  209300: Ran 100 train steps in 48.26 secs\n",
            "Step  209300: train CrossEntropyLoss |  2.90087438\n",
            "Step  209300: eval  CrossEntropyLoss |  3.18971634\n",
            "Step  209300: eval          Accuracy |  0.45833334\n",
            "\n",
            "Step  209400: Ran 100 train steps in 48.19 secs\n",
            "Step  209400: train CrossEntropyLoss |  2.97533965\n",
            "Step  209400: eval  CrossEntropyLoss |  3.20678186\n",
            "Step  209400: eval          Accuracy |  0.44329900\n",
            "\n",
            "Step  209500: Ran 100 train steps in 48.37 secs\n",
            "Step  209500: train CrossEntropyLoss |  2.91708851\n",
            "Step  209500: eval  CrossEntropyLoss |  2.70820045\n",
            "Step  209500: eval          Accuracy |  0.52747256\n",
            "\n",
            "Step  209600: Ran 100 train steps in 48.27 secs\n",
            "Step  209600: train CrossEntropyLoss |  2.83980608\n",
            "Step  209600: eval  CrossEntropyLoss |  3.32390642\n",
            "Step  209600: eval          Accuracy |  0.46938774\n",
            "\n",
            "Step  209700: Ran 100 train steps in 48.21 secs\n",
            "Step  209700: train CrossEntropyLoss |  2.93695784\n",
            "Step  209700: eval  CrossEntropyLoss |  2.51664376\n",
            "Step  209700: eval          Accuracy |  0.58407080\n",
            "\n",
            "Step  209800: Ran 100 train steps in 48.38 secs\n",
            "Step  209800: train CrossEntropyLoss |  2.91574478\n",
            "Step  209800: eval  CrossEntropyLoss |  2.86234307\n",
            "Step  209800: eval          Accuracy |  0.52475244\n",
            "\n",
            "Step  209900: Ran 100 train steps in 48.42 secs\n",
            "Step  209900: train CrossEntropyLoss |  2.88609052\n",
            "Step  209900: eval  CrossEntropyLoss |  1.65241313\n",
            "Step  209900: eval          Accuracy |  0.70967740\n",
            "\n",
            "Step  210000: Ran 100 train steps in 48.47 secs\n",
            "Step  210000: train CrossEntropyLoss |  2.97849751\n",
            "Step  210000: eval  CrossEntropyLoss |  3.17961907\n",
            "Step  210000: eval          Accuracy |  0.41904762\n",
            "\n",
            "Step  210100: Ran 100 train steps in 48.39 secs\n",
            "Step  210100: train CrossEntropyLoss |  2.94632435\n",
            "Step  210100: eval  CrossEntropyLoss |  2.63169408\n",
            "Step  210100: eval          Accuracy |  0.53763443\n",
            "\n",
            "Step  210200: Ran 100 train steps in 48.47 secs\n",
            "Step  210200: train CrossEntropyLoss |  2.86955404\n",
            "Step  210200: eval  CrossEntropyLoss |  2.70287991\n",
            "Step  210200: eval          Accuracy |  0.49541283\n",
            "\n",
            "Step  210300: Ran 100 train steps in 48.33 secs\n",
            "Step  210300: train CrossEntropyLoss |  2.86444855\n",
            "Step  210300: eval  CrossEntropyLoss |  3.24685359\n",
            "Step  210300: eval          Accuracy |  0.50999999\n",
            "\n",
            "Step  210400: Ran 100 train steps in 48.22 secs\n",
            "Step  210400: train CrossEntropyLoss |  2.88998914\n",
            "Step  210400: eval  CrossEntropyLoss |  3.02336979\n",
            "Step  210400: eval          Accuracy |  0.48514852\n",
            "\n",
            "Step  210500: Ran 100 train steps in 48.38 secs\n",
            "Step  210500: train CrossEntropyLoss |  2.90738511\n",
            "Step  210500: eval  CrossEntropyLoss |  3.54607129\n",
            "Step  210500: eval          Accuracy |  0.46017700\n",
            "\n",
            "Step  210600: Ran 100 train steps in 48.27 secs\n",
            "Step  210600: train CrossEntropyLoss |  2.86886168\n",
            "Step  210600: eval  CrossEntropyLoss |  1.95542729\n",
            "Step  210600: eval          Accuracy |  0.63157898\n",
            "\n",
            "Step  210700: Ran 100 train steps in 48.38 secs\n",
            "Step  210700: train CrossEntropyLoss |  2.87376118\n",
            "Step  210700: eval  CrossEntropyLoss |  3.04465508\n",
            "Step  210700: eval          Accuracy |  0.48387095\n",
            "\n",
            "Step  210800: Ran 100 train steps in 48.24 secs\n",
            "Step  210800: train CrossEntropyLoss |  2.88493371\n",
            "Step  210800: eval  CrossEntropyLoss |  2.60691333\n",
            "Step  210800: eval          Accuracy |  0.53465348\n",
            "\n",
            "Step  210900: Ran 100 train steps in 48.21 secs\n",
            "Step  210900: train CrossEntropyLoss |  2.86022615\n",
            "Step  210900: eval  CrossEntropyLoss |  3.27951002\n",
            "Step  210900: eval          Accuracy |  0.48113209\n",
            "\n",
            "Step  211000: Ran 100 train steps in 48.41 secs\n",
            "Step  211000: train CrossEntropyLoss |  2.87711596\n",
            "Step  211000: eval  CrossEntropyLoss |  2.84542227\n",
            "Step  211000: eval          Accuracy |  0.50847459\n",
            "\n",
            "Step  211100: Ran 100 train steps in 48.36 secs\n",
            "Step  211100: train CrossEntropyLoss |  2.83453488\n",
            "Step  211100: eval  CrossEntropyLoss |  3.02565503\n",
            "Step  211100: eval          Accuracy |  0.45871559\n",
            "\n",
            "Step  211200: Ran 100 train steps in 48.29 secs\n",
            "Step  211200: train CrossEntropyLoss |  2.90870810\n",
            "Step  211200: eval  CrossEntropyLoss |  2.65805721\n",
            "Step  211200: eval          Accuracy |  0.53465348\n",
            "\n",
            "Step  211300: Ran 100 train steps in 48.52 secs\n",
            "Step  211300: train CrossEntropyLoss |  2.89549708\n",
            "Step  211300: eval  CrossEntropyLoss |  2.42239285\n",
            "Step  211300: eval          Accuracy |  0.58620691\n",
            "\n",
            "Step  211400: Ran 100 train steps in 48.23 secs\n",
            "Step  211400: train CrossEntropyLoss |  2.84663987\n",
            "Step  211400: eval  CrossEntropyLoss |  2.66137886\n",
            "Step  211400: eval          Accuracy |  0.51515150\n",
            "\n",
            "Step  211500: Ran 100 train steps in 48.44 secs\n",
            "Step  211500: train CrossEntropyLoss |  2.84097338\n",
            "Step  211500: eval  CrossEntropyLoss |  3.38087010\n",
            "Step  211500: eval          Accuracy |  0.44705883\n",
            "\n",
            "Step  211600: Ran 100 train steps in 48.14 secs\n",
            "Step  211600: train CrossEntropyLoss |  2.86493039\n",
            "Step  211600: eval  CrossEntropyLoss |  3.30339408\n",
            "Step  211600: eval          Accuracy |  0.44736841\n",
            "\n",
            "Step  211700: Ran 100 train steps in 48.11 secs\n",
            "Step  211700: train CrossEntropyLoss |  2.84182906\n",
            "Step  211700: eval  CrossEntropyLoss |  2.99294186\n",
            "Step  211700: eval          Accuracy |  0.51578951\n",
            "\n",
            "Step  211800: Ran 100 train steps in 48.29 secs\n",
            "Step  211800: train CrossEntropyLoss |  2.84557486\n",
            "Step  211800: eval  CrossEntropyLoss |  3.25694013\n",
            "Step  211800: eval          Accuracy |  0.45263159\n",
            "\n",
            "Step  211900: Ran 100 train steps in 48.12 secs\n",
            "Step  211900: train CrossEntropyLoss |  2.88806653\n",
            "Step  211900: eval  CrossEntropyLoss |  2.44585943\n",
            "Step  211900: eval          Accuracy |  0.54945058\n",
            "\n",
            "Step  212000: Ran 100 train steps in 48.28 secs\n",
            "Step  212000: train CrossEntropyLoss |  2.92416716\n",
            "Step  212000: eval  CrossEntropyLoss |  2.99191999\n",
            "Step  212000: eval          Accuracy |  0.49038464\n",
            "\n",
            "Step  212100: Ran 100 train steps in 48.27 secs\n",
            "Step  212100: train CrossEntropyLoss |  2.83919358\n",
            "Step  212100: eval  CrossEntropyLoss |  3.62645364\n",
            "Step  212100: eval          Accuracy |  0.40206188\n",
            "\n",
            "Step  212200: Ran 100 train steps in 48.19 secs\n",
            "Step  212200: train CrossEntropyLoss |  2.87948966\n",
            "Step  212200: eval  CrossEntropyLoss |  2.45205474\n",
            "Step  212200: eval          Accuracy |  0.56999999\n",
            "\n",
            "Step  212300: Ran 100 train steps in 48.09 secs\n",
            "Step  212300: train CrossEntropyLoss |  2.86572719\n",
            "Step  212300: eval  CrossEntropyLoss |  1.96190584\n",
            "Step  212300: eval          Accuracy |  0.64210528\n",
            "\n",
            "Step  212400: Ran 100 train steps in 48.16 secs\n",
            "Step  212400: train CrossEntropyLoss |  2.87505841\n",
            "Step  212400: eval  CrossEntropyLoss |  2.92457175\n",
            "Step  212400: eval          Accuracy |  0.48999998\n",
            "\n",
            "Step  212500: Ran 100 train steps in 48.24 secs\n",
            "Step  212500: train CrossEntropyLoss |  2.90777588\n",
            "Step  212500: eval  CrossEntropyLoss |  2.97407651\n",
            "Step  212500: eval          Accuracy |  0.46315792\n",
            "\n",
            "Step  212600: Ran 100 train steps in 48.21 secs\n",
            "Step  212600: train CrossEntropyLoss |  2.92825222\n",
            "Step  212600: eval  CrossEntropyLoss |  2.44517350\n",
            "Step  212600: eval          Accuracy |  0.53535354\n",
            "\n",
            "Step  212700: Ran 100 train steps in 48.19 secs\n",
            "Step  212700: train CrossEntropyLoss |  2.76448369\n",
            "Step  212700: eval  CrossEntropyLoss |  2.56740117\n",
            "Step  212700: eval          Accuracy |  0.54736847\n",
            "\n",
            "Step  212800: Ran 100 train steps in 48.20 secs\n",
            "Step  212800: train CrossEntropyLoss |  2.80893254\n",
            "Step  212800: eval  CrossEntropyLoss |  2.06681275\n",
            "Step  212800: eval          Accuracy |  0.58585858\n",
            "\n",
            "Step  212900: Ran 100 train steps in 48.13 secs\n",
            "Step  212900: train CrossEntropyLoss |  2.80357313\n",
            "Step  212900: eval  CrossEntropyLoss |  2.41749239\n",
            "Step  212900: eval          Accuracy |  0.57272726\n",
            "\n",
            "Step  213000: Ran 100 train steps in 48.32 secs\n",
            "Step  213000: train CrossEntropyLoss |  2.87687802\n",
            "Step  213000: eval  CrossEntropyLoss |  2.77148032\n",
            "Step  213000: eval          Accuracy |  0.53061223\n",
            "\n",
            "Step  213100: Ran 100 train steps in 48.21 secs\n",
            "Step  213100: train CrossEntropyLoss |  2.85921216\n",
            "Step  213100: eval  CrossEntropyLoss |  3.01919079\n",
            "Step  213100: eval          Accuracy |  0.51401865\n",
            "\n",
            "Step  213200: Ran 100 train steps in 48.36 secs\n",
            "Step  213200: train CrossEntropyLoss |  2.94126916\n",
            "Step  213200: eval  CrossEntropyLoss |  3.00465322\n",
            "Step  213200: eval          Accuracy |  0.47169811\n",
            "\n",
            "Step  213300: Ran 100 train steps in 48.37 secs\n",
            "Step  213300: train CrossEntropyLoss |  2.81672359\n",
            "Step  213300: eval  CrossEntropyLoss |  3.35999417\n",
            "Step  213300: eval          Accuracy |  0.47777778\n",
            "\n",
            "Step  213400: Ran 100 train steps in 48.25 secs\n",
            "Step  213400: train CrossEntropyLoss |  2.90122867\n",
            "Step  213400: eval  CrossEntropyLoss |  2.83607078\n",
            "Step  213400: eval          Accuracy |  0.53703701\n",
            "\n",
            "Step  213500: Ran 100 train steps in 48.59 secs\n",
            "Step  213500: train CrossEntropyLoss |  2.82639003\n",
            "Step  213500: eval  CrossEntropyLoss |  2.77907467\n",
            "Step  213500: eval          Accuracy |  0.54444444\n",
            "\n",
            "Step  213600: Ran 100 train steps in 48.21 secs\n",
            "Step  213600: train CrossEntropyLoss |  2.84524417\n",
            "Step  213600: eval  CrossEntropyLoss |  3.34212708\n",
            "Step  213600: eval          Accuracy |  0.44859812\n",
            "\n",
            "Step  213700: Ran 100 train steps in 48.35 secs\n",
            "Step  213700: train CrossEntropyLoss |  2.87609148\n",
            "Step  213700: eval  CrossEntropyLoss |  2.65357924\n",
            "Step  213700: eval          Accuracy |  0.55284560\n",
            "\n",
            "Step  213800: Ran 100 train steps in 48.25 secs\n",
            "Step  213800: train CrossEntropyLoss |  2.86449790\n",
            "Step  213800: eval  CrossEntropyLoss |  2.74868631\n",
            "Step  213800: eval          Accuracy |  0.48351648\n",
            "\n",
            "Step  213900: Ran 100 train steps in 48.41 secs\n",
            "Step  213900: train CrossEntropyLoss |  2.82014585\n",
            "Step  213900: eval  CrossEntropyLoss |  2.74582982\n",
            "Step  213900: eval          Accuracy |  0.57017547\n",
            "\n",
            "Step  214000: Ran 100 train steps in 48.26 secs\n",
            "Step  214000: train CrossEntropyLoss |  2.86400318\n",
            "Step  214000: eval  CrossEntropyLoss |  2.97517586\n",
            "Step  214000: eval          Accuracy |  0.44628099\n",
            "\n",
            "Step  214100: Ran 100 train steps in 48.25 secs\n",
            "Step  214100: train CrossEntropyLoss |  2.78295493\n",
            "Step  214100: eval  CrossEntropyLoss |  2.59876752\n",
            "Step  214100: eval          Accuracy |  0.54464287\n",
            "\n",
            "Step  214200: Ran 100 train steps in 48.27 secs\n",
            "Step  214200: train CrossEntropyLoss |  2.87045622\n",
            "Step  214200: eval  CrossEntropyLoss |  2.82989907\n",
            "Step  214200: eval          Accuracy |  0.57303369\n",
            "\n",
            "Step  214300: Ran 100 train steps in 48.37 secs\n",
            "Step  214300: train CrossEntropyLoss |  2.92465305\n",
            "Step  214300: eval  CrossEntropyLoss |  1.98196661\n",
            "Step  214300: eval          Accuracy |  0.65420556\n",
            "\n",
            "Step  214400: Ran 100 train steps in 48.20 secs\n",
            "Step  214400: train CrossEntropyLoss |  2.84029412\n",
            "Step  214400: eval  CrossEntropyLoss |  2.68617582\n",
            "Step  214400: eval          Accuracy |  0.51041669\n",
            "\n",
            "Step  214500: Ran 100 train steps in 48.51 secs\n",
            "Step  214500: train CrossEntropyLoss |  2.84872675\n",
            "Step  214500: eval  CrossEntropyLoss |  2.80956411\n",
            "Step  214500: eval          Accuracy |  0.51041669\n",
            "\n",
            "Step  214600: Ran 100 train steps in 48.32 secs\n",
            "Step  214600: train CrossEntropyLoss |  2.93911433\n",
            "Step  214600: eval  CrossEntropyLoss |  3.27439117\n",
            "Step  214600: eval          Accuracy |  0.45714289\n",
            "\n",
            "Step  214700: Ran 100 train steps in 48.33 secs\n",
            "Step  214700: train CrossEntropyLoss |  2.85984540\n",
            "Step  214700: eval  CrossEntropyLoss |  2.37381792\n",
            "Step  214700: eval          Accuracy |  0.57954550\n",
            "\n",
            "Step  214800: Ran 100 train steps in 48.23 secs\n",
            "Step  214800: train CrossEntropyLoss |  2.88788104\n",
            "Step  214800: eval  CrossEntropyLoss |  2.56726837\n",
            "Step  214800: eval          Accuracy |  0.55660379\n",
            "\n",
            "Step  214900: Ran 100 train steps in 48.27 secs\n",
            "Step  214900: train CrossEntropyLoss |  2.85152745\n",
            "Step  214900: eval  CrossEntropyLoss |  2.56584144\n",
            "Step  214900: eval          Accuracy |  0.52173913\n",
            "\n",
            "Step  215000: Ran 100 train steps in 48.35 secs\n",
            "Step  215000: train CrossEntropyLoss |  2.87380910\n",
            "Step  215000: eval  CrossEntropyLoss |  3.53579021\n",
            "Step  215000: eval          Accuracy |  0.42307693\n",
            "\n",
            "Step  215100: Ran 100 train steps in 48.22 secs\n",
            "Step  215100: train CrossEntropyLoss |  2.88041472\n",
            "Step  215100: eval  CrossEntropyLoss |  2.85584569\n",
            "Step  215100: eval          Accuracy |  0.55263156\n",
            "\n",
            "Step  215200: Ran 100 train steps in 48.33 secs\n",
            "Step  215200: train CrossEntropyLoss |  2.87086606\n",
            "Step  215200: eval  CrossEntropyLoss |  2.75765848\n",
            "Step  215200: eval          Accuracy |  0.51304346\n",
            "\n",
            "Step  215300: Ran 100 train steps in 48.11 secs\n",
            "Step  215300: train CrossEntropyLoss |  2.87369275\n",
            "Step  215300: eval  CrossEntropyLoss |  2.87302136\n",
            "Step  215300: eval          Accuracy |  0.49074075\n",
            "\n",
            "Step  215400: Ran 100 train steps in 48.34 secs\n",
            "Step  215400: train CrossEntropyLoss |  2.81090283\n",
            "Step  215400: eval  CrossEntropyLoss |  3.19675136\n",
            "Step  215400: eval          Accuracy |  0.48214287\n",
            "\n",
            "Step  215500: Ran 100 train steps in 48.33 secs\n",
            "Step  215500: train CrossEntropyLoss |  2.92951989\n",
            "Step  215500: eval  CrossEntropyLoss |  3.36031938\n",
            "Step  215500: eval          Accuracy |  0.44554454\n",
            "\n",
            "Step  215600: Ran 100 train steps in 48.26 secs\n",
            "Step  215600: train CrossEntropyLoss |  2.76123476\n",
            "Step  215600: eval  CrossEntropyLoss |  2.74015880\n",
            "Step  215600: eval          Accuracy |  0.50467288\n",
            "\n",
            "Step  215700: Ran 100 train steps in 48.22 secs\n",
            "Step  215700: train CrossEntropyLoss |  2.82341433\n",
            "Step  215700: eval  CrossEntropyLoss |  2.54589748\n",
            "Step  215700: eval          Accuracy |  0.55789477\n",
            "\n",
            "Step  215800: Ran 100 train steps in 48.29 secs\n",
            "Step  215800: train CrossEntropyLoss |  2.78665161\n",
            "Step  215800: eval  CrossEntropyLoss |  2.67227483\n",
            "Step  215800: eval          Accuracy |  0.49484539\n",
            "\n",
            "Step  215900: Ran 100 train steps in 48.25 secs\n",
            "Step  215900: train CrossEntropyLoss |  2.92761016\n",
            "Step  215900: eval  CrossEntropyLoss |  2.83670712\n",
            "Step  215900: eval          Accuracy |  0.51999998\n",
            "\n",
            "Step  216000: Ran 100 train steps in 48.33 secs\n",
            "Step  216000: train CrossEntropyLoss |  2.83348441\n",
            "Step  216000: eval  CrossEntropyLoss |  3.28519130\n",
            "Step  216000: eval          Accuracy |  0.44761905\n",
            "\n",
            "Step  216100: Ran 100 train steps in 48.24 secs\n",
            "Step  216100: train CrossEntropyLoss |  2.77998662\n",
            "Step  216100: eval  CrossEntropyLoss |  3.02351069\n",
            "Step  216100: eval          Accuracy |  0.51304346\n",
            "\n",
            "Step  216200: Ran 100 train steps in 48.20 secs\n",
            "Step  216200: train CrossEntropyLoss |  2.82709312\n",
            "Step  216200: eval  CrossEntropyLoss |  2.74470472\n",
            "Step  216200: eval          Accuracy |  0.52032524\n",
            "\n",
            "Step  216300: Ran 100 train steps in 48.21 secs\n",
            "Step  216300: train CrossEntropyLoss |  2.76556444\n",
            "Step  216300: eval  CrossEntropyLoss |  3.26141620\n",
            "Step  216300: eval          Accuracy |  0.44954127\n",
            "\n",
            "Step  216400: Ran 100 train steps in 48.20 secs\n",
            "Step  216400: train CrossEntropyLoss |  2.76155543\n",
            "Step  216400: eval  CrossEntropyLoss |  2.46154928\n",
            "Step  216400: eval          Accuracy |  0.55555558\n",
            "\n",
            "Step  216500: Ran 100 train steps in 48.49 secs\n",
            "Step  216500: train CrossEntropyLoss |  2.80656052\n",
            "Step  216500: eval  CrossEntropyLoss |  3.07722211\n",
            "Step  216500: eval          Accuracy |  0.47222224\n",
            "\n",
            "Step  216600: Ran 100 train steps in 48.09 secs\n",
            "Step  216600: train CrossEntropyLoss |  2.82983422\n",
            "Step  216600: eval  CrossEntropyLoss |  2.65652800\n",
            "Step  216600: eval          Accuracy |  0.55357146\n",
            "\n",
            "Step  216700: Ran 100 train steps in 48.30 secs\n",
            "Step  216700: train CrossEntropyLoss |  2.81804705\n",
            "Step  216700: eval  CrossEntropyLoss |  2.68721795\n",
            "Step  216700: eval          Accuracy |  0.56363636\n",
            "\n",
            "Step  216800: Ran 100 train steps in 48.23 secs\n",
            "Step  216800: train CrossEntropyLoss |  2.77218843\n",
            "Step  216800: eval  CrossEntropyLoss |  3.11856842\n",
            "Step  216800: eval          Accuracy |  0.47706419\n",
            "\n",
            "Step  216900: Ran 100 train steps in 48.31 secs\n",
            "Step  216900: train CrossEntropyLoss |  2.86661911\n",
            "Step  216900: eval  CrossEntropyLoss |  3.10017014\n",
            "Step  216900: eval          Accuracy |  0.44545454\n",
            "\n",
            "Step  217000: Ran 100 train steps in 48.39 secs\n",
            "Step  217000: train CrossEntropyLoss |  2.83079767\n",
            "Step  217000: eval  CrossEntropyLoss |  2.81350088\n",
            "Step  217000: eval          Accuracy |  0.55238098\n",
            "\n",
            "Step  217100: Ran 100 train steps in 48.12 secs\n",
            "Step  217100: train CrossEntropyLoss |  2.88213396\n",
            "Step  217100: eval  CrossEntropyLoss |  3.53941011\n",
            "Step  217100: eval          Accuracy |  0.41304350\n",
            "\n",
            "Step  217200: Ran 100 train steps in 48.20 secs\n",
            "Step  217200: train CrossEntropyLoss |  2.85575223\n",
            "Step  217200: eval  CrossEntropyLoss |  2.74758935\n",
            "Step  217200: eval          Accuracy |  0.50819677\n",
            "\n",
            "Step  217300: Ran 100 train steps in 48.46 secs\n",
            "Step  217300: train CrossEntropyLoss |  2.84475708\n",
            "Step  217300: eval  CrossEntropyLoss |  2.38380003\n",
            "Step  217300: eval          Accuracy |  0.57547170\n",
            "\n",
            "Step  217400: Ran 100 train steps in 48.16 secs\n",
            "Step  217400: train CrossEntropyLoss |  2.80949426\n",
            "Step  217400: eval  CrossEntropyLoss |  2.66715884\n",
            "Step  217400: eval          Accuracy |  0.55555558\n",
            "\n",
            "Step  217500: Ran 100 train steps in 48.41 secs\n",
            "Step  217500: train CrossEntropyLoss |  2.81689811\n",
            "Step  217500: eval  CrossEntropyLoss |  2.61277509\n",
            "Step  217500: eval          Accuracy |  0.52941179\n",
            "\n",
            "Step  217600: Ran 100 train steps in 47.98 secs\n",
            "Step  217600: train CrossEntropyLoss |  2.79090071\n",
            "Step  217600: eval  CrossEntropyLoss |  3.10008049\n",
            "Step  217600: eval          Accuracy |  0.41414142\n",
            "\n",
            "Step  217700: Ran 100 train steps in 48.48 secs\n",
            "Step  217700: train CrossEntropyLoss |  2.90600753\n",
            "Step  217700: eval  CrossEntropyLoss |  2.29967237\n",
            "Step  217700: eval          Accuracy |  0.62365592\n",
            "\n",
            "Step  217800: Ran 100 train steps in 48.21 secs\n",
            "Step  217800: train CrossEntropyLoss |  2.84234118\n",
            "Step  217800: eval  CrossEntropyLoss |  2.77265692\n",
            "Step  217800: eval          Accuracy |  0.53999996\n",
            "\n",
            "Step  217900: Ran 100 train steps in 48.13 secs\n",
            "Step  217900: train CrossEntropyLoss |  2.80021572\n",
            "Step  217900: eval  CrossEntropyLoss |  2.68421459\n",
            "Step  217900: eval          Accuracy |  0.52777779\n",
            "\n",
            "Step  218000: Ran 100 train steps in 48.32 secs\n",
            "Step  218000: train CrossEntropyLoss |  2.87780166\n",
            "Step  218000: eval  CrossEntropyLoss |  2.98013425\n",
            "Step  218000: eval          Accuracy |  0.44545454\n",
            "\n",
            "Step  218100: Ran 100 train steps in 48.09 secs\n",
            "Step  218100: train CrossEntropyLoss |  2.76043844\n",
            "Step  218100: eval  CrossEntropyLoss |  3.25730491\n",
            "Step  218100: eval          Accuracy |  0.38383839\n",
            "\n",
            "Step  218200: Ran 100 train steps in 48.26 secs\n",
            "Step  218200: train CrossEntropyLoss |  2.81261158\n",
            "Step  218200: eval  CrossEntropyLoss |  2.79646707\n",
            "Step  218200: eval          Accuracy |  0.53333336\n",
            "\n",
            "Step  218300: Ran 100 train steps in 48.05 secs\n",
            "Step  218300: train CrossEntropyLoss |  2.82907486\n",
            "Step  218300: eval  CrossEntropyLoss |  2.14675212\n",
            "Step  218300: eval          Accuracy |  0.61818182\n",
            "\n",
            "Step  218400: Ran 100 train steps in 48.15 secs\n",
            "Step  218400: train CrossEntropyLoss |  2.81182194\n",
            "Step  218400: eval  CrossEntropyLoss |  3.08833408\n",
            "Step  218400: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  218500: Ran 100 train steps in 48.24 secs\n",
            "Step  218500: train CrossEntropyLoss |  2.87905550\n",
            "Step  218500: eval  CrossEntropyLoss |  2.48142648\n",
            "Step  218500: eval          Accuracy |  0.54639179\n",
            "\n",
            "Step  218600: Ran 100 train steps in 48.44 secs\n",
            "Step  218600: train CrossEntropyLoss |  2.84560061\n",
            "Step  218600: eval  CrossEntropyLoss |  2.52119946\n",
            "Step  218600: eval          Accuracy |  0.55140185\n",
            "\n",
            "Step  218700: Ran 100 train steps in 48.37 secs\n",
            "Step  218700: train CrossEntropyLoss |  2.78047132\n",
            "Step  218700: eval  CrossEntropyLoss |  2.81540942\n",
            "Step  218700: eval          Accuracy |  0.47422683\n",
            "\n",
            "Step  218800: Ran 100 train steps in 47.99 secs\n",
            "Step  218800: train CrossEntropyLoss |  2.79191089\n",
            "Step  218800: eval  CrossEntropyLoss |  3.23423314\n",
            "Step  218800: eval          Accuracy |  0.41414142\n",
            "\n",
            "Step  218900: Ran 100 train steps in 48.33 secs\n",
            "Step  218900: train CrossEntropyLoss |  2.73440862\n",
            "Step  218900: eval  CrossEntropyLoss |  2.96295857\n",
            "Step  218900: eval          Accuracy |  0.51785719\n",
            "\n",
            "Step  219000: Ran 100 train steps in 48.67 secs\n",
            "Step  219000: train CrossEntropyLoss |  2.81731558\n",
            "Step  219000: eval  CrossEntropyLoss |  3.34246063\n",
            "Step  219000: eval          Accuracy |  0.42056075\n",
            "\n",
            "Step  219100: Ran 100 train steps in 48.30 secs\n",
            "Step  219100: train CrossEntropyLoss |  2.87710381\n",
            "Step  219100: eval  CrossEntropyLoss |  3.35700512\n",
            "Step  219100: eval          Accuracy |  0.51351351\n",
            "\n",
            "Step  219200: Ran 100 train steps in 48.45 secs\n",
            "Step  219200: train CrossEntropyLoss |  2.78449154\n",
            "Step  219200: eval  CrossEntropyLoss |  2.34174919\n",
            "Step  219200: eval          Accuracy |  0.57391304\n",
            "\n",
            "Step  219300: Ran 100 train steps in 48.50 secs\n",
            "Step  219300: train CrossEntropyLoss |  2.80347371\n",
            "Step  219300: eval  CrossEntropyLoss |  2.81728053\n",
            "Step  219300: eval          Accuracy |  0.56190479\n",
            "\n",
            "Step  219400: Ran 100 train steps in 48.45 secs\n",
            "Step  219400: train CrossEntropyLoss |  2.81289601\n",
            "Step  219400: eval  CrossEntropyLoss |  2.39212918\n",
            "Step  219400: eval          Accuracy |  0.54867256\n",
            "\n",
            "Step  219500: Ran 100 train steps in 48.49 secs\n",
            "Step  219500: train CrossEntropyLoss |  2.80348897\n",
            "Step  219500: eval  CrossEntropyLoss |  2.41600895\n",
            "Step  219500: eval          Accuracy |  0.55660379\n",
            "\n",
            "Step  219600: Ran 100 train steps in 48.52 secs\n",
            "Step  219600: train CrossEntropyLoss |  2.77984428\n",
            "Step  219600: eval  CrossEntropyLoss |  2.93525410\n",
            "Step  219600: eval          Accuracy |  0.44444445\n",
            "\n",
            "Step  219700: Ran 100 train steps in 48.43 secs\n",
            "Step  219700: train CrossEntropyLoss |  2.87416792\n",
            "Step  219700: eval  CrossEntropyLoss |  3.02143955\n",
            "Step  219700: eval          Accuracy |  0.50387597\n",
            "\n",
            "Step  219800: Ran 100 train steps in 48.35 secs\n",
            "Step  219800: train CrossEntropyLoss |  2.74396563\n",
            "Step  219800: eval  CrossEntropyLoss |  3.66965842\n",
            "Step  219800: eval          Accuracy |  0.38297871\n",
            "\n",
            "Step  219900: Ran 100 train steps in 48.07 secs\n",
            "Step  219900: train CrossEntropyLoss |  2.82032824\n",
            "Step  219900: eval  CrossEntropyLoss |  3.26975560\n",
            "Step  219900: eval          Accuracy |  0.47747749\n",
            "\n",
            "Step  220000: Ran 100 train steps in 48.38 secs\n",
            "Step  220000: train CrossEntropyLoss |  2.86094141\n",
            "Step  220000: eval  CrossEntropyLoss |  3.16531467\n",
            "Step  220000: eval          Accuracy |  0.46666670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "# !rsync -a /content/drive/MyDrive/model2/ ~/\r\n",
        "\r\n",
        "# copy the model to Google Drive\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model/\r\n",
        "\r\n",
        "# sync Google Drive dir with the train dir\r\n",
        "# !rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe9cf78-547c-4c4c-bfb2-1f4b3c97fa9e"
      },
      "source": [
        "train_article = train_text_pairs[5][0]\r\n",
        "train_summary = train_text_pairs[5][1]\r\n",
        "print(wrapper.fill(train_article))\r\n",
        "print('')\r\n",
        "eval_article = eval_text_pairs[1][0]\r\n",
        "eval_summary = eval_text_pairs[1][1]\r\n",
        "print(wrapper.fill(eval_article))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые придумали новый способ взаимодействия с графеном, который\n",
            "позволяет избавиться от \"слипающихся\" листов. статья ученых появилась\n",
            "в журнале acs nano, а ее краткое изложение приводится на сайте северо-\n",
            "западного университета, сотрудники которого принимали участие в\n",
            "работе. известно, что основной трудностью при работе с графеновыми\n",
            "листами является то, что при соприкосновении они слипаются под\n",
            "воздействием сил ван-дер-ваальса между собой при наложении друг на\n",
            "друга. это приводит к потере большинства уникальных свойств материала.\n",
            "для решения подобной проблемы, например, некоторые исследователи\n",
            "кладут между листами прокладки из другого материала, однако такое\n",
            "решение часто не слишком эффективно - атомы прокладки могут\n",
            "образовывать связи с атомами углерода в графене, что снова приводит к\n",
            "появлению дефектов в материале. в рамках нового исследования ученые\n",
            "предложили использовать графен не в виде ровных листов, а в виде\n",
            "смятых в комок листов. по словам исследователей, в подобном виде\n",
            "графен ведет себя как бумажные комки в мусорной корзине - несмотря на\n",
            "достаточно плотное расположение, поверхности листов, из которых они\n",
            "состоят, не соприкасаются. расчеты показывают, что при подобной\n",
            "упаковке листов графен сохраняет около 45 процентов исходной площади\n",
            "поверхности. для сравнения, при других способах организации удается\n",
            "спасти не более 16 процентов площади. графен как теоретическая\n",
            "абстракция рассматривался еще в конце 20-х годов прошлого века.\n",
            "начиная с 1960-х годов, он выступал в качестве удобной математической\n",
            "модели для расчетов в квантовой механике. впервые графен получили на\n",
            "практике константин новоселов и андрей гейм в 2004 году.\n",
            "\n",
            "сша планируют сократить численность военного контингента в южной\n",
            "корее. по информации корейского министерства иностранных дел, к концу\n",
            "2005 года из страны будет выведена треть американского контингента,\n",
            "составляющего в настоящее время 37500 военнослужащих, сообщает\n",
            "reuters. всего к концу 2005 года страну покинут 12500 американских\n",
            "солдат. 3600 из них продолжат службу в ираке. глава корейского мид\n",
            "отметил, что сша подходят к выводу войск очень внимательно, так как\n",
            "ситуация на полуострове остается напряженной. тем не менее, сша пошли\n",
            "навстречу желанию властей южной кореи иметь более независимую армию, и\n",
            "обещают оказать им в этом всяческое содействие. собственные силы южной\n",
            "кореи составляют на сегодняшний день 690 000 человек. армия северной\n",
            "кореи насчитывает 1 100 000 военнослужащих.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fb8fb7-2d38-4be8-962a-ae5b78f48c64"
      },
      "source": [
        "# checking first symbol generation\r\n",
        "print(detokenize([next_symbol(tokenize(train_article)+[0], model)]))\r\n",
        "print(detokenize([next_symbol(tokenize(eval_article)+[0], model)]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые\n",
            "сша\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        \r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7c5040-e19d-470b-f964-c97d01eacc93"
      },
      "source": [
        "print(train_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(train_article, model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые предложили использовать графен в мятом виде\n",
            "\n",
            "\n",
            "ученые\n",
            "ученые нашли\n",
            "ученые нашли новый\n",
            "ученые нашли новый способ\n",
            "ученые нашли новый способ с\n",
            "ученые нашли новый способ сже\n",
            "ученые нашли новый способ сжечь\n",
            "ученые нашли новый способ сжечьи\n",
            "ученые нашли новый способ сжечьивать\n",
            "ученые нашли новый способ сжечьивать \"\n",
            "ученые нашли новый способ сжечьивать \"сли\n",
            "ученые нашли новый способ сжечьивать \"слипа\n",
            "ученые нашли новый способ сжечьивать \"слипаемые\n",
            "ученые нашли новый способ сжечьивать \"слипаемые\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cdbdbc2-e695-4aae-bf3e-deb376f24fe5"
      },
      "source": [
        "print(eval_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article, model)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша на треть сократят корейскую группировку\n",
            "\n",
            "\n",
            "сша\n",
            "сша сокра\n",
            "сша сократят\n",
            "сша сократят военное\n",
            "сша сократят военное командование\n",
            "сша сократят военное командование в\n",
            "сша сократят военное командование в южной\n",
            "сша сократят военное командование в южной корее\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L83lEskk4L7"
      },
      "source": [
        "model = SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='eval',\r\n",
        "                  ff_activation=tl.Relu)\r\n",
        "\r\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSDbAXjlF2f"
      },
      "source": [
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True, input_signature=shape11)\r\n",
        "\r\n",
        "# save the starting state\r\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-yINo6McPK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c163f460-e5ce-4b41-d4f3-dc85f032398d"
      },
      "source": [
        "np.array(tokenize(eval_article))[None, :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  401,  5641,  7162, 10989,  4302, 15374,     5,  3396, 11441,\n",
              "        15949,    17,  1205,  8980,   204,  2919,  2799,  1996, 15945,\n",
              "           64,  6453,  2730,   173,    86,   719,   372,   102,  1982,\n",
              "        15926,  1469,  3148, 15374, 15945,  1191,  1839,     5,  1416,\n",
              "          368,  4897, 15974,   423,  4061, 15945,   258,  1761, 15949,\n",
              "          983,    64,  6453,  2730,   173,  4387,  2208,  1132, 14538,\n",
              "          423,  3019,  4848, 15949,  3525,   423,    86,   994,  1869,\n",
              "        15930,  6533,     5,  3961, 15949,   915,  8980,   204,  2872,\n",
              "          996, 15945,    79,   401,   149,  2692,    64,  6696,  3516,\n",
              "         2176,  2557,   833, 15945,   208,   207,  4369,    25, 12391,\n",
              "         4875,  6328, 11725, 15949,   734,    57,  1977, 15945,   401,\n",
              "         6176,    25, 15933,   443,  1323,  3054,   810,  2964,  3396,\n",
              "         5813,  6940,   565,  3121,  4819, 13988, 15945,    16,  4424,\n",
              "          162,  9965,   680,     5,   226,  6550,  7020,    48,  8164,\n",
              "        15949,  9568,  3864,  3396,  5813,  8089,    25,  7553,   892,\n",
              "         9250, 15958, 15924, 10749,   534, 15949,  7248,  3768,  5813,\n",
              "        15210,   107,  1728, 15924, 10749,  4061, 15949,     1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197d3f96-edf8-4e30-aa29-c3d640ad4787"
      },
      "source": [
        "\r\n",
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "model.state = STARTING_STATE\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(tokenize(eval_article) + [0])[None, :],\r\n",
        "                                        temperature=0.3, max_length=20)\r\n",
        "print(wrapper.fill(detokenize(output[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша евро euro евро евро евро евро евро евро евро евро евро евро евро\n",
            "евротяже евро евро рублей евро\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}