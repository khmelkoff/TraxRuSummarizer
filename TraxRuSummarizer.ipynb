{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNs7pbWgepfWqrL0FouAAgT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d0611e-7090-42fd-c744-663e3acf07d2"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 19.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 50.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 52.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 56.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 53.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 46.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 53.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 47.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "65beb65e-8086-4aed-df2a-8a62bde6afcb"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "# data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "# data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "# data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "# data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "# text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "# for i in tqdm(range(data.shape[0])):\r\n",
        "    # if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        # text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafd90d3-592e-44a8-d602-30424903033f"
      },
      "source": [
        "# text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "9a443a1c-929d-46b7-a380-16c307c65131"
      },
      "source": [
        "# s0 = text_pairs[10][0]\r\n",
        "# text_list = wrapper.wrap(s0[:300])\r\n",
        "# for line in text_list:\r\n",
        "#     print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLsMmUFFnHA",
        "outputId": "8dae7e91-6649-4075-c50a-27891d0f088e"
      },
      "source": [
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n",
            "в киеве исключили новый раунд минских переговоров\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "73a0672e-bdab-4f35-fe0a-acd287057e86"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC79r_7EPy6",
        "outputId": "16bae6e3-515d-4d0d-bb33-6136efe8113f"
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ceb78e-30ac-4d27-acb4-2311cdfd0604"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5e8ea4-26e3-4ca1-b2c0-2001af653a85"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c27711f0-7b17-428b-a61c-fd50246bc6b4"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        "    trax.data.FilterByLength(2048)\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37955ef-eeb5-4d24-b95c-3fc073198334"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 6380   759    86 10316 15949     1     0  1244   573   374  4046   188\n",
            "   707 11201    17  9950  1355  8943 10406     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "dded72c8-0a54-4b8c-bda6-3c2364fbc21d"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "8eda710f-f43c-4d37-a7a8-d74616ec351f"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   17,  2293,  4248,  5676,  2778,   857,  4226,  2650,  7915,\n",
              "         167,   244,   552,   537, 13597,     5,   289,  6757,  2745,\n",
              "       15945,  4889,  5726,  1396,  1131,   881,    72,    20,  2767,\n",
              "          72, 10519, 15949,    63,   226,   258,  2809,  2515,  8677,\n",
              "       15949,    82,  3569, 13468,    46,  6011, 12804,  4044,  6076,\n",
              "       12301,  3558,   678,   167, 15939,  7090, 15977,  7170,   964,\n",
              "       15974,  1510, 15945,    46,  5656, 10632,   188, 14874,    73,\n",
              "        1637,  7915, 10973,   274,   171, 15934,  3599,   210, 11791,\n",
              "       15945,    46, 11996, 13335,   212,  4332,  6249,  1202, 15960,\n",
              "        2101,  3059,    10,   210,   441, 15945,  2335,  5441,  6011,\n",
              "       12804,  8677, 15945,  2069,    31,   493, 10612, 12780,  2462,\n",
              "       15940,   210,  4229, 15949,  1370,  8313,   355,  8677, 15945,\n",
              "         249,  4003,    82,  3569,  9103,  7227,  1721,   348,  3280,\n",
              "          54,  4855, 11028,    75,   142, 15985,  3987, 15949,    17,\n",
              "        2293,  2667,  7261,  2778,  7738,  6612,   175,    57,   348,\n",
              "       15949,  2767,    72, 10519,  1537, 11791,  3987, 15945, 10973,\n",
              "       15950,   171, 15934,   796,   210,  8447, 15945,  5602,  1714,\n",
              "         189,  6491, 13689, 15867,    61, 15949,  5040, 13502,  6275,\n",
              "        9852, 12438,    45,    40,  7778,   480,   764,  1606, 15949,\n",
              "         498, 11244,  7360,  1278,   164,  1029,  2060,  2650, 11004,\n",
              "          16, 15206,    29,   452,   205,  5297,  2172, 10281,    25,\n",
              "        2680, 15949,     5,  7809,  2767,    72, 10519,  3818,     4,\n",
              "        1211,  4463,   173, 15949,     4,   107,  1118,  1409,   173,\n",
              "          16,    70,  1854,  6623,   180,    25,  1889,   857,  8677,\n",
              "         236,  6588,  4159,  6011, 12804,  7809, 15949,   498,   348,\n",
              "       10771,  6011, 12804,    16,  9554,  1790,    17,  1792,     8,\n",
              "        1187,  5358, 15905, 15985, 12802,   537,  5367,  2132,  5058,\n",
              "        3732, 15960,     9,   238,   155,    61,   439,   167, 15956,\n",
              "         365,   644,   210,  2255,  1802,   537,    41,   277,  9554,\n",
              "        5561,  3300,  1457,    53,   257,   141,    16,  4209,   125,\n",
              "        2068,   167,   655,    45,   508,  3185,  3408, 15985,   426,\n",
              "           1,     0,   881,    72,    20,  2767,    72, 10519,  8111,\n",
              "       10973,   274,   171, 15934,  3599,    25,  2680,   857,  8677,\n",
              "           1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "d55afd6d-eba9-4b0b-82b6-d6076c759b17"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.0001)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.0001), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "2acb90f6-18e9-4075-dd8f-c55e8d27487b"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  120100: Ran 100 train steps in 84.00 secs\n",
            "Step  120100: train CrossEntropyLoss |  4.64011669\n",
            "Step  120100: eval  CrossEntropyLoss |  3.95796466\n",
            "Step  120100: eval          Accuracy |  0.39240506\n",
            "\n",
            "Step  120200: Ran 100 train steps in 63.05 secs\n",
            "Step  120200: train CrossEntropyLoss |  4.69159174\n",
            "Step  120200: eval  CrossEntropyLoss |  4.81570959\n",
            "Step  120200: eval          Accuracy |  0.25603864\n",
            "\n",
            "Step  120300: Ran 100 train steps in 47.08 secs\n",
            "Step  120300: train CrossEntropyLoss |  4.54871464\n",
            "Step  120300: eval  CrossEntropyLoss |  4.57701111\n",
            "Step  120300: eval          Accuracy |  0.28301889\n",
            "\n",
            "Step  120400: Ran 100 train steps in 47.48 secs\n",
            "Step  120400: train CrossEntropyLoss |  4.54306746\n",
            "Step  120400: eval  CrossEntropyLoss |  4.27548933\n",
            "Step  120400: eval          Accuracy |  0.33644858\n",
            "\n",
            "Step  120500: Ran 100 train steps in 47.16 secs\n",
            "Step  120500: train CrossEntropyLoss |  4.52011251\n",
            "Step  120500: eval  CrossEntropyLoss |  4.39051199\n",
            "Step  120500: eval          Accuracy |  0.35789475\n",
            "\n",
            "Step  120600: Ran 100 train steps in 47.66 secs\n",
            "Step  120600: train CrossEntropyLoss |  4.42797327\n",
            "Step  120600: eval  CrossEntropyLoss |  4.44227314\n",
            "Step  120600: eval          Accuracy |  0.26923078\n",
            "\n",
            "Step  120700: Ran 100 train steps in 47.50 secs\n",
            "Step  120700: train CrossEntropyLoss |  4.41607141\n",
            "Step  120700: eval  CrossEntropyLoss |  5.15514278\n",
            "Step  120700: eval          Accuracy |  0.23577237\n",
            "\n",
            "Step  120800: Ran 100 train steps in 47.45 secs\n",
            "Step  120800: train CrossEntropyLoss |  4.45359707\n",
            "Step  120800: eval  CrossEntropyLoss |  4.19658422\n",
            "Step  120800: eval          Accuracy |  0.33333334\n",
            "\n",
            "Step  120900: Ran 100 train steps in 47.42 secs\n",
            "Step  120900: train CrossEntropyLoss |  4.31069708\n",
            "Step  120900: eval  CrossEntropyLoss |  4.81473207\n",
            "Step  120900: eval          Accuracy |  0.20999999\n",
            "\n",
            "Step  121000: Ran 100 train steps in 47.28 secs\n",
            "Step  121000: train CrossEntropyLoss |  4.36711931\n",
            "Step  121000: eval  CrossEntropyLoss |  4.24197960\n",
            "Step  121000: eval          Accuracy |  0.36651585\n",
            "\n",
            "Step  121100: Ran 100 train steps in 47.61 secs\n",
            "Step  121100: train CrossEntropyLoss |  4.28299284\n",
            "Step  121100: eval  CrossEntropyLoss |  4.05895042\n",
            "Step  121100: eval          Accuracy |  0.35353535\n",
            "\n",
            "Step  121200: Ran 100 train steps in 47.55 secs\n",
            "Step  121200: train CrossEntropyLoss |  4.27761507\n",
            "Step  121200: eval  CrossEntropyLoss |  4.79214334\n",
            "Step  121200: eval          Accuracy |  0.30097088\n",
            "\n",
            "Step  121300: Ran 100 train steps in 47.57 secs\n",
            "Step  121300: train CrossEntropyLoss |  4.26980829\n",
            "Step  121300: eval  CrossEntropyLoss |  4.83822966\n",
            "Step  121300: eval          Accuracy |  0.28455287\n",
            "\n",
            "Step  121400: Ran 100 train steps in 47.57 secs\n",
            "Step  121400: train CrossEntropyLoss |  4.25858593\n",
            "Step  121400: eval  CrossEntropyLoss |  4.03145742\n",
            "Step  121400: eval          Accuracy |  0.35514018\n",
            "\n",
            "Step  121500: Ran 100 train steps in 47.69 secs\n",
            "Step  121500: train CrossEntropyLoss |  4.29510927\n",
            "Step  121500: eval  CrossEntropyLoss |  4.26718426\n",
            "Step  121500: eval          Accuracy |  0.34285715\n",
            "\n",
            "Step  121600: Ran 100 train steps in 47.67 secs\n",
            "Step  121600: train CrossEntropyLoss |  4.25949621\n",
            "Step  121600: eval  CrossEntropyLoss |  4.08429384\n",
            "Step  121600: eval          Accuracy |  0.32653061\n",
            "\n",
            "Step  121700: Ran 100 train steps in 47.69 secs\n",
            "Step  121700: train CrossEntropyLoss |  4.22982121\n",
            "Step  121700: eval  CrossEntropyLoss |  3.97907305\n",
            "Step  121700: eval          Accuracy |  0.32989693\n",
            "\n",
            "Step  121800: Ran 100 train steps in 47.59 secs\n",
            "Step  121800: train CrossEntropyLoss |  4.26503134\n",
            "Step  121800: eval  CrossEntropyLoss |  4.58732748\n",
            "Step  121800: eval          Accuracy |  0.30158731\n",
            "\n",
            "Step  121900: Ran 100 train steps in 47.76 secs\n",
            "Step  121900: train CrossEntropyLoss |  4.26685095\n",
            "Step  121900: eval  CrossEntropyLoss |  3.74163198\n",
            "Step  121900: eval          Accuracy |  0.38095239\n",
            "\n",
            "Step  122000: Ran 100 train steps in 47.72 secs\n",
            "Step  122000: train CrossEntropyLoss |  4.19638491\n",
            "Step  122000: eval  CrossEntropyLoss |  4.02211380\n",
            "Step  122000: eval          Accuracy |  0.36458334\n",
            "\n",
            "Step  122100: Ran 100 train steps in 47.98 secs\n",
            "Step  122100: train CrossEntropyLoss |  4.12222862\n",
            "Step  122100: eval  CrossEntropyLoss |  4.50122595\n",
            "Step  122100: eval          Accuracy |  0.33620688\n",
            "\n",
            "Step  122200: Ran 100 train steps in 47.74 secs\n",
            "Step  122200: train CrossEntropyLoss |  4.15570354\n",
            "Step  122200: eval  CrossEntropyLoss |  4.03039074\n",
            "Step  122200: eval          Accuracy |  0.34343433\n",
            "\n",
            "Step  122300: Ran 100 train steps in 47.75 secs\n",
            "Step  122300: train CrossEntropyLoss |  4.16167068\n",
            "Step  122300: eval  CrossEntropyLoss |  4.76526833\n",
            "Step  122300: eval          Accuracy |  0.30107528\n",
            "\n",
            "Step  122400: Ran 100 train steps in 47.44 secs\n",
            "Step  122400: train CrossEntropyLoss |  4.20141459\n",
            "Step  122400: eval  CrossEntropyLoss |  4.20275879\n",
            "Step  122400: eval          Accuracy |  0.35114503\n",
            "\n",
            "Step  122500: Ran 100 train steps in 47.70 secs\n",
            "Step  122500: train CrossEntropyLoss |  4.17178869\n",
            "Step  122500: eval  CrossEntropyLoss |  3.93836284\n",
            "Step  122500: eval          Accuracy |  0.30612245\n",
            "\n",
            "Step  122600: Ran 100 train steps in 47.85 secs\n",
            "Step  122600: train CrossEntropyLoss |  4.09600163\n",
            "Step  122600: eval  CrossEntropyLoss |  4.61040020\n",
            "Step  122600: eval          Accuracy |  0.28947368\n",
            "\n",
            "Step  122700: Ran 100 train steps in 47.68 secs\n",
            "Step  122700: train CrossEntropyLoss |  4.12263441\n",
            "Step  122700: eval  CrossEntropyLoss |  3.95526457\n",
            "Step  122700: eval          Accuracy |  0.35051548\n",
            "\n",
            "Step  122800: Ran 100 train steps in 47.59 secs\n",
            "Step  122800: train CrossEntropyLoss |  4.11534119\n",
            "Step  122800: eval  CrossEntropyLoss |  4.47350025\n",
            "Step  122800: eval          Accuracy |  0.30508474\n",
            "\n",
            "Step  122900: Ran 100 train steps in 47.63 secs\n",
            "Step  122900: train CrossEntropyLoss |  4.12502956\n",
            "Step  122900: eval  CrossEntropyLoss |  3.99285674\n",
            "Step  122900: eval          Accuracy |  0.30851063\n",
            "\n",
            "Step  123000: Ran 100 train steps in 48.17 secs\n",
            "Step  123000: train CrossEntropyLoss |  4.09718943\n",
            "Step  123000: eval  CrossEntropyLoss |  3.70778775\n",
            "Step  123000: eval          Accuracy |  0.36279070\n",
            "\n",
            "Step  123100: Ran 100 train steps in 47.77 secs\n",
            "Step  123100: train CrossEntropyLoss |  4.10352468\n",
            "Step  123100: eval  CrossEntropyLoss |  4.50874662\n",
            "Step  123100: eval          Accuracy |  0.33333337\n",
            "\n",
            "Step  123200: Ran 100 train steps in 47.83 secs\n",
            "Step  123200: train CrossEntropyLoss |  4.14205217\n",
            "Step  123200: eval  CrossEntropyLoss |  4.14849567\n",
            "Step  123200: eval          Accuracy |  0.32743362\n",
            "\n",
            "Step  123300: Ran 100 train steps in 47.72 secs\n",
            "Step  123300: train CrossEntropyLoss |  4.01633215\n",
            "Step  123300: eval  CrossEntropyLoss |  3.81709552\n",
            "Step  123300: eval          Accuracy |  0.36842105\n",
            "\n",
            "Step  123400: Ran 100 train steps in 47.60 secs\n",
            "Step  123400: train CrossEntropyLoss |  4.12663698\n",
            "Step  123400: eval  CrossEntropyLoss |  4.18875122\n",
            "Step  123400: eval          Accuracy |  0.37735850\n",
            "\n",
            "Step  123500: Ran 100 train steps in 47.70 secs\n",
            "Step  123500: train CrossEntropyLoss |  4.07169962\n",
            "Step  123500: eval  CrossEntropyLoss |  3.84777570\n",
            "Step  123500: eval          Accuracy |  0.35555556\n",
            "\n",
            "Step  123600: Ran 100 train steps in 47.76 secs\n",
            "Step  123600: train CrossEntropyLoss |  4.02845621\n",
            "Step  123600: eval  CrossEntropyLoss |  4.04554033\n",
            "Step  123600: eval          Accuracy |  0.31355932\n",
            "\n",
            "Step  123700: Ran 100 train steps in 47.69 secs\n",
            "Step  123700: train CrossEntropyLoss |  4.08501291\n",
            "Step  123700: eval  CrossEntropyLoss |  3.89728570\n",
            "Step  123700: eval          Accuracy |  0.31606218\n",
            "\n",
            "Step  123800: Ran 100 train steps in 47.81 secs\n",
            "Step  123800: train CrossEntropyLoss |  4.05195999\n",
            "Step  123800: eval  CrossEntropyLoss |  5.00280571\n",
            "Step  123800: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  123900: Ran 100 train steps in 47.70 secs\n",
            "Step  123900: train CrossEntropyLoss |  4.04511881\n",
            "Step  123900: eval  CrossEntropyLoss |  4.33129930\n",
            "Step  123900: eval          Accuracy |  0.35087720\n",
            "\n",
            "Step  124000: Ran 100 train steps in 47.64 secs\n",
            "Step  124000: train CrossEntropyLoss |  4.08191109\n",
            "Step  124000: eval  CrossEntropyLoss |  3.58949018\n",
            "Step  124000: eval          Accuracy |  0.41935483\n",
            "\n",
            "Step  124100: Ran 100 train steps in 48.12 secs\n",
            "Step  124100: train CrossEntropyLoss |  4.03398991\n",
            "Step  124100: eval  CrossEntropyLoss |  4.08481836\n",
            "Step  124100: eval          Accuracy |  0.31034482\n",
            "\n",
            "Step  124200: Ran 100 train steps in 47.63 secs\n",
            "Step  124200: train CrossEntropyLoss |  4.03214169\n",
            "Step  124200: eval  CrossEntropyLoss |  3.67602491\n",
            "Step  124200: eval          Accuracy |  0.34196892\n",
            "\n",
            "Step  124300: Ran 100 train steps in 47.86 secs\n",
            "Step  124300: train CrossEntropyLoss |  4.06958008\n",
            "Step  124300: eval  CrossEntropyLoss |  4.30569267\n",
            "Step  124300: eval          Accuracy |  0.33333334\n",
            "\n",
            "Step  124400: Ran 100 train steps in 47.63 secs\n",
            "Step  124400: train CrossEntropyLoss |  4.07725239\n",
            "Step  124400: eval  CrossEntropyLoss |  3.46713042\n",
            "Step  124400: eval          Accuracy |  0.48999998\n",
            "\n",
            "Step  124500: Ran 100 train steps in 47.81 secs\n",
            "Step  124500: train CrossEntropyLoss |  4.04295015\n",
            "Step  124500: eval  CrossEntropyLoss |  3.62101150\n",
            "Step  124500: eval          Accuracy |  0.40625000\n",
            "\n",
            "Step  124600: Ran 100 train steps in 47.58 secs\n",
            "Step  124600: train CrossEntropyLoss |  4.05809736\n",
            "Step  124600: eval  CrossEntropyLoss |  3.55457139\n",
            "Step  124600: eval          Accuracy |  0.35999998\n",
            "\n",
            "Step  124700: Ran 100 train steps in 47.65 secs\n",
            "Step  124700: train CrossEntropyLoss |  4.02675724\n",
            "Step  124700: eval  CrossEntropyLoss |  4.30201101\n",
            "Step  124700: eval          Accuracy |  0.28181818\n",
            "\n",
            "Step  124800: Ran 100 train steps in 47.73 secs\n",
            "Step  124800: train CrossEntropyLoss |  4.00134182\n",
            "Step  124800: eval  CrossEntropyLoss |  4.32746220\n",
            "Step  124800: eval          Accuracy |  0.28571430\n",
            "\n",
            "Step  124900: Ran 100 train steps in 47.81 secs\n",
            "Step  124900: train CrossEntropyLoss |  4.00601482\n",
            "Step  124900: eval  CrossEntropyLoss |  4.01293993\n",
            "Step  124900: eval          Accuracy |  0.33944952\n",
            "\n",
            "Step  125000: Ran 100 train steps in 47.60 secs\n",
            "Step  125000: train CrossEntropyLoss |  4.02439737\n",
            "Step  125000: eval  CrossEntropyLoss |  4.31782913\n",
            "Step  125000: eval          Accuracy |  0.30653265\n",
            "\n",
            "Step  125100: Ran 100 train steps in 47.88 secs\n",
            "Step  125100: train CrossEntropyLoss |  3.99335289\n",
            "Step  125100: eval  CrossEntropyLoss |  3.94000077\n",
            "Step  125100: eval          Accuracy |  0.34951457\n",
            "\n",
            "Step  125200: Ran 100 train steps in 47.74 secs\n",
            "Step  125200: train CrossEntropyLoss |  3.96542931\n",
            "Step  125200: eval  CrossEntropyLoss |  4.37546492\n",
            "Step  125200: eval          Accuracy |  0.26041669\n",
            "\n",
            "Step  125300: Ran 100 train steps in 47.70 secs\n",
            "Step  125300: train CrossEntropyLoss |  4.05593872\n",
            "Step  125300: eval  CrossEntropyLoss |  4.14621544\n",
            "Step  125300: eval          Accuracy |  0.34408602\n",
            "\n",
            "Step  125400: Ran 100 train steps in 47.67 secs\n",
            "Step  125400: train CrossEntropyLoss |  4.00232935\n",
            "Step  125400: eval  CrossEntropyLoss |  3.86486888\n",
            "Step  125400: eval          Accuracy |  0.33333334\n",
            "\n",
            "Step  125500: Ran 100 train steps in 47.98 secs\n",
            "Step  125500: train CrossEntropyLoss |  3.99476624\n",
            "Step  125500: eval  CrossEntropyLoss |  3.77165341\n",
            "Step  125500: eval          Accuracy |  0.32727271\n",
            "\n",
            "Step  125600: Ran 100 train steps in 47.79 secs\n",
            "Step  125600: train CrossEntropyLoss |  3.96832824\n",
            "Step  125600: eval  CrossEntropyLoss |  4.06452560\n",
            "Step  125600: eval          Accuracy |  0.35576925\n",
            "\n",
            "Step  125700: Ran 100 train steps in 47.65 secs\n",
            "Step  125700: train CrossEntropyLoss |  3.93433976\n",
            "Step  125700: eval  CrossEntropyLoss |  4.42822218\n",
            "Step  125700: eval          Accuracy |  0.29906541\n",
            "\n",
            "Step  125800: Ran 100 train steps in 47.86 secs\n",
            "Step  125800: train CrossEntropyLoss |  3.97397900\n",
            "Step  125800: eval  CrossEntropyLoss |  4.21282578\n",
            "Step  125800: eval          Accuracy |  0.30731708\n",
            "\n",
            "Step  125900: Ran 100 train steps in 47.84 secs\n",
            "Step  125900: train CrossEntropyLoss |  3.96906972\n",
            "Step  125900: eval  CrossEntropyLoss |  4.77108717\n",
            "Step  125900: eval          Accuracy |  0.26315790\n",
            "\n",
            "Step  126000: Ran 100 train steps in 47.84 secs\n",
            "Step  126000: train CrossEntropyLoss |  3.95605969\n",
            "Step  126000: eval  CrossEntropyLoss |  3.86992049\n",
            "Step  126000: eval          Accuracy |  0.34313726\n",
            "\n",
            "Step  126100: Ran 100 train steps in 47.49 secs\n",
            "Step  126100: train CrossEntropyLoss |  3.89976835\n",
            "Step  126100: eval  CrossEntropyLoss |  4.25792789\n",
            "Step  126100: eval          Accuracy |  0.34579438\n",
            "\n",
            "Step  126200: Ran 100 train steps in 47.32 secs\n",
            "Step  126200: train CrossEntropyLoss |  3.94961715\n",
            "Step  126200: eval  CrossEntropyLoss |  3.92364836\n",
            "Step  126200: eval          Accuracy |  0.29824561\n",
            "\n",
            "Step  126300: Ran 100 train steps in 47.12 secs\n",
            "Step  126300: train CrossEntropyLoss |  3.97685885\n",
            "Step  126300: eval  CrossEntropyLoss |  4.13348722\n",
            "Step  126300: eval          Accuracy |  0.34090909\n",
            "\n",
            "Step  126400: Ran 100 train steps in 46.94 secs\n",
            "Step  126400: train CrossEntropyLoss |  3.96156883\n",
            "Step  126400: eval  CrossEntropyLoss |  3.49259257\n",
            "Step  126400: eval          Accuracy |  0.41904762\n",
            "\n",
            "Step  126500: Ran 100 train steps in 46.95 secs\n",
            "Step  126500: train CrossEntropyLoss |  4.00825930\n",
            "Step  126500: eval  CrossEntropyLoss |  3.58347726\n",
            "Step  126500: eval          Accuracy |  0.41284403\n",
            "\n",
            "Step  126600: Ran 100 train steps in 46.88 secs\n",
            "Step  126600: train CrossEntropyLoss |  3.91051316\n",
            "Step  126600: eval  CrossEntropyLoss |  3.61999583\n",
            "Step  126600: eval          Accuracy |  0.39047620\n",
            "\n",
            "Step  126700: Ran 100 train steps in 46.88 secs\n",
            "Step  126700: train CrossEntropyLoss |  3.95095396\n",
            "Step  126700: eval  CrossEntropyLoss |  4.27290678\n",
            "Step  126700: eval          Accuracy |  0.30516434\n",
            "\n",
            "Step  126800: Ran 100 train steps in 46.88 secs\n",
            "Step  126800: train CrossEntropyLoss |  3.86103344\n",
            "Step  126800: eval  CrossEntropyLoss |  3.93956256\n",
            "Step  126800: eval          Accuracy |  0.36363634\n",
            "\n",
            "Step  126900: Ran 100 train steps in 47.01 secs\n",
            "Step  126900: train CrossEntropyLoss |  3.95367169\n",
            "Step  126900: eval  CrossEntropyLoss |  4.38981771\n",
            "Step  126900: eval          Accuracy |  0.33653846\n",
            "\n",
            "Step  127000: Ran 100 train steps in 46.92 secs\n",
            "Step  127000: train CrossEntropyLoss |  3.91439223\n",
            "Step  127000: eval  CrossEntropyLoss |  3.78797603\n",
            "Step  127000: eval          Accuracy |  0.41489360\n",
            "\n",
            "Step  127100: Ran 100 train steps in 46.89 secs\n",
            "Step  127100: train CrossEntropyLoss |  3.98079371\n",
            "Step  127100: eval  CrossEntropyLoss |  3.98397064\n",
            "Step  127100: eval          Accuracy |  0.35483870\n",
            "\n",
            "Step  127200: Ran 100 train steps in 46.84 secs\n",
            "Step  127200: train CrossEntropyLoss |  3.97115779\n",
            "Step  127200: eval  CrossEntropyLoss |  4.06486273\n",
            "Step  127200: eval          Accuracy |  0.35833335\n",
            "\n",
            "Step  127300: Ran 100 train steps in 46.73 secs\n",
            "Step  127300: train CrossEntropyLoss |  3.93669820\n",
            "Step  127300: eval  CrossEntropyLoss |  3.96106815\n",
            "Step  127300: eval          Accuracy |  0.34343433\n",
            "\n",
            "Step  127400: Ran 100 train steps in 46.85 secs\n",
            "Step  127400: train CrossEntropyLoss |  3.93813825\n",
            "Step  127400: eval  CrossEntropyLoss |  3.93223286\n",
            "Step  127400: eval          Accuracy |  0.37272727\n",
            "\n",
            "Step  127500: Ran 100 train steps in 46.98 secs\n",
            "Step  127500: train CrossEntropyLoss |  3.96565819\n",
            "Step  127500: eval  CrossEntropyLoss |  4.27056313\n",
            "Step  127500: eval          Accuracy |  0.32432434\n",
            "\n",
            "Step  127600: Ran 100 train steps in 46.89 secs\n",
            "Step  127600: train CrossEntropyLoss |  3.88135147\n",
            "Step  127600: eval  CrossEntropyLoss |  3.66450620\n",
            "Step  127600: eval          Accuracy |  0.38636366\n",
            "\n",
            "Step  127700: Ran 100 train steps in 47.05 secs\n",
            "Step  127700: train CrossEntropyLoss |  3.94079757\n",
            "Step  127700: eval  CrossEntropyLoss |  4.35940313\n",
            "Step  127700: eval          Accuracy |  0.31578949\n",
            "\n",
            "Step  127800: Ran 100 train steps in 47.02 secs\n",
            "Step  127800: train CrossEntropyLoss |  3.92785478\n",
            "Step  127800: eval  CrossEntropyLoss |  4.46287489\n",
            "Step  127800: eval          Accuracy |  0.27619049\n",
            "\n",
            "Step  127900: Ran 100 train steps in 46.92 secs\n",
            "Step  127900: train CrossEntropyLoss |  4.02370644\n",
            "Step  127900: eval  CrossEntropyLoss |  4.13179684\n",
            "Step  127900: eval          Accuracy |  0.31221721\n",
            "\n",
            "Step  128000: Ran 100 train steps in 46.95 secs\n",
            "Step  128000: train CrossEntropyLoss |  3.92025542\n",
            "Step  128000: eval  CrossEntropyLoss |  4.38465929\n",
            "Step  128000: eval          Accuracy |  0.28155339\n",
            "\n",
            "Step  128100: Ran 100 train steps in 46.79 secs\n",
            "Step  128100: train CrossEntropyLoss |  3.93673015\n",
            "Step  128100: eval  CrossEntropyLoss |  4.06362867\n",
            "Step  128100: eval          Accuracy |  0.33636361\n",
            "\n",
            "Step  128200: Ran 100 train steps in 46.98 secs\n",
            "Step  128200: train CrossEntropyLoss |  3.90879011\n",
            "Step  128200: eval  CrossEntropyLoss |  3.67617941\n",
            "Step  128200: eval          Accuracy |  0.41237116\n",
            "\n",
            "Step  128300: Ran 100 train steps in 46.88 secs\n",
            "Step  128300: train CrossEntropyLoss |  3.91701102\n",
            "Step  128300: eval  CrossEntropyLoss |  3.46531153\n",
            "Step  128300: eval          Accuracy |  0.41573033\n",
            "\n",
            "Step  128400: Ran 100 train steps in 46.95 secs\n",
            "Step  128400: train CrossEntropyLoss |  3.89750576\n",
            "Step  128400: eval  CrossEntropyLoss |  3.97466826\n",
            "Step  128400: eval          Accuracy |  0.37142858\n",
            "\n",
            "Step  128500: Ran 100 train steps in 46.85 secs\n",
            "Step  128500: train CrossEntropyLoss |  3.94708085\n",
            "Step  128500: eval  CrossEntropyLoss |  3.06618023\n",
            "Step  128500: eval          Accuracy |  0.46534652\n",
            "\n",
            "Step  128600: Ran 100 train steps in 46.73 secs\n",
            "Step  128600: train CrossEntropyLoss |  3.89119935\n",
            "Step  128600: eval  CrossEntropyLoss |  3.61605668\n",
            "Step  128600: eval          Accuracy |  0.43478259\n",
            "\n",
            "Step  128700: Ran 100 train steps in 46.96 secs\n",
            "Step  128700: train CrossEntropyLoss |  3.85754228\n",
            "Step  128700: eval  CrossEntropyLoss |  3.36216807\n",
            "Step  128700: eval          Accuracy |  0.42162162\n",
            "\n",
            "Step  128800: Ran 100 train steps in 46.74 secs\n",
            "Step  128800: train CrossEntropyLoss |  3.88769674\n",
            "Step  128800: eval  CrossEntropyLoss |  4.23631668\n",
            "Step  128800: eval          Accuracy |  0.30909091\n",
            "\n",
            "Step  128900: Ran 100 train steps in 46.89 secs\n",
            "Step  128900: train CrossEntropyLoss |  3.87276936\n",
            "Step  128900: eval  CrossEntropyLoss |  3.31777048\n",
            "Step  128900: eval          Accuracy |  0.47959182\n",
            "\n",
            "Step  129000: Ran 100 train steps in 46.93 secs\n",
            "Step  129000: train CrossEntropyLoss |  3.84872246\n",
            "Step  129000: eval  CrossEntropyLoss |  4.50942326\n",
            "Step  129000: eval          Accuracy |  0.32110089\n",
            "\n",
            "Step  129100: Ran 100 train steps in 46.95 secs\n",
            "Step  129100: train CrossEntropyLoss |  3.82501173\n",
            "Step  129100: eval  CrossEntropyLoss |  3.72131586\n",
            "Step  129100: eval          Accuracy |  0.37688443\n",
            "\n",
            "Step  129200: Ran 100 train steps in 46.82 secs\n",
            "Step  129200: train CrossEntropyLoss |  3.89703941\n",
            "Step  129200: eval  CrossEntropyLoss |  3.54008532\n",
            "Step  129200: eval          Accuracy |  0.33928573\n",
            "\n",
            "Step  129300: Ran 100 train steps in 46.84 secs\n",
            "Step  129300: train CrossEntropyLoss |  3.86661744\n",
            "Step  129300: eval  CrossEntropyLoss |  3.78240824\n",
            "Step  129300: eval          Accuracy |  0.29166669\n",
            "\n",
            "Step  129400: Ran 100 train steps in 46.94 secs\n",
            "Step  129400: train CrossEntropyLoss |  3.89378929\n",
            "Step  129400: eval  CrossEntropyLoss |  4.18463326\n",
            "Step  129400: eval          Accuracy |  0.30252102\n",
            "\n",
            "Step  129500: Ran 100 train steps in 46.85 secs\n",
            "Step  129500: train CrossEntropyLoss |  3.82594132\n",
            "Step  129500: eval  CrossEntropyLoss |  4.30745745\n",
            "Step  129500: eval          Accuracy |  0.31404957\n",
            "\n",
            "Step  129600: Ran 100 train steps in 47.02 secs\n",
            "Step  129600: train CrossEntropyLoss |  3.82953358\n",
            "Step  129600: eval  CrossEntropyLoss |  3.86502481\n",
            "Step  129600: eval          Accuracy |  0.42929292\n",
            "\n",
            "Step  129700: Ran 100 train steps in 46.61 secs\n",
            "Step  129700: train CrossEntropyLoss |  3.82457328\n",
            "Step  129700: eval  CrossEntropyLoss |  3.90880632\n",
            "Step  129700: eval          Accuracy |  0.39999998\n",
            "\n",
            "Step  129800: Ran 100 train steps in 46.88 secs\n",
            "Step  129800: train CrossEntropyLoss |  3.88205099\n",
            "Step  129800: eval  CrossEntropyLoss |  3.85900307\n",
            "Step  129800: eval          Accuracy |  0.31775701\n",
            "\n",
            "Step  129900: Ran 100 train steps in 46.66 secs\n",
            "Step  129900: train CrossEntropyLoss |  3.87388206\n",
            "Step  129900: eval  CrossEntropyLoss |  3.75166273\n",
            "Step  129900: eval          Accuracy |  0.37113404\n",
            "\n",
            "Step  130000: Ran 100 train steps in 46.78 secs\n",
            "Step  130000: train CrossEntropyLoss |  3.85414004\n",
            "Step  130000: eval  CrossEntropyLoss |  3.40220952\n",
            "Step  130000: eval          Accuracy |  0.47368422\n",
            "\n",
            "Step  130100: Ran 100 train steps in 46.86 secs\n",
            "Step  130100: train CrossEntropyLoss |  3.83485866\n",
            "Step  130100: eval  CrossEntropyLoss |  4.01340342\n",
            "Step  130100: eval          Accuracy |  0.36936939\n",
            "\n",
            "Step  130200: Ran 100 train steps in 46.71 secs\n",
            "Step  130200: train CrossEntropyLoss |  3.80412793\n",
            "Step  130200: eval  CrossEntropyLoss |  3.95407557\n",
            "Step  130200: eval          Accuracy |  0.38596493\n",
            "\n",
            "Step  130300: Ran 100 train steps in 46.90 secs\n",
            "Step  130300: train CrossEntropyLoss |  3.84626293\n",
            "Step  130300: eval  CrossEntropyLoss |  3.64460683\n",
            "Step  130300: eval          Accuracy |  0.42999998\n",
            "\n",
            "Step  130400: Ran 100 train steps in 46.86 secs\n",
            "Step  130400: train CrossEntropyLoss |  3.85155320\n",
            "Step  130400: eval  CrossEntropyLoss |  3.36983180\n",
            "Step  130400: eval          Accuracy |  0.45454547\n",
            "\n",
            "Step  130500: Ran 100 train steps in 46.76 secs\n",
            "Step  130500: train CrossEntropyLoss |  3.81229615\n",
            "Step  130500: eval  CrossEntropyLoss |  3.55502605\n",
            "Step  130500: eval          Accuracy |  0.39583334\n",
            "\n",
            "Step  130600: Ran 100 train steps in 46.86 secs\n",
            "Step  130600: train CrossEntropyLoss |  3.88665247\n",
            "Step  130600: eval  CrossEntropyLoss |  4.20769978\n",
            "Step  130600: eval          Accuracy |  0.31858408\n",
            "\n",
            "Step  130700: Ran 100 train steps in 46.74 secs\n",
            "Step  130700: train CrossEntropyLoss |  3.80865550\n",
            "Step  130700: eval  CrossEntropyLoss |  3.86224842\n",
            "Step  130700: eval          Accuracy |  0.39316240\n",
            "\n",
            "Step  130800: Ran 100 train steps in 47.09 secs\n",
            "Step  130800: train CrossEntropyLoss |  3.79205823\n",
            "Step  130800: eval  CrossEntropyLoss |  3.82245898\n",
            "Step  130800: eval          Accuracy |  0.41752580\n",
            "\n",
            "Step  130900: Ran 100 train steps in 46.74 secs\n",
            "Step  130900: train CrossEntropyLoss |  3.80824828\n",
            "Step  130900: eval  CrossEntropyLoss |  3.65683508\n",
            "Step  130900: eval          Accuracy |  0.43801650\n",
            "\n",
            "Step  131000: Ran 100 train steps in 46.88 secs\n",
            "Step  131000: train CrossEntropyLoss |  3.84590912\n",
            "Step  131000: eval  CrossEntropyLoss |  3.23091888\n",
            "Step  131000: eval          Accuracy |  0.43684211\n",
            "\n",
            "Step  131100: Ran 100 train steps in 46.78 secs\n",
            "Step  131100: train CrossEntropyLoss |  3.80528283\n",
            "Step  131100: eval  CrossEntropyLoss |  2.95901370\n",
            "Step  131100: eval          Accuracy |  0.48999998\n",
            "\n",
            "Step  131200: Ran 100 train steps in 46.90 secs\n",
            "Step  131200: train CrossEntropyLoss |  3.80042052\n",
            "Step  131200: eval  CrossEntropyLoss |  3.97315383\n",
            "Step  131200: eval          Accuracy |  0.35384616\n",
            "\n",
            "Step  131300: Ran 100 train steps in 47.00 secs\n",
            "Step  131300: train CrossEntropyLoss |  3.82229781\n",
            "Step  131300: eval  CrossEntropyLoss |  3.59731960\n",
            "Step  131300: eval          Accuracy |  0.38461539\n",
            "\n",
            "Step  131400: Ran 100 train steps in 46.75 secs\n",
            "Step  131400: train CrossEntropyLoss |  3.81415939\n",
            "Step  131400: eval  CrossEntropyLoss |  4.01592207\n",
            "Step  131400: eval          Accuracy |  0.34020621\n",
            "\n",
            "Step  131500: Ran 100 train steps in 46.97 secs\n",
            "Step  131500: train CrossEntropyLoss |  3.77403545\n",
            "Step  131500: eval  CrossEntropyLoss |  3.67236066\n",
            "Step  131500: eval          Accuracy |  0.37634408\n",
            "\n",
            "Step  131600: Ran 100 train steps in 46.77 secs\n",
            "Step  131600: train CrossEntropyLoss |  3.77211761\n",
            "Step  131600: eval  CrossEntropyLoss |  3.85591245\n",
            "Step  131600: eval          Accuracy |  0.37142858\n",
            "\n",
            "Step  131700: Ran 100 train steps in 46.87 secs\n",
            "Step  131700: train CrossEntropyLoss |  3.79159999\n",
            "Step  131700: eval  CrossEntropyLoss |  3.98837566\n",
            "Step  131700: eval          Accuracy |  0.31666669\n",
            "\n",
            "Step  131800: Ran 100 train steps in 46.90 secs\n",
            "Step  131800: train CrossEntropyLoss |  3.77976942\n",
            "Step  131800: eval  CrossEntropyLoss |  3.58999300\n",
            "Step  131800: eval          Accuracy |  0.37765956\n",
            "\n",
            "Step  131900: Ran 100 train steps in 46.91 secs\n",
            "Step  131900: train CrossEntropyLoss |  3.82671595\n",
            "Step  131900: eval  CrossEntropyLoss |  3.64146137\n",
            "Step  131900: eval          Accuracy |  0.39285716\n",
            "\n",
            "Step  132000: Ran 100 train steps in 46.86 secs\n",
            "Step  132000: train CrossEntropyLoss |  3.78512955\n",
            "Step  132000: eval  CrossEntropyLoss |  3.30191994\n",
            "Step  132000: eval          Accuracy |  0.43103448\n",
            "\n",
            "Step  132100: Ran 100 train steps in 46.67 secs\n",
            "Step  132100: train CrossEntropyLoss |  3.76911664\n",
            "Step  132100: eval  CrossEntropyLoss |  3.35876584\n",
            "Step  132100: eval          Accuracy |  0.38888890\n",
            "\n",
            "Step  132200: Ran 100 train steps in 47.07 secs\n",
            "Step  132200: train CrossEntropyLoss |  3.75733685\n",
            "Step  132200: eval  CrossEntropyLoss |  3.41963887\n",
            "Step  132200: eval          Accuracy |  0.41850218\n",
            "\n",
            "Step  132300: Ran 100 train steps in 46.79 secs\n",
            "Step  132300: train CrossEntropyLoss |  3.76679564\n",
            "Step  132300: eval  CrossEntropyLoss |  3.06280732\n",
            "Step  132300: eval          Accuracy |  0.46808508\n",
            "\n",
            "Step  132400: Ran 100 train steps in 46.81 secs\n",
            "Step  132400: train CrossEntropyLoss |  3.75347924\n",
            "Step  132400: eval  CrossEntropyLoss |  3.32466054\n",
            "Step  132400: eval          Accuracy |  0.44285715\n",
            "\n",
            "Step  132500: Ran 100 train steps in 46.77 secs\n",
            "Step  132500: train CrossEntropyLoss |  3.76775193\n",
            "Step  132500: eval  CrossEntropyLoss |  3.19877028\n",
            "Step  132500: eval          Accuracy |  0.48305085\n",
            "\n",
            "Step  132600: Ran 100 train steps in 46.74 secs\n",
            "Step  132600: train CrossEntropyLoss |  3.74210811\n",
            "Step  132600: eval  CrossEntropyLoss |  3.92937231\n",
            "Step  132600: eval          Accuracy |  0.31818181\n",
            "\n",
            "Step  132700: Ran 100 train steps in 46.89 secs\n",
            "Step  132700: train CrossEntropyLoss |  3.81283140\n",
            "Step  132700: eval  CrossEntropyLoss |  3.85404825\n",
            "Step  132700: eval          Accuracy |  0.39622641\n",
            "\n",
            "Step  132800: Ran 100 train steps in 46.86 secs\n",
            "Step  132800: train CrossEntropyLoss |  3.78023362\n",
            "Step  132800: eval  CrossEntropyLoss |  3.39086032\n",
            "Step  132800: eval          Accuracy |  0.43157896\n",
            "\n",
            "Step  132900: Ran 100 train steps in 46.85 secs\n",
            "Step  132900: train CrossEntropyLoss |  3.77169943\n",
            "Step  132900: eval  CrossEntropyLoss |  3.84149885\n",
            "Step  132900: eval          Accuracy |  0.35643563\n",
            "\n",
            "Step  133000: Ran 100 train steps in 46.78 secs\n",
            "Step  133000: train CrossEntropyLoss |  3.82251143\n",
            "Step  133000: eval  CrossEntropyLoss |  4.00868654\n",
            "Step  133000: eval          Accuracy |  0.36000001\n",
            "\n",
            "Step  133100: Ran 100 train steps in 46.91 secs\n",
            "Step  133100: train CrossEntropyLoss |  3.76075959\n",
            "Step  133100: eval  CrossEntropyLoss |  3.88969731\n",
            "Step  133100: eval          Accuracy |  0.34951457\n",
            "\n",
            "Step  133200: Ran 100 train steps in 47.11 secs\n",
            "Step  133200: train CrossEntropyLoss |  3.71899128\n",
            "Step  133200: eval  CrossEntropyLoss |  3.93939209\n",
            "Step  133200: eval          Accuracy |  0.31481481\n",
            "\n",
            "Step  133300: Ran 100 train steps in 46.81 secs\n",
            "Step  133300: train CrossEntropyLoss |  3.75372267\n",
            "Step  133300: eval  CrossEntropyLoss |  3.49310684\n",
            "Step  133300: eval          Accuracy |  0.44680849\n",
            "\n",
            "Step  133400: Ran 100 train steps in 46.93 secs\n",
            "Step  133400: train CrossEntropyLoss |  3.79983997\n",
            "Step  133400: eval  CrossEntropyLoss |  3.24553800\n",
            "Step  133400: eval          Accuracy |  0.48598129\n",
            "\n",
            "Step  133500: Ran 100 train steps in 46.89 secs\n",
            "Step  133500: train CrossEntropyLoss |  3.72461414\n",
            "Step  133500: eval  CrossEntropyLoss |  3.70021534\n",
            "Step  133500: eval          Accuracy |  0.36871511\n",
            "\n",
            "Step  133600: Ran 100 train steps in 46.82 secs\n",
            "Step  133600: train CrossEntropyLoss |  3.74783206\n",
            "Step  133600: eval  CrossEntropyLoss |  3.99097109\n",
            "Step  133600: eval          Accuracy |  0.35849059\n",
            "\n",
            "Step  133700: Ran 100 train steps in 46.93 secs\n",
            "Step  133700: train CrossEntropyLoss |  3.73836637\n",
            "Step  133700: eval  CrossEntropyLoss |  3.50101137\n",
            "Step  133700: eval          Accuracy |  0.37837839\n",
            "\n",
            "Step  133800: Ran 100 train steps in 46.76 secs\n",
            "Step  133800: train CrossEntropyLoss |  3.71839190\n",
            "Step  133800: eval  CrossEntropyLoss |  3.17447352\n",
            "Step  133800: eval          Accuracy |  0.39593908\n",
            "\n",
            "Step  133900: Ran 100 train steps in 46.96 secs\n",
            "Step  133900: train CrossEntropyLoss |  3.73837781\n",
            "Step  133900: eval  CrossEntropyLoss |  3.43359256\n",
            "Step  133900: eval          Accuracy |  0.40178573\n",
            "\n",
            "Step  134000: Ran 100 train steps in 46.83 secs\n",
            "Step  134000: train CrossEntropyLoss |  3.72805166\n",
            "Step  134000: eval  CrossEntropyLoss |  3.90854788\n",
            "Step  134000: eval          Accuracy |  0.37500000\n",
            "\n",
            "Step  134100: Ran 100 train steps in 46.81 secs\n",
            "Step  134100: train CrossEntropyLoss |  3.68899250\n",
            "Step  134100: eval  CrossEntropyLoss |  4.36740732\n",
            "Step  134100: eval          Accuracy |  0.33663365\n",
            "\n",
            "Step  134200: Ran 100 train steps in 46.81 secs\n",
            "Step  134200: train CrossEntropyLoss |  3.77993894\n",
            "Step  134200: eval  CrossEntropyLoss |  3.67578316\n",
            "Step  134200: eval          Accuracy |  0.40909091\n",
            "\n",
            "Step  134300: Ran 100 train steps in 46.80 secs\n",
            "Step  134300: train CrossEntropyLoss |  3.70901966\n",
            "Step  134300: eval  CrossEntropyLoss |  3.70578146\n",
            "Step  134300: eval          Accuracy |  0.36363634\n",
            "\n",
            "Step  134400: Ran 100 train steps in 47.07 secs\n",
            "Step  134400: train CrossEntropyLoss |  3.77043867\n",
            "Step  134400: eval  CrossEntropyLoss |  3.51383877\n",
            "Step  134400: eval          Accuracy |  0.37142858\n",
            "\n",
            "Step  134500: Ran 100 train steps in 46.91 secs\n",
            "Step  134500: train CrossEntropyLoss |  3.74156189\n",
            "Step  134500: eval  CrossEntropyLoss |  3.68864155\n",
            "Step  134500: eval          Accuracy |  0.37864077\n",
            "\n",
            "Step  134600: Ran 100 train steps in 46.96 secs\n",
            "Step  134600: train CrossEntropyLoss |  3.71474957\n",
            "Step  134600: eval  CrossEntropyLoss |  3.62275481\n",
            "Step  134600: eval          Accuracy |  0.42583734\n",
            "\n",
            "Step  134700: Ran 100 train steps in 46.84 secs\n",
            "Step  134700: train CrossEntropyLoss |  3.69669628\n",
            "Step  134700: eval  CrossEntropyLoss |  4.06009007\n",
            "Step  134700: eval          Accuracy |  0.30841121\n",
            "\n",
            "Step  134800: Ran 100 train steps in 46.96 secs\n",
            "Step  134800: train CrossEntropyLoss |  3.69946957\n",
            "Step  134800: eval  CrossEntropyLoss |  4.03796816\n",
            "Step  134800: eval          Accuracy |  0.31775701\n",
            "\n",
            "Step  134900: Ran 100 train steps in 46.89 secs\n",
            "Step  134900: train CrossEntropyLoss |  3.72430229\n",
            "Step  134900: eval  CrossEntropyLoss |  3.60897350\n",
            "Step  134900: eval          Accuracy |  0.45631069\n",
            "\n",
            "Step  135000: Ran 100 train steps in 46.92 secs\n",
            "Step  135000: train CrossEntropyLoss |  3.65066648\n",
            "Step  135000: eval  CrossEntropyLoss |  3.92780900\n",
            "Step  135000: eval          Accuracy |  0.35087720\n",
            "\n",
            "Step  135100: Ran 100 train steps in 47.05 secs\n",
            "Step  135100: train CrossEntropyLoss |  3.73136687\n",
            "Step  135100: eval  CrossEntropyLoss |  3.89970946\n",
            "Step  135100: eval          Accuracy |  0.36320755\n",
            "\n",
            "Step  135200: Ran 100 train steps in 46.75 secs\n",
            "Step  135200: train CrossEntropyLoss |  3.64799929\n",
            "Step  135200: eval  CrossEntropyLoss |  3.27040887\n",
            "Step  135200: eval          Accuracy |  0.48648649\n",
            "\n",
            "Step  135300: Ran 100 train steps in 46.82 secs\n",
            "Step  135300: train CrossEntropyLoss |  3.66294742\n",
            "Step  135300: eval  CrossEntropyLoss |  4.39263153\n",
            "Step  135300: eval          Accuracy |  0.23846154\n",
            "\n",
            "Step  135400: Ran 100 train steps in 46.89 secs\n",
            "Step  135400: train CrossEntropyLoss |  3.63189745\n",
            "Step  135400: eval  CrossEntropyLoss |  3.69814086\n",
            "Step  135400: eval          Accuracy |  0.41025645\n",
            "\n",
            "Step  135500: Ran 100 train steps in 46.68 secs\n",
            "Step  135500: train CrossEntropyLoss |  3.71050763\n",
            "Step  135500: eval  CrossEntropyLoss |  3.74113512\n",
            "Step  135500: eval          Accuracy |  0.37142858\n",
            "\n",
            "Step  135600: Ran 100 train steps in 46.89 secs\n",
            "Step  135600: train CrossEntropyLoss |  3.69853783\n",
            "Step  135600: eval  CrossEntropyLoss |  3.38094687\n",
            "Step  135600: eval          Accuracy |  0.44888890\n",
            "\n",
            "Step  135700: Ran 100 train steps in 47.16 secs\n",
            "Step  135700: train CrossEntropyLoss |  3.65559673\n",
            "Step  135700: eval  CrossEntropyLoss |  3.75847888\n",
            "Step  135700: eval          Accuracy |  0.37500003\n",
            "\n",
            "Step  135800: Ran 100 train steps in 46.97 secs\n",
            "Step  135800: train CrossEntropyLoss |  3.65575409\n",
            "Step  135800: eval  CrossEntropyLoss |  4.09830093\n",
            "Step  135800: eval          Accuracy |  0.34375000\n",
            "\n",
            "Step  135900: Ran 100 train steps in 46.99 secs\n",
            "Step  135900: train CrossEntropyLoss |  3.65074587\n",
            "Step  135900: eval  CrossEntropyLoss |  3.71828675\n",
            "Step  135900: eval          Accuracy |  0.36752138\n",
            "\n",
            "Step  136000: Ran 100 train steps in 46.92 secs\n",
            "Step  136000: train CrossEntropyLoss |  3.68732476\n",
            "Step  136000: eval  CrossEntropyLoss |  3.56396890\n",
            "Step  136000: eval          Accuracy |  0.44247788\n",
            "\n",
            "Step  136100: Ran 100 train steps in 46.91 secs\n",
            "Step  136100: train CrossEntropyLoss |  3.74542403\n",
            "Step  136100: eval  CrossEntropyLoss |  3.72163963\n",
            "Step  136100: eval          Accuracy |  0.37037039\n",
            "\n",
            "Step  136200: Ran 100 train steps in 46.81 secs\n",
            "Step  136200: train CrossEntropyLoss |  3.69654465\n",
            "Step  136200: eval  CrossEntropyLoss |  4.01591492\n",
            "Step  136200: eval          Accuracy |  0.31958765\n",
            "\n",
            "Step  136300: Ran 100 train steps in 47.05 secs\n",
            "Step  136300: train CrossEntropyLoss |  3.64603090\n",
            "Step  136300: eval  CrossEntropyLoss |  3.61480546\n",
            "Step  136300: eval          Accuracy |  0.45192310\n",
            "\n",
            "Step  136400: Ran 100 train steps in 46.94 secs\n",
            "Step  136400: train CrossEntropyLoss |  3.70773864\n",
            "Step  136400: eval  CrossEntropyLoss |  3.84671354\n",
            "Step  136400: eval          Accuracy |  0.33830845\n",
            "\n",
            "Step  136500: Ran 100 train steps in 46.89 secs\n",
            "Step  136500: train CrossEntropyLoss |  3.67668033\n",
            "Step  136500: eval  CrossEntropyLoss |  4.19522905\n",
            "Step  136500: eval          Accuracy |  0.36190477\n",
            "\n",
            "Step  136600: Ran 100 train steps in 46.83 secs\n",
            "Step  136600: train CrossEntropyLoss |  3.62523890\n",
            "Step  136600: eval  CrossEntropyLoss |  3.57534719\n",
            "Step  136600: eval          Accuracy |  0.47169811\n",
            "\n",
            "Step  136700: Ran 100 train steps in 46.74 secs\n",
            "Step  136700: train CrossEntropyLoss |  3.73272872\n",
            "Step  136700: eval  CrossEntropyLoss |  3.58232284\n",
            "Step  136700: eval          Accuracy |  0.41284403\n",
            "\n",
            "Step  136800: Ran 100 train steps in 46.74 secs\n",
            "Step  136800: train CrossEntropyLoss |  3.70107055\n",
            "Step  136800: eval  CrossEntropyLoss |  3.56846356\n",
            "Step  136800: eval          Accuracy |  0.47663549\n",
            "\n",
            "Step  136900: Ran 100 train steps in 46.83 secs\n",
            "Step  136900: train CrossEntropyLoss |  3.66896558\n",
            "Step  136900: eval  CrossEntropyLoss |  3.73993230\n",
            "Step  136900: eval          Accuracy |  0.38666669\n",
            "\n",
            "Step  137000: Ran 100 train steps in 47.11 secs\n",
            "Step  137000: train CrossEntropyLoss |  3.71573710\n",
            "Step  137000: eval  CrossEntropyLoss |  4.27296782\n",
            "Step  137000: eval          Accuracy |  0.32727271\n",
            "\n",
            "Step  137100: Ran 100 train steps in 46.81 secs\n",
            "Step  137100: train CrossEntropyLoss |  3.72072625\n",
            "Step  137100: eval  CrossEntropyLoss |  3.34252143\n",
            "Step  137100: eval          Accuracy |  0.46268657\n",
            "\n",
            "Step  137200: Ran 100 train steps in 46.82 secs\n",
            "Step  137200: train CrossEntropyLoss |  3.70547366\n",
            "Step  137200: eval  CrossEntropyLoss |  3.77883887\n",
            "Step  137200: eval          Accuracy |  0.33606559\n",
            "\n",
            "Step  137300: Ran 100 train steps in 46.73 secs\n",
            "Step  137300: train CrossEntropyLoss |  3.69562030\n",
            "Step  137300: eval  CrossEntropyLoss |  4.11057997\n",
            "Step  137300: eval          Accuracy |  0.34166670\n",
            "\n",
            "Step  137400: Ran 100 train steps in 46.71 secs\n",
            "Step  137400: train CrossEntropyLoss |  3.61867023\n",
            "Step  137400: eval  CrossEntropyLoss |  2.97781134\n",
            "Step  137400: eval          Accuracy |  0.44827586\n",
            "\n",
            "Step  137500: Ran 100 train steps in 46.85 secs\n",
            "Step  137500: train CrossEntropyLoss |  3.69082475\n",
            "Step  137500: eval  CrossEntropyLoss |  3.37314653\n",
            "Step  137500: eval          Accuracy |  0.42148760\n",
            "\n",
            "Step  137600: Ran 100 train steps in 46.76 secs\n",
            "Step  137600: train CrossEntropyLoss |  3.69377470\n",
            "Step  137600: eval  CrossEntropyLoss |  3.93380570\n",
            "Step  137600: eval          Accuracy |  0.37962964\n",
            "\n",
            "Step  137700: Ran 100 train steps in 46.88 secs\n",
            "Step  137700: train CrossEntropyLoss |  3.64204574\n",
            "Step  137700: eval  CrossEntropyLoss |  3.24747944\n",
            "Step  137700: eval          Accuracy |  0.43076923\n",
            "\n",
            "Step  137800: Ran 100 train steps in 46.65 secs\n",
            "Step  137800: train CrossEntropyLoss |  3.71320915\n",
            "Step  137800: eval  CrossEntropyLoss |  4.14619350\n",
            "Step  137800: eval          Accuracy |  0.29921260\n",
            "\n",
            "Step  137900: Ran 100 train steps in 46.74 secs\n",
            "Step  137900: train CrossEntropyLoss |  3.65376449\n",
            "Step  137900: eval  CrossEntropyLoss |  4.27116537\n",
            "Step  137900: eval          Accuracy |  0.33018869\n",
            "\n",
            "Step  138000: Ran 100 train steps in 46.73 secs\n",
            "Step  138000: train CrossEntropyLoss |  3.69635510\n",
            "Step  138000: eval  CrossEntropyLoss |  3.73584461\n",
            "Step  138000: eval          Accuracy |  0.34999999\n",
            "\n",
            "Step  138100: Ran 100 train steps in 46.75 secs\n",
            "Step  138100: train CrossEntropyLoss |  3.63564968\n",
            "Step  138100: eval  CrossEntropyLoss |  3.85751700\n",
            "Step  138100: eval          Accuracy |  0.38211384\n",
            "\n",
            "Step  138200: Ran 100 train steps in 46.89 secs\n",
            "Step  138200: train CrossEntropyLoss |  3.60261679\n",
            "Step  138200: eval  CrossEntropyLoss |  4.22616291\n",
            "Step  138200: eval          Accuracy |  0.32407409\n",
            "\n",
            "Step  138300: Ran 100 train steps in 46.84 secs\n",
            "Step  138300: train CrossEntropyLoss |  3.61102748\n",
            "Step  138300: eval  CrossEntropyLoss |  3.23065424\n",
            "Step  138300: eval          Accuracy |  0.37967914\n",
            "\n",
            "Step  138400: Ran 100 train steps in 46.95 secs\n",
            "Step  138400: train CrossEntropyLoss |  3.62007928\n",
            "Step  138400: eval  CrossEntropyLoss |  4.11083317\n",
            "Step  138400: eval          Accuracy |  0.35593221\n",
            "\n",
            "Step  138500: Ran 100 train steps in 46.86 secs\n",
            "Step  138500: train CrossEntropyLoss |  3.61172986\n",
            "Step  138500: eval  CrossEntropyLoss |  4.04802513\n",
            "Step  138500: eval          Accuracy |  0.38541669\n",
            "\n",
            "Step  138600: Ran 100 train steps in 46.97 secs\n",
            "Step  138600: train CrossEntropyLoss |  3.61947346\n",
            "Step  138600: eval  CrossEntropyLoss |  3.26738644\n",
            "Step  138600: eval          Accuracy |  0.38053098\n",
            "\n",
            "Step  138700: Ran 100 train steps in 47.21 secs\n",
            "Step  138700: train CrossEntropyLoss |  3.57660842\n",
            "Step  138700: eval  CrossEntropyLoss |  3.79876304\n",
            "Step  138700: eval          Accuracy |  0.39795917\n",
            "\n",
            "Step  138800: Ran 100 train steps in 46.89 secs\n",
            "Step  138800: train CrossEntropyLoss |  3.65736818\n",
            "Step  138800: eval  CrossEntropyLoss |  3.74205041\n",
            "Step  138800: eval          Accuracy |  0.42727271\n",
            "\n",
            "Step  138900: Ran 100 train steps in 47.16 secs\n",
            "Step  138900: train CrossEntropyLoss |  3.64090180\n",
            "Step  138900: eval  CrossEntropyLoss |  3.20506287\n",
            "Step  138900: eval          Accuracy |  0.45871559\n",
            "\n",
            "Step  139000: Ran 100 train steps in 47.12 secs\n",
            "Step  139000: train CrossEntropyLoss |  3.67687392\n",
            "Step  139000: eval  CrossEntropyLoss |  3.76016355\n",
            "Step  139000: eval          Accuracy |  0.36315790\n",
            "\n",
            "Step  139100: Ran 100 train steps in 47.00 secs\n",
            "Step  139100: train CrossEntropyLoss |  3.63310361\n",
            "Step  139100: eval  CrossEntropyLoss |  3.25048566\n",
            "Step  139100: eval          Accuracy |  0.47999999\n",
            "\n",
            "Step  139200: Ran 100 train steps in 46.88 secs\n",
            "Step  139200: train CrossEntropyLoss |  3.61416745\n",
            "Step  139200: eval  CrossEntropyLoss |  3.57588530\n",
            "Step  139200: eval          Accuracy |  0.40566039\n",
            "\n",
            "Step  139300: Ran 100 train steps in 46.93 secs\n",
            "Step  139300: train CrossEntropyLoss |  3.69498491\n",
            "Step  139300: eval  CrossEntropyLoss |  3.59140563\n",
            "Step  139300: eval          Accuracy |  0.37500000\n",
            "\n",
            "Step  139400: Ran 100 train steps in 46.99 secs\n",
            "Step  139400: train CrossEntropyLoss |  3.63516665\n",
            "Step  139400: eval  CrossEntropyLoss |  3.38860750\n",
            "Step  139400: eval          Accuracy |  0.40659341\n",
            "\n",
            "Step  139500: Ran 100 train steps in 46.97 secs\n",
            "Step  139500: train CrossEntropyLoss |  3.63258934\n",
            "Step  139500: eval  CrossEntropyLoss |  3.09488821\n",
            "Step  139500: eval          Accuracy |  0.47619051\n",
            "\n",
            "Step  139600: Ran 100 train steps in 46.99 secs\n",
            "Step  139600: train CrossEntropyLoss |  3.56976533\n",
            "Step  139600: eval  CrossEntropyLoss |  3.52172947\n",
            "Step  139600: eval          Accuracy |  0.38144332\n",
            "\n",
            "Step  139700: Ran 100 train steps in 47.05 secs\n",
            "Step  139700: train CrossEntropyLoss |  3.62501192\n",
            "Step  139700: eval  CrossEntropyLoss |  3.34801698\n",
            "Step  139700: eval          Accuracy |  0.46078432\n",
            "\n",
            "Step  139800: Ran 100 train steps in 47.00 secs\n",
            "Step  139800: train CrossEntropyLoss |  3.62043214\n",
            "Step  139800: eval  CrossEntropyLoss |  3.90999532\n",
            "Step  139800: eval          Accuracy |  0.35858586\n",
            "\n",
            "Step  139900: Ran 100 train steps in 47.03 secs\n",
            "Step  139900: train CrossEntropyLoss |  3.62584615\n",
            "Step  139900: eval  CrossEntropyLoss |  3.61038375\n",
            "Step  139900: eval          Accuracy |  0.35833335\n",
            "\n",
            "Step  140000: Ran 100 train steps in 47.00 secs\n",
            "Step  140000: train CrossEntropyLoss |  3.64295197\n",
            "Step  140000: eval  CrossEntropyLoss |  3.52452779\n",
            "Step  140000: eval          Accuracy |  0.45454544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "# !rsync -a /content/drive/MyDrive/model2/ ~/\r\n",
        "\r\n",
        "# copy the model to Google Drive\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model/\r\n",
        "\r\n",
        "# sync Google Drive dir with the train dir\r\n",
        "# !rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36707faa-569b-4652-8ea4-e8bf44d4567e"
      },
      "source": [
        "train_article = train_text_pairs[5][0]\r\n",
        "train_summary = train_text_pairs[5][1]\r\n",
        "print(wrapper.fill(train_article))\r\n",
        "print('')\r\n",
        "eval_article = eval_text_pairs[1][0]\r\n",
        "eval_summary = eval_text_pairs[1][1]\r\n",
        "print(wrapper.fill(eval_article))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые придумали новый способ взаимодействия с графеном, который\n",
            "позволяет избавиться от \"слипающихся\" листов. статья ученых появилась\n",
            "в журнале acs nano, а ее краткое изложение приводится на сайте северо-\n",
            "западного университета, сотрудники которого принимали участие в\n",
            "работе. известно, что основной трудностью при работе с графеновыми\n",
            "листами является то, что при соприкосновении они слипаются под\n",
            "воздействием сил ван-дер-ваальса между собой при наложении друг на\n",
            "друга. это приводит к потере большинства уникальных свойств материала.\n",
            "для решения подобной проблемы, например, некоторые исследователи\n",
            "кладут между листами прокладки из другого материала, однако такое\n",
            "решение часто не слишком эффективно - атомы прокладки могут\n",
            "образовывать связи с атомами углерода в графене, что снова приводит к\n",
            "появлению дефектов в материале. в рамках нового исследования ученые\n",
            "предложили использовать графен не в виде ровных листов, а в виде\n",
            "смятых в комок листов. по словам исследователей, в подобном виде\n",
            "графен ведет себя как бумажные комки в мусорной корзине - несмотря на\n",
            "достаточно плотное расположение, поверхности листов, из которых они\n",
            "состоят, не соприкасаются. расчеты показывают, что при подобной\n",
            "упаковке листов графен сохраняет около 45 процентов исходной площади\n",
            "поверхности. для сравнения, при других способах организации удается\n",
            "спасти не более 16 процентов площади. графен как теоретическая\n",
            "абстракция рассматривался еще в конце 20-х годов прошлого века.\n",
            "начиная с 1960-х годов, он выступал в качестве удобной математической\n",
            "модели для расчетов в квантовой механике. впервые графен получили на\n",
            "практике константин новоселов и андрей гейм в 2004 году.\n",
            "\n",
            "сша планируют сократить численность военного контингента в южной\n",
            "корее. по информации корейского министерства иностранных дел, к концу\n",
            "2005 года из страны будет выведена треть американского контингента,\n",
            "составляющего в настоящее время 37500 военнослужащих, сообщает\n",
            "reuters. всего к концу 2005 года страну покинут 12500 американских\n",
            "солдат. 3600 из них продолжат службу в ираке. глава корейского мид\n",
            "отметил, что сша подходят к выводу войск очень внимательно, так как\n",
            "ситуация на полуострове остается напряженной. тем не менее, сша пошли\n",
            "навстречу желанию властей южной кореи иметь более независимую армию, и\n",
            "обещают оказать им в этом всяческое содействие. собственные силы южной\n",
            "кореи составляют на сегодняшний день 690 000 человек. армия северной\n",
            "кореи насчитывает 1 100 000 военнослужащих.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620f603b-939a-4700-bba0-e5d71c049af7"
      },
      "source": [
        "# checking first symbol generation\r\n",
        "print(detokenize([next_symbol(tokenize(train_article)+[0], model)]))\r\n",
        "print(detokenize([next_symbol(tokenize(eval_article)+[0], model)]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые\n",
            "сша\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        \r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4942c256-d2eb-4e75-ea5b-b078d094de7f"
      },
      "source": [
        "print(train_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(train_article, model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые предложили использовать графен в мятом виде\n",
            "\n",
            "\n",
            "ученые\n",
            "ученые нау\n",
            "ученые научились\n",
            "ученые научились использовать\n",
            "ученые научились использовать \"\n",
            "ученые научились использовать \"гу\n",
            "ученые научились использовать \"гупа\n",
            "ученые научились использовать \"гупатные\n",
            "ученые научились использовать \"гупатные\"\n",
            "ученые научились использовать \"гупатные\" ли\n",
            "ученые научились использовать \"гупатные\" листы\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751306a6-8f7e-4d1c-d3e3-8ebf0c158034"
      },
      "source": [
        "print(eval_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article, model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша на треть сократят корейскую группировку\n",
            "\n",
            "\n",
            "сша\n",
            "сша сокра\n",
            "сша сократят\n",
            "сша сократят вывод\n",
            "сша сократят вывод войск\n",
            "сша сократят вывод войск в\n",
            "сша сократят вывод войск в сирии\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L83lEskk4L7"
      },
      "source": [
        "model = SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='predict',\r\n",
        "                  ff_activation=tl.Relu)\r\n",
        "\r\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSDbAXjlF2f"
      },
      "source": [
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True, input_signature=shape11)\r\n",
        "\r\n",
        "# save the starting state\r\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c616d9-2057-4ab6-9dbe-6bcbca726638"
      },
      "source": [
        "\r\n",
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "model.state = STARTING_STATE\r\n",
        "output = decoding.autoregressive_sample(model, inputs=None,\r\n",
        "                                        temperature=0.5, max_length=20)\r\n",
        "print(wrapper.fill(detokenize(output[0])))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "преступникзированченченненставшенринщен крастенненченральщен крас\n",
            "сдаченченчен\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}