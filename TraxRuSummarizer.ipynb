{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOF1Iy9+mD26dJCPrzVDZ1B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541bfc7e-f877-477e-858a-9a18aa6bc633"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 31.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 57.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 49.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 56.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 12.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 51.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 64.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 50.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.4MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "6374bdff-585e-4b42-d347-8bae44eb4de8"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "for i in tqdm(range(data.shape[0])):\r\n",
        "    if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file for tokenizer training\r\n",
        "with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "    file.write('\\n'.join(text_full))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "                               --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "sp = spm.SentencePieceProcessor()\r\n",
        "sp.load('bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcS3BZXUdU2L"
      },
      "source": [
        "s0 = text_pairs[10][0]\r\n",
        "text_list = wrapper.wrap(s0[:300])\r\n",
        "for line in text_list:\r\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# tokenizer check\r\n",
        "print('encode: text => id:')\r\n",
        "print(sp.encode_as_pieces(s0[:300]))\r\n",
        "print('')\r\n",
        "print(sp.encode_as_ids(s0[:300]))\r\n",
        "print('')\r\n",
        "print('decode: id => text:')\r\n",
        "print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "print('')\r\n",
        "print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "print(f'Pad id: {sp.pad_id()}')\r\n",
        "print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "print(f'Unknown id: {sp.unk_id()}')\r\n",
        "print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLsMmUFFnHA",
        "outputId": "7cd31166-788a-40ba-b2aa-2d7e9dde9ec6"
      },
      "source": [
        "# loading prepared set to save time\r\n",
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n",
            "в киеве исключили новый раунд минских переговоров\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "4e6aa66a-2dff-47c4-ea16-011cb7db16a3"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC79r_7EPy6",
        "outputId": "481c10d9-80c8-4a5d-ba76-17ac74988da7"
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd91e9d4-0b37-4c54-efbd-12d14e296282"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/') # loading pre-prepared model to save time\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        "    trax.data.FilterByLength(1024)\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97c4aa6-14b6-47ea-b2b4-0709d37a0c01"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[11695   671   664   345  4389    88 15949     1     0  8291    46  3397\n",
            "  2026    54    11 15538     4  1011  4016     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "36782930-df62-4f0d-bfbe-80687cda0f0a"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "5b9e42ac-1aa6-4f7d-f672-aadd83ce05d8"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2809,  1354,  3216,   204,  1586,    65,  2862,   345,  3059,\n",
              "       15929,  6660,  5024,   167, 15991, 15989,  1652,  1443,  1128,\n",
              "       15967,   714, 15963,   406, 10270, 15197, 15977,   455,   814,\n",
              "        1095, 15945,    79,  3216,  1565,    57,   372,  4258,     4,\n",
              "          81,   157,  2183,   329, 15957,  5585,  7944,   448,  2792,\n",
              "          17,  1301,  7955,  5692, 15945,   258,  3320, 15949,    17,\n",
              "        1205,  1877, 15945,  6660,  5024,   277,  2006, 15945,    79,\n",
              "        2792,    25,  2463, 10438,    75,  2611,   707,  4258,   114,\n",
              "         736,     4,  6845,  4136,  1877,    17, 11220, 10170,   167,\n",
              "        9173, 10065,   426,  1671, 15945,    79,     5,  1612,  1095,\n",
              "        1403,   173,  1408,  1341,  2342, 14424,  5923,   358, 14940,\n",
              "          17,  1454,   127, 15945,   976,  6338,  8129,   835, 11502,\n",
              "        5312,     5,  1813,   719, 15945,    41,   277,  5376,  1092,\n",
              "        4485,    25,  8874,   164,  8821,     5,  1813,  1175,  2943,\n",
              "        8446,  7955,  2553, 15949,  3216, 15945,     5,  1140,  2162,\n",
              "       15945,  2198, 14940, 13490,    31,  2342,  3213,  1828,   106,\n",
              "          16,  4105, 15795,  1228, 15945,    41,   277,   455, 15945,\n",
              "          79, 11182,  2459,  6394,   678,    81,    35,  9872,    22,\n",
              "          25,   945,  3216,   204,  7641,    16,  1586, 11367,   309,\n",
              "          64,  3022,  2401, 10991,  6517,   132,    16,  1618,   132,\n",
              "        2172,    25,  8514, 12054, 14179,    83,   295,  7297, 15945,\n",
              "          79,     5,    81,   157,  2183,   118, 15957,  5585,  7944,\n",
              "         448,  5704,  7984,    16,  1659,  3429,  2627,  3015, 13490,\n",
              "          31,  2342,   245,   903, 15945,   401, 15945,  1829, 15945,\n",
              "        8850,    16,  8465, 15949,     1,     0,  3216, 15016,  2792,\n",
              "           4,    81,   157,  2183,   329, 15957,     1,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "acdde951-0395-415d-81b5-cb1926bb3d0a"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    # lr_schedule = trax.supervised.lr_schedules.constant(0.0001)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.00005), # Optimizer \r\n",
        "      # lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "625b6755-19a0-45f9-bf74-2681c165c49e"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  220100: Ran 100 train steps in 83.47 secs\n",
            "Step  220100: train CrossEntropyLoss |  2.84579158\n",
            "Step  220100: eval  CrossEntropyLoss |  3.12146425\n",
            "Step  220100: eval          Accuracy |  0.47826087\n",
            "\n",
            "Step  220200: Ran 100 train steps in 61.73 secs\n",
            "Step  220200: train CrossEntropyLoss |  2.83856988\n",
            "Step  220200: eval  CrossEntropyLoss |  2.86509085\n",
            "Step  220200: eval          Accuracy |  0.53211010\n",
            "\n",
            "Step  220300: Ran 100 train steps in 44.69 secs\n",
            "Step  220300: train CrossEntropyLoss |  2.78737187\n",
            "Step  220300: eval  CrossEntropyLoss |  3.33410168\n",
            "Step  220300: eval          Accuracy |  0.43298972\n",
            "\n",
            "Step  220400: Ran 100 train steps in 45.12 secs\n",
            "Step  220400: train CrossEntropyLoss |  2.82055998\n",
            "Step  220400: eval  CrossEntropyLoss |  2.80942559\n",
            "Step  220400: eval          Accuracy |  0.51813477\n",
            "\n",
            "Step  220500: Ran 100 train steps in 45.39 secs\n",
            "Step  220500: train CrossEntropyLoss |  2.73655415\n",
            "Step  220500: eval  CrossEntropyLoss |  2.32558990\n",
            "Step  220500: eval          Accuracy |  0.66019416\n",
            "\n",
            "Step  220600: Ran 100 train steps in 45.55 secs\n",
            "Step  220600: train CrossEntropyLoss |  2.74180484\n",
            "Step  220600: eval  CrossEntropyLoss |  2.36768651\n",
            "Step  220600: eval          Accuracy |  0.60204083\n",
            "\n",
            "Step  220700: Ran 100 train steps in 45.45 secs\n",
            "Step  220700: train CrossEntropyLoss |  2.69260836\n",
            "Step  220700: eval  CrossEntropyLoss |  2.49303222\n",
            "Step  220700: eval          Accuracy |  0.60344827\n",
            "\n",
            "Step  220800: Ran 100 train steps in 45.20 secs\n",
            "Step  220800: train CrossEntropyLoss |  2.68924403\n",
            "Step  220800: eval  CrossEntropyLoss |  2.45153117\n",
            "Step  220800: eval          Accuracy |  0.60377359\n",
            "\n",
            "Step  220900: Ran 100 train steps in 45.40 secs\n",
            "Step  220900: train CrossEntropyLoss |  2.66575861\n",
            "Step  220900: eval  CrossEntropyLoss |  2.50621080\n",
            "Step  220900: eval          Accuracy |  0.55172414\n",
            "\n",
            "Step  221000: Ran 100 train steps in 45.62 secs\n",
            "Step  221000: train CrossEntropyLoss |  2.65961266\n",
            "Step  221000: eval  CrossEntropyLoss |  2.30576468\n",
            "Step  221000: eval          Accuracy |  0.64406782\n",
            "\n",
            "Step  221100: Ran 100 train steps in 45.65 secs\n",
            "Step  221100: train CrossEntropyLoss |  2.61182237\n",
            "Step  221100: eval  CrossEntropyLoss |  3.74061394\n",
            "Step  221100: eval          Accuracy |  0.41964287\n",
            "\n",
            "Step  221200: Ran 100 train steps in 45.83 secs\n",
            "Step  221200: train CrossEntropyLoss |  2.62804866\n",
            "Step  221200: eval  CrossEntropyLoss |  3.02149606\n",
            "Step  221200: eval          Accuracy |  0.46153846\n",
            "\n",
            "Step  221300: Ran 100 train steps in 45.53 secs\n",
            "Step  221300: train CrossEntropyLoss |  2.61020994\n",
            "Step  221300: eval  CrossEntropyLoss |  2.40438652\n",
            "Step  221300: eval          Accuracy |  0.57446808\n",
            "\n",
            "Step  221400: Ran 100 train steps in 45.70 secs\n",
            "Step  221400: train CrossEntropyLoss |  2.62524033\n",
            "Step  221400: eval  CrossEntropyLoss |  2.94831634\n",
            "Step  221400: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  221500: Ran 100 train steps in 45.64 secs\n",
            "Step  221500: train CrossEntropyLoss |  2.65348697\n",
            "Step  221500: eval  CrossEntropyLoss |  3.17686129\n",
            "Step  221500: eval          Accuracy |  0.49074075\n",
            "\n",
            "Step  221600: Ran 100 train steps in 45.52 secs\n",
            "Step  221600: train CrossEntropyLoss |  2.61024857\n",
            "Step  221600: eval  CrossEntropyLoss |  2.27912831\n",
            "Step  221600: eval          Accuracy |  0.59693879\n",
            "\n",
            "Step  221700: Ran 100 train steps in 45.71 secs\n",
            "Step  221700: train CrossEntropyLoss |  2.61734271\n",
            "Step  221700: eval  CrossEntropyLoss |  2.94923711\n",
            "Step  221700: eval          Accuracy |  0.53389829\n",
            "\n",
            "Step  221800: Ran 100 train steps in 45.78 secs\n",
            "Step  221800: train CrossEntropyLoss |  2.60102797\n",
            "Step  221800: eval  CrossEntropyLoss |  2.29448891\n",
            "Step  221800: eval          Accuracy |  0.65384620\n",
            "\n",
            "Step  221900: Ran 100 train steps in 45.98 secs\n",
            "Step  221900: train CrossEntropyLoss |  2.59703445\n",
            "Step  221900: eval  CrossEntropyLoss |  2.45716071\n",
            "Step  221900: eval          Accuracy |  0.62608695\n",
            "\n",
            "Step  222000: Ran 100 train steps in 45.58 secs\n",
            "Step  222000: train CrossEntropyLoss |  2.53129959\n",
            "Step  222000: eval  CrossEntropyLoss |  2.69815826\n",
            "Step  222000: eval          Accuracy |  0.55263156\n",
            "\n",
            "Step  222100: Ran 100 train steps in 45.68 secs\n",
            "Step  222100: train CrossEntropyLoss |  2.59954381\n",
            "Step  222100: eval  CrossEntropyLoss |  2.75071645\n",
            "Step  222100: eval          Accuracy |  0.48167539\n",
            "\n",
            "Step  222200: Ran 100 train steps in 45.79 secs\n",
            "Step  222200: train CrossEntropyLoss |  2.65178466\n",
            "Step  222200: eval  CrossEntropyLoss |  3.20998263\n",
            "Step  222200: eval          Accuracy |  0.46491230\n",
            "\n",
            "Step  222300: Ran 100 train steps in 45.83 secs\n",
            "Step  222300: train CrossEntropyLoss |  2.67022324\n",
            "Step  222300: eval  CrossEntropyLoss |  2.67207646\n",
            "Step  222300: eval          Accuracy |  0.57009345\n",
            "\n",
            "Step  222400: Ran 100 train steps in 45.80 secs\n",
            "Step  222400: train CrossEntropyLoss |  2.60320115\n",
            "Step  222400: eval  CrossEntropyLoss |  2.70993638\n",
            "Step  222400: eval          Accuracy |  0.52941179\n",
            "\n",
            "Step  222500: Ran 100 train steps in 45.97 secs\n",
            "Step  222500: train CrossEntropyLoss |  2.64060855\n",
            "Step  222500: eval  CrossEntropyLoss |  2.98862028\n",
            "Step  222500: eval          Accuracy |  0.51239669\n",
            "\n",
            "Step  222600: Ran 100 train steps in 46.85 secs\n",
            "Step  222600: train CrossEntropyLoss |  2.61320090\n",
            "Step  222600: eval  CrossEntropyLoss |  2.95661497\n",
            "Step  222600: eval          Accuracy |  0.48148149\n",
            "\n",
            "Step  222700: Ran 100 train steps in 47.94 secs\n",
            "Step  222700: train CrossEntropyLoss |  2.60677505\n",
            "Step  222700: eval  CrossEntropyLoss |  2.40532804\n",
            "Step  222700: eval          Accuracy |  0.58252430\n",
            "\n",
            "Step  222800: Ran 100 train steps in 47.53 secs\n",
            "Step  222800: train CrossEntropyLoss |  2.62979364\n",
            "Step  222800: eval  CrossEntropyLoss |  2.65230107\n",
            "Step  222800: eval          Accuracy |  0.57843137\n",
            "\n",
            "Step  222900: Ran 100 train steps in 47.88 secs\n",
            "Step  222900: train CrossEntropyLoss |  2.62162733\n",
            "Step  222900: eval  CrossEntropyLoss |  2.65803409\n",
            "Step  222900: eval          Accuracy |  0.51456308\n",
            "\n",
            "Step  223000: Ran 100 train steps in 47.75 secs\n",
            "Step  223000: train CrossEntropyLoss |  2.59591937\n",
            "Step  223000: eval  CrossEntropyLoss |  3.06282163\n",
            "Step  223000: eval          Accuracy |  0.43809524\n",
            "\n",
            "Step  223100: Ran 100 train steps in 47.90 secs\n",
            "Step  223100: train CrossEntropyLoss |  2.53805804\n",
            "Step  223100: eval  CrossEntropyLoss |  2.72778797\n",
            "Step  223100: eval          Accuracy |  0.52205884\n",
            "\n",
            "Step  223200: Ran 100 train steps in 47.79 secs\n",
            "Step  223200: train CrossEntropyLoss |  2.63444090\n",
            "Step  223200: eval  CrossEntropyLoss |  2.42899585\n",
            "Step  223200: eval          Accuracy |  0.57999998\n",
            "\n",
            "Step  223300: Ran 100 train steps in 47.68 secs\n",
            "Step  223300: train CrossEntropyLoss |  2.54276848\n",
            "Step  223300: eval  CrossEntropyLoss |  2.69147038\n",
            "Step  223300: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  223400: Ran 100 train steps in 46.76 secs\n",
            "Step  223400: train CrossEntropyLoss |  2.54439640\n",
            "Step  223400: eval  CrossEntropyLoss |  3.16598868\n",
            "Step  223400: eval          Accuracy |  0.47008550\n",
            "\n",
            "Step  223500: Ran 100 train steps in 47.38 secs\n",
            "Step  223500: train CrossEntropyLoss |  2.51358294\n",
            "Step  223500: eval  CrossEntropyLoss |  2.17688370\n",
            "Step  223500: eval          Accuracy |  0.59512198\n",
            "\n",
            "Step  223600: Ran 100 train steps in 47.96 secs\n",
            "Step  223600: train CrossEntropyLoss |  2.64234877\n",
            "Step  223600: eval  CrossEntropyLoss |  2.33703256\n",
            "Step  223600: eval          Accuracy |  0.62616819\n",
            "\n",
            "Step  223700: Ran 100 train steps in 47.64 secs\n",
            "Step  223700: train CrossEntropyLoss |  2.55053353\n",
            "Step  223700: eval  CrossEntropyLoss |  2.49230003\n",
            "Step  223700: eval          Accuracy |  0.49504951\n",
            "\n",
            "Step  223800: Ran 100 train steps in 47.89 secs\n",
            "Step  223800: train CrossEntropyLoss |  2.53601599\n",
            "Step  223800: eval  CrossEntropyLoss |  2.70032001\n",
            "Step  223800: eval          Accuracy |  0.59292036\n",
            "\n",
            "Step  223900: Ran 100 train steps in 47.64 secs\n",
            "Step  223900: train CrossEntropyLoss |  2.52377868\n",
            "Step  223900: eval  CrossEntropyLoss |  2.63125706\n",
            "Step  223900: eval          Accuracy |  0.62037039\n",
            "\n",
            "Step  224000: Ran 100 train steps in 47.64 secs\n",
            "Step  224000: train CrossEntropyLoss |  2.54125547\n",
            "Step  224000: eval  CrossEntropyLoss |  2.53065181\n",
            "Step  224000: eval          Accuracy |  0.58499998\n",
            "\n",
            "Step  224100: Ran 100 train steps in 47.80 secs\n",
            "Step  224100: train CrossEntropyLoss |  2.52150941\n",
            "Step  224100: eval  CrossEntropyLoss |  2.78295660\n",
            "Step  224100: eval          Accuracy |  0.53271025\n",
            "\n",
            "Step  224200: Ran 100 train steps in 47.74 secs\n",
            "Step  224200: train CrossEntropyLoss |  2.59118772\n",
            "Step  224200: eval  CrossEntropyLoss |  2.44015288\n",
            "Step  224200: eval          Accuracy |  0.56310678\n",
            "\n",
            "Step  224300: Ran 100 train steps in 47.70 secs\n",
            "Step  224300: train CrossEntropyLoss |  2.61318851\n",
            "Step  224300: eval  CrossEntropyLoss |  2.57482529\n",
            "Step  224300: eval          Accuracy |  0.55789477\n",
            "\n",
            "Step  224400: Ran 100 train steps in 47.75 secs\n",
            "Step  224400: train CrossEntropyLoss |  2.51921368\n",
            "Step  224400: eval  CrossEntropyLoss |  2.70453787\n",
            "Step  224400: eval          Accuracy |  0.47999999\n",
            "\n",
            "Step  224500: Ran 100 train steps in 47.66 secs\n",
            "Step  224500: train CrossEntropyLoss |  2.48929787\n",
            "Step  224500: eval  CrossEntropyLoss |  2.90698981\n",
            "Step  224500: eval          Accuracy |  0.51794875\n",
            "\n",
            "Step  224600: Ran 100 train steps in 47.74 secs\n",
            "Step  224600: train CrossEntropyLoss |  2.56222582\n",
            "Step  224600: eval  CrossEntropyLoss |  3.03907347\n",
            "Step  224600: eval          Accuracy |  0.50400001\n",
            "\n",
            "Step  224700: Ran 100 train steps in 47.72 secs\n",
            "Step  224700: train CrossEntropyLoss |  2.48772907\n",
            "Step  224700: eval  CrossEntropyLoss |  3.31026816\n",
            "Step  224700: eval          Accuracy |  0.46601942\n",
            "\n",
            "Step  224800: Ran 100 train steps in 47.97 secs\n",
            "Step  224800: train CrossEntropyLoss |  2.50301337\n",
            "Step  224800: eval  CrossEntropyLoss |  2.83109546\n",
            "Step  224800: eval          Accuracy |  0.51388890\n",
            "\n",
            "Step  224900: Ran 100 train steps in 47.68 secs\n",
            "Step  224900: train CrossEntropyLoss |  2.61269331\n",
            "Step  224900: eval  CrossEntropyLoss |  2.20755267\n",
            "Step  224900: eval          Accuracy |  0.58620691\n",
            "\n",
            "Step  225000: Ran 100 train steps in 47.91 secs\n",
            "Step  225000: train CrossEntropyLoss |  2.59651756\n",
            "Step  225000: eval  CrossEntropyLoss |  3.19133544\n",
            "Step  225000: eval          Accuracy |  0.47663549\n",
            "\n",
            "Step  225100: Ran 100 train steps in 47.90 secs\n",
            "Step  225100: train CrossEntropyLoss |  2.56957459\n",
            "Step  225100: eval  CrossEntropyLoss |  2.67007232\n",
            "Step  225100: eval          Accuracy |  0.56390977\n",
            "\n",
            "Step  225200: Ran 100 train steps in 47.81 secs\n",
            "Step  225200: train CrossEntropyLoss |  2.58979917\n",
            "Step  225200: eval  CrossEntropyLoss |  2.99745703\n",
            "Step  225200: eval          Accuracy |  0.51562500\n",
            "\n",
            "Step  225300: Ran 100 train steps in 47.90 secs\n",
            "Step  225300: train CrossEntropyLoss |  2.63558364\n",
            "Step  225300: eval  CrossEntropyLoss |  2.48801327\n",
            "Step  225300: eval          Accuracy |  0.58823532\n",
            "\n",
            "Step  225400: Ran 100 train steps in 47.75 secs\n",
            "Step  225400: train CrossEntropyLoss |  2.55233002\n",
            "Step  225400: eval  CrossEntropyLoss |  2.83933711\n",
            "Step  225400: eval          Accuracy |  0.53211010\n",
            "\n",
            "Step  225500: Ran 100 train steps in 47.68 secs\n",
            "Step  225500: train CrossEntropyLoss |  2.55528355\n",
            "Step  225500: eval  CrossEntropyLoss |  2.76621199\n",
            "Step  225500: eval          Accuracy |  0.53703701\n",
            "\n",
            "Step  225600: Ran 100 train steps in 47.84 secs\n",
            "Step  225600: train CrossEntropyLoss |  2.53882146\n",
            "Step  225600: eval  CrossEntropyLoss |  2.71168900\n",
            "Step  225600: eval          Accuracy |  0.54271358\n",
            "\n",
            "Step  225700: Ran 100 train steps in 47.73 secs\n",
            "Step  225700: train CrossEntropyLoss |  2.59294391\n",
            "Step  225700: eval  CrossEntropyLoss |  3.44785690\n",
            "Step  225700: eval          Accuracy |  0.42857143\n",
            "\n",
            "Step  225800: Ran 100 train steps in 47.96 secs\n",
            "Step  225800: train CrossEntropyLoss |  2.58450198\n",
            "Step  225800: eval  CrossEntropyLoss |  3.30215645\n",
            "Step  225800: eval          Accuracy |  0.42727271\n",
            "\n",
            "Step  225900: Ran 100 train steps in 47.76 secs\n",
            "Step  225900: train CrossEntropyLoss |  2.52417302\n",
            "Step  225900: eval  CrossEntropyLoss |  2.46467328\n",
            "Step  225900: eval          Accuracy |  0.62601632\n",
            "\n",
            "Step  226000: Ran 100 train steps in 47.83 secs\n",
            "Step  226000: train CrossEntropyLoss |  2.52127385\n",
            "Step  226000: eval  CrossEntropyLoss |  2.15634322\n",
            "Step  226000: eval          Accuracy |  0.56216216\n",
            "\n",
            "Step  226100: Ran 100 train steps in 47.71 secs\n",
            "Step  226100: train CrossEntropyLoss |  2.47344470\n",
            "Step  226100: eval  CrossEntropyLoss |  3.15942383\n",
            "Step  226100: eval          Accuracy |  0.50925928\n",
            "\n",
            "Step  226200: Ran 100 train steps in 47.78 secs\n",
            "Step  226200: train CrossEntropyLoss |  2.50462151\n",
            "Step  226200: eval  CrossEntropyLoss |  3.11795068\n",
            "Step  226200: eval          Accuracy |  0.46363634\n",
            "\n",
            "Step  226300: Ran 100 train steps in 47.73 secs\n",
            "Step  226300: train CrossEntropyLoss |  2.57642603\n",
            "Step  226300: eval  CrossEntropyLoss |  2.82784271\n",
            "Step  226300: eval          Accuracy |  0.56132078\n",
            "\n",
            "Step  226400: Ran 100 train steps in 47.89 secs\n",
            "Step  226400: train CrossEntropyLoss |  2.50355077\n",
            "Step  226400: eval  CrossEntropyLoss |  3.14905238\n",
            "Step  226400: eval          Accuracy |  0.49532709\n",
            "\n",
            "Step  226500: Ran 100 train steps in 47.73 secs\n",
            "Step  226500: train CrossEntropyLoss |  2.58313799\n",
            "Step  226500: eval  CrossEntropyLoss |  2.56554842\n",
            "Step  226500: eval          Accuracy |  0.56190479\n",
            "\n",
            "Step  226600: Ran 100 train steps in 47.88 secs\n",
            "Step  226600: train CrossEntropyLoss |  2.49244857\n",
            "Step  226600: eval  CrossEntropyLoss |  3.33655524\n",
            "Step  226600: eval          Accuracy |  0.43589747\n",
            "\n",
            "Step  226700: Ran 100 train steps in 47.64 secs\n",
            "Step  226700: train CrossEntropyLoss |  2.56400895\n",
            "Step  226700: eval  CrossEntropyLoss |  2.23856330\n",
            "Step  226700: eval          Accuracy |  0.57073170\n",
            "\n",
            "Step  226800: Ran 100 train steps in 47.71 secs\n",
            "Step  226800: train CrossEntropyLoss |  2.48359776\n",
            "Step  226800: eval  CrossEntropyLoss |  2.50527072\n",
            "Step  226800: eval          Accuracy |  0.59793818\n",
            "\n",
            "Step  226900: Ran 100 train steps in 48.09 secs\n",
            "Step  226900: train CrossEntropyLoss |  2.51648235\n",
            "Step  226900: eval  CrossEntropyLoss |  2.94637847\n",
            "Step  226900: eval          Accuracy |  0.52631581\n",
            "\n",
            "Step  227000: Ran 100 train steps in 47.66 secs\n",
            "Step  227000: train CrossEntropyLoss |  2.54795122\n",
            "Step  227000: eval  CrossEntropyLoss |  2.86930728\n",
            "Step  227000: eval          Accuracy |  0.49450549\n",
            "\n",
            "Step  227100: Ran 100 train steps in 47.74 secs\n",
            "Step  227100: train CrossEntropyLoss |  2.50609851\n",
            "Step  227100: eval  CrossEntropyLoss |  2.22855854\n",
            "Step  227100: eval          Accuracy |  0.61946905\n",
            "\n",
            "Step  227200: Ran 100 train steps in 47.61 secs\n",
            "Step  227200: train CrossEntropyLoss |  2.52635670\n",
            "Step  227200: eval  CrossEntropyLoss |  2.91773558\n",
            "Step  227200: eval          Accuracy |  0.49514565\n",
            "\n",
            "Step  227300: Ran 100 train steps in 47.79 secs\n",
            "Step  227300: train CrossEntropyLoss |  2.55014706\n",
            "Step  227300: eval  CrossEntropyLoss |  2.44157743\n",
            "Step  227300: eval          Accuracy |  0.53333336\n",
            "\n",
            "Step  227400: Ran 100 train steps in 47.73 secs\n",
            "Step  227400: train CrossEntropyLoss |  2.53875589\n",
            "Step  227400: eval  CrossEntropyLoss |  2.44534469\n",
            "Step  227400: eval          Accuracy |  0.54444444\n",
            "\n",
            "Step  227500: Ran 100 train steps in 47.95 secs\n",
            "Step  227500: train CrossEntropyLoss |  2.47445941\n",
            "Step  227500: eval  CrossEntropyLoss |  2.04543018\n",
            "Step  227500: eval          Accuracy |  0.61083740\n",
            "\n",
            "Step  227600: Ran 100 train steps in 47.81 secs\n",
            "Step  227600: train CrossEntropyLoss |  2.52640414\n",
            "Step  227600: eval  CrossEntropyLoss |  2.22775578\n",
            "Step  227600: eval          Accuracy |  0.61764705\n",
            "\n",
            "Step  227700: Ran 100 train steps in 48.03 secs\n",
            "Step  227700: train CrossEntropyLoss |  2.50211000\n",
            "Step  227700: eval  CrossEntropyLoss |  2.31570077\n",
            "Step  227700: eval          Accuracy |  0.51020408\n",
            "\n",
            "Step  227800: Ran 100 train steps in 47.53 secs\n",
            "Step  227800: train CrossEntropyLoss |  2.52092505\n",
            "Step  227800: eval  CrossEntropyLoss |  2.68893242\n",
            "Step  227800: eval          Accuracy |  0.52195120\n",
            "\n",
            "Step  227900: Ran 100 train steps in 47.61 secs\n",
            "Step  227900: train CrossEntropyLoss |  2.55468106\n",
            "Step  227900: eval  CrossEntropyLoss |  2.05691433\n",
            "Step  227900: eval          Accuracy |  0.57798165\n",
            "\n",
            "Step  228000: Ran 100 train steps in 47.62 secs\n",
            "Step  228000: train CrossEntropyLoss |  2.60344291\n",
            "Step  228000: eval  CrossEntropyLoss |  2.32371879\n",
            "Step  228000: eval          Accuracy |  0.56666672\n",
            "\n",
            "Step  228100: Ran 100 train steps in 47.87 secs\n",
            "Step  228100: train CrossEntropyLoss |  2.55030847\n",
            "Step  228100: eval  CrossEntropyLoss |  3.43359184\n",
            "Step  228100: eval          Accuracy |  0.43564355\n",
            "\n",
            "Step  228200: Ran 100 train steps in 47.58 secs\n",
            "Step  228200: train CrossEntropyLoss |  2.55417705\n",
            "Step  228200: eval  CrossEntropyLoss |  2.32923508\n",
            "Step  228200: eval          Accuracy |  0.59259260\n",
            "\n",
            "Step  228300: Ran 100 train steps in 47.88 secs\n",
            "Step  228300: train CrossEntropyLoss |  2.48686814\n",
            "Step  228300: eval  CrossEntropyLoss |  2.60207748\n",
            "Step  228300: eval          Accuracy |  0.59813082\n",
            "\n",
            "Step  228400: Ran 100 train steps in 47.59 secs\n",
            "Step  228400: train CrossEntropyLoss |  2.48176312\n",
            "Step  228400: eval  CrossEntropyLoss |  2.83330107\n",
            "Step  228400: eval          Accuracy |  0.49074075\n",
            "\n",
            "Step  228500: Ran 100 train steps in 47.78 secs\n",
            "Step  228500: train CrossEntropyLoss |  2.48887372\n",
            "Step  228500: eval  CrossEntropyLoss |  2.77551341\n",
            "Step  228500: eval          Accuracy |  0.56302524\n",
            "\n",
            "Step  228600: Ran 100 train steps in 47.72 secs\n",
            "Step  228600: train CrossEntropyLoss |  2.51788282\n",
            "Step  228600: eval  CrossEntropyLoss |  2.76681852\n",
            "Step  228600: eval          Accuracy |  0.50549453\n",
            "\n",
            "Step  228700: Ran 100 train steps in 47.61 secs\n",
            "Step  228700: train CrossEntropyLoss |  2.50078845\n",
            "Step  228700: eval  CrossEntropyLoss |  2.72214508\n",
            "Step  228700: eval          Accuracy |  0.52830189\n",
            "\n",
            "Step  228800: Ran 100 train steps in 47.74 secs\n",
            "Step  228800: train CrossEntropyLoss |  2.49141741\n",
            "Step  228800: eval  CrossEntropyLoss |  3.23467708\n",
            "Step  228800: eval          Accuracy |  0.42452830\n",
            "\n",
            "Step  228900: Ran 100 train steps in 47.64 secs\n",
            "Step  228900: train CrossEntropyLoss |  2.49295807\n",
            "Step  228900: eval  CrossEntropyLoss |  2.44463348\n",
            "Step  228900: eval          Accuracy |  0.55855858\n",
            "\n",
            "Step  229000: Ran 100 train steps in 47.77 secs\n",
            "Step  229000: train CrossEntropyLoss |  2.47089505\n",
            "Step  229000: eval  CrossEntropyLoss |  2.25001812\n",
            "Step  229000: eval          Accuracy |  0.60000002\n",
            "\n",
            "Step  229100: Ran 100 train steps in 47.59 secs\n",
            "Step  229100: train CrossEntropyLoss |  2.53272843\n",
            "Step  229100: eval  CrossEntropyLoss |  2.45939469\n",
            "Step  229100: eval          Accuracy |  0.56122446\n",
            "\n",
            "Step  229200: Ran 100 train steps in 47.71 secs\n",
            "Step  229200: train CrossEntropyLoss |  2.53549314\n",
            "Step  229200: eval  CrossEntropyLoss |  2.73609066\n",
            "Step  229200: eval          Accuracy |  0.51376146\n",
            "\n",
            "Step  229300: Ran 100 train steps in 47.61 secs\n",
            "Step  229300: train CrossEntropyLoss |  2.56544709\n",
            "Step  229300: eval  CrossEntropyLoss |  2.33600974\n",
            "Step  229300: eval          Accuracy |  0.52525252\n",
            "\n",
            "Step  229400: Ran 100 train steps in 47.51 secs\n",
            "Step  229400: train CrossEntropyLoss |  2.53305984\n",
            "Step  229400: eval  CrossEntropyLoss |  2.53556085\n",
            "Step  229400: eval          Accuracy |  0.56435645\n",
            "\n",
            "Step  229500: Ran 100 train steps in 47.85 secs\n",
            "Step  229500: train CrossEntropyLoss |  2.53249979\n",
            "Step  229500: eval  CrossEntropyLoss |  3.76541615\n",
            "Step  229500: eval          Accuracy |  0.27777779\n",
            "\n",
            "Step  229600: Ran 100 train steps in 48.17 secs\n",
            "Step  229600: train CrossEntropyLoss |  2.49778891\n",
            "Step  229600: eval  CrossEntropyLoss |  1.78679001\n",
            "Step  229600: eval          Accuracy |  0.65957445\n",
            "\n",
            "Step  229700: Ran 100 train steps in 47.71 secs\n",
            "Step  229700: train CrossEntropyLoss |  2.56117702\n",
            "Step  229700: eval  CrossEntropyLoss |  2.15929294\n",
            "Step  229700: eval          Accuracy |  0.60377359\n",
            "\n",
            "Step  229800: Ran 100 train steps in 47.73 secs\n",
            "Step  229800: train CrossEntropyLoss |  2.49239874\n",
            "Step  229800: eval  CrossEntropyLoss |  2.68826771\n",
            "Step  229800: eval          Accuracy |  0.52336448\n",
            "\n",
            "Step  229900: Ran 100 train steps in 47.65 secs\n",
            "Step  229900: train CrossEntropyLoss |  2.57232285\n",
            "Step  229900: eval  CrossEntropyLoss |  2.70251250\n",
            "Step  229900: eval          Accuracy |  0.48623851\n",
            "\n",
            "Step  230000: Ran 100 train steps in 47.87 secs\n",
            "Step  230000: train CrossEntropyLoss |  2.49641776\n",
            "Step  230000: eval  CrossEntropyLoss |  2.76209140\n",
            "Step  230000: eval          Accuracy |  0.52336448\n",
            "\n",
            "Step  230100: Ran 100 train steps in 47.80 secs\n",
            "Step  230100: train CrossEntropyLoss |  2.52030826\n",
            "Step  230100: eval  CrossEntropyLoss |  3.00646996\n",
            "Step  230100: eval          Accuracy |  0.50505048\n",
            "\n",
            "Step  230200: Ran 100 train steps in 47.88 secs\n",
            "Step  230200: train CrossEntropyLoss |  2.48940110\n",
            "Step  230200: eval  CrossEntropyLoss |  3.22322416\n",
            "Step  230200: eval          Accuracy |  0.42201832\n",
            "\n",
            "Step  230300: Ran 100 train steps in 47.77 secs\n",
            "Step  230300: train CrossEntropyLoss |  2.47108364\n",
            "Step  230300: eval  CrossEntropyLoss |  2.61924696\n",
            "Step  230300: eval          Accuracy |  0.51485145\n",
            "\n",
            "Step  230400: Ran 100 train steps in 47.72 secs\n",
            "Step  230400: train CrossEntropyLoss |  2.49256420\n",
            "Step  230400: eval  CrossEntropyLoss |  2.78146768\n",
            "Step  230400: eval          Accuracy |  0.49246231\n",
            "\n",
            "Step  230500: Ran 100 train steps in 47.63 secs\n",
            "Step  230500: train CrossEntropyLoss |  2.47835159\n",
            "Step  230500: eval  CrossEntropyLoss |  3.03568816\n",
            "Step  230500: eval          Accuracy |  0.44247788\n",
            "\n",
            "Step  230600: Ran 100 train steps in 47.75 secs\n",
            "Step  230600: train CrossEntropyLoss |  2.47238111\n",
            "Step  230600: eval  CrossEntropyLoss |  3.01094079\n",
            "Step  230600: eval          Accuracy |  0.45370370\n",
            "\n",
            "Step  230700: Ran 100 train steps in 47.82 secs\n",
            "Step  230700: train CrossEntropyLoss |  2.46019149\n",
            "Step  230700: eval  CrossEntropyLoss |  2.66370440\n",
            "Step  230700: eval          Accuracy |  0.56000000\n",
            "\n",
            "Step  230800: Ran 100 train steps in 47.74 secs\n",
            "Step  230800: train CrossEntropyLoss |  2.55576134\n",
            "Step  230800: eval  CrossEntropyLoss |  2.76060414\n",
            "Step  230800: eval          Accuracy |  0.54500002\n",
            "\n",
            "Step  230900: Ran 100 train steps in 47.68 secs\n",
            "Step  230900: train CrossEntropyLoss |  2.53240800\n",
            "Step  230900: eval  CrossEntropyLoss |  2.21193266\n",
            "Step  230900: eval          Accuracy |  0.61983466\n",
            "\n",
            "Step  231000: Ran 100 train steps in 47.78 secs\n",
            "Step  231000: train CrossEntropyLoss |  2.49056625\n",
            "Step  231000: eval  CrossEntropyLoss |  2.87262273\n",
            "Step  231000: eval          Accuracy |  0.49438202\n",
            "\n",
            "Step  231100: Ran 100 train steps in 47.46 secs\n",
            "Step  231100: train CrossEntropyLoss |  2.51571012\n",
            "Step  231100: eval  CrossEntropyLoss |  2.58238745\n",
            "Step  231100: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  231200: Ran 100 train steps in 48.08 secs\n",
            "Step  231200: train CrossEntropyLoss |  2.49925160\n",
            "Step  231200: eval  CrossEntropyLoss |  2.24070978\n",
            "Step  231200: eval          Accuracy |  0.59907836\n",
            "\n",
            "Step  231300: Ran 100 train steps in 47.73 secs\n",
            "Step  231300: train CrossEntropyLoss |  2.56042719\n",
            "Step  231300: eval  CrossEntropyLoss |  2.71040058\n",
            "Step  231300: eval          Accuracy |  0.55357146\n",
            "\n",
            "Step  231400: Ran 100 train steps in 47.42 secs\n",
            "Step  231400: train CrossEntropyLoss |  2.48312998\n",
            "Step  231400: eval  CrossEntropyLoss |  2.77557707\n",
            "Step  231400: eval          Accuracy |  0.49572653\n",
            "\n",
            "Step  231500: Ran 100 train steps in 47.86 secs\n",
            "Step  231500: train CrossEntropyLoss |  2.47705674\n",
            "Step  231500: eval  CrossEntropyLoss |  2.68887901\n",
            "Step  231500: eval          Accuracy |  0.51724136\n",
            "\n",
            "Step  231600: Ran 100 train steps in 47.67 secs\n",
            "Step  231600: train CrossEntropyLoss |  2.49272084\n",
            "Step  231600: eval  CrossEntropyLoss |  2.73897600\n",
            "Step  231600: eval          Accuracy |  0.55045867\n",
            "\n",
            "Step  231700: Ran 100 train steps in 47.94 secs\n",
            "Step  231700: train CrossEntropyLoss |  2.47309256\n",
            "Step  231700: eval  CrossEntropyLoss |  2.25719929\n",
            "Step  231700: eval          Accuracy |  0.60000002\n",
            "\n",
            "Step  231800: Ran 100 train steps in 47.62 secs\n",
            "Step  231800: train CrossEntropyLoss |  2.56405854\n",
            "Step  231800: eval  CrossEntropyLoss |  2.69843388\n",
            "Step  231800: eval          Accuracy |  0.56768560\n",
            "\n",
            "Step  231900: Ran 100 train steps in 47.63 secs\n",
            "Step  231900: train CrossEntropyLoss |  2.46049833\n",
            "Step  231900: eval  CrossEntropyLoss |  3.72674060\n",
            "Step  231900: eval          Accuracy |  0.48039219\n",
            "\n",
            "Step  232000: Ran 100 train steps in 47.72 secs\n",
            "Step  232000: train CrossEntropyLoss |  2.44604182\n",
            "Step  232000: eval  CrossEntropyLoss |  2.56479216\n",
            "Step  232000: eval          Accuracy |  0.56521738\n",
            "\n",
            "Step  232100: Ran 100 train steps in 47.71 secs\n",
            "Step  232100: train CrossEntropyLoss |  2.46892691\n",
            "Step  232100: eval  CrossEntropyLoss |  2.73522282\n",
            "Step  232100: eval          Accuracy |  0.52999997\n",
            "\n",
            "Step  232200: Ran 100 train steps in 47.88 secs\n",
            "Step  232200: train CrossEntropyLoss |  2.49883604\n",
            "Step  232200: eval  CrossEntropyLoss |  3.07881236\n",
            "Step  232200: eval          Accuracy |  0.46551725\n",
            "\n",
            "Step  232300: Ran 100 train steps in 47.82 secs\n",
            "Step  232300: train CrossEntropyLoss |  2.48148370\n",
            "Step  232300: eval  CrossEntropyLoss |  2.98960757\n",
            "Step  232300: eval          Accuracy |  0.48019803\n",
            "\n",
            "Step  232400: Ran 100 train steps in 47.87 secs\n",
            "Step  232400: train CrossEntropyLoss |  2.51629186\n",
            "Step  232400: eval  CrossEntropyLoss |  2.37401867\n",
            "Step  232400: eval          Accuracy |  0.61363637\n",
            "\n",
            "Step  232500: Ran 100 train steps in 47.74 secs\n",
            "Step  232500: train CrossEntropyLoss |  2.52440524\n",
            "Step  232500: eval  CrossEntropyLoss |  2.80257344\n",
            "Step  232500: eval          Accuracy |  0.50400001\n",
            "\n",
            "Step  232600: Ran 100 train steps in 47.84 secs\n",
            "Step  232600: train CrossEntropyLoss |  2.47377944\n",
            "Step  232600: eval  CrossEntropyLoss |  3.64340544\n",
            "Step  232600: eval          Accuracy |  0.41304350\n",
            "\n",
            "Step  232700: Ran 100 train steps in 47.86 secs\n",
            "Step  232700: train CrossEntropyLoss |  2.44880438\n",
            "Step  232700: eval  CrossEntropyLoss |  2.08271766\n",
            "Step  232700: eval          Accuracy |  0.57286429\n",
            "\n",
            "Step  232800: Ran 100 train steps in 47.72 secs\n",
            "Step  232800: train CrossEntropyLoss |  2.50659323\n",
            "Step  232800: eval  CrossEntropyLoss |  2.23423696\n",
            "Step  232800: eval          Accuracy |  0.63114756\n",
            "\n",
            "Step  232900: Ran 100 train steps in 47.68 secs\n",
            "Step  232900: train CrossEntropyLoss |  2.58959031\n",
            "Step  232900: eval  CrossEntropyLoss |  2.96339798\n",
            "Step  232900: eval          Accuracy |  0.45600003\n",
            "\n",
            "Step  233000: Ran 100 train steps in 47.85 secs\n",
            "Step  233000: train CrossEntropyLoss |  2.46876884\n",
            "Step  233000: eval  CrossEntropyLoss |  2.59775066\n",
            "Step  233000: eval          Accuracy |  0.57541901\n",
            "\n",
            "Step  233100: Ran 100 train steps in 47.57 secs\n",
            "Step  233100: train CrossEntropyLoss |  2.44819260\n",
            "Step  233100: eval  CrossEntropyLoss |  2.86430860\n",
            "Step  233100: eval          Accuracy |  0.49572653\n",
            "\n",
            "Step  233200: Ran 100 train steps in 47.86 secs\n",
            "Step  233200: train CrossEntropyLoss |  2.50873709\n",
            "Step  233200: eval  CrossEntropyLoss |  2.20872855\n",
            "Step  233200: eval          Accuracy |  0.53061223\n",
            "\n",
            "Step  233300: Ran 100 train steps in 47.45 secs\n",
            "Step  233300: train CrossEntropyLoss |  2.50736642\n",
            "Step  233300: eval  CrossEntropyLoss |  2.55942822\n",
            "Step  233300: eval          Accuracy |  0.57281554\n",
            "\n",
            "Step  233400: Ran 100 train steps in 47.80 secs\n",
            "Step  233400: train CrossEntropyLoss |  2.50623536\n",
            "Step  233400: eval  CrossEntropyLoss |  2.94494390\n",
            "Step  233400: eval          Accuracy |  0.50241548\n",
            "\n",
            "Step  233500: Ran 100 train steps in 47.72 secs\n",
            "Step  233500: train CrossEntropyLoss |  2.45500779\n",
            "Step  233500: eval  CrossEntropyLoss |  2.17723465\n",
            "Step  233500: eval          Accuracy |  0.56000000\n",
            "\n",
            "Step  233600: Ran 100 train steps in 47.61 secs\n",
            "Step  233600: train CrossEntropyLoss |  2.49643564\n",
            "Step  233600: eval  CrossEntropyLoss |  2.49248481\n",
            "Step  233600: eval          Accuracy |  0.59090906\n",
            "\n",
            "Step  233700: Ran 100 train steps in 47.53 secs\n",
            "Step  233700: train CrossEntropyLoss |  2.44598246\n",
            "Step  233700: eval  CrossEntropyLoss |  3.13754463\n",
            "Step  233700: eval          Accuracy |  0.48717952\n",
            "\n",
            "Step  233800: Ran 100 train steps in 47.53 secs\n",
            "Step  233800: train CrossEntropyLoss |  2.51649475\n",
            "Step  233800: eval  CrossEntropyLoss |  2.65836978\n",
            "Step  233800: eval          Accuracy |  0.48717949\n",
            "\n",
            "Step  233900: Ran 100 train steps in 47.84 secs\n",
            "Step  233900: train CrossEntropyLoss |  2.50958252\n",
            "Step  233900: eval  CrossEntropyLoss |  3.17803478\n",
            "Step  233900: eval          Accuracy |  0.44339624\n",
            "\n",
            "Step  234000: Ran 100 train steps in 47.62 secs\n",
            "Step  234000: train CrossEntropyLoss |  2.46189237\n",
            "Step  234000: eval  CrossEntropyLoss |  2.85300159\n",
            "Step  234000: eval          Accuracy |  0.54368931\n",
            "\n",
            "Step  234100: Ran 100 train steps in 47.68 secs\n",
            "Step  234100: train CrossEntropyLoss |  2.51642346\n",
            "Step  234100: eval  CrossEntropyLoss |  2.02160931\n",
            "Step  234100: eval          Accuracy |  0.64864868\n",
            "\n",
            "Step  234200: Ran 100 train steps in 47.42 secs\n",
            "Step  234200: train CrossEntropyLoss |  2.47895408\n",
            "Step  234200: eval  CrossEntropyLoss |  2.78281736\n",
            "Step  234200: eval          Accuracy |  0.45226130\n",
            "\n",
            "Step  234300: Ran 100 train steps in 47.67 secs\n",
            "Step  234300: train CrossEntropyLoss |  2.47195482\n",
            "Step  234300: eval  CrossEntropyLoss |  2.07597852\n",
            "Step  234300: eval          Accuracy |  0.60747659\n",
            "\n",
            "Step  234400: Ran 100 train steps in 47.58 secs\n",
            "Step  234400: train CrossEntropyLoss |  2.50034475\n",
            "Step  234400: eval  CrossEntropyLoss |  2.51557732\n",
            "Step  234400: eval          Accuracy |  0.52631581\n",
            "\n",
            "Step  234500: Ran 100 train steps in 47.76 secs\n",
            "Step  234500: train CrossEntropyLoss |  2.56394148\n",
            "Step  234500: eval  CrossEntropyLoss |  2.63266182\n",
            "Step  234500: eval          Accuracy |  0.55778897\n",
            "\n",
            "Step  234600: Ran 100 train steps in 47.75 secs\n",
            "Step  234600: train CrossEntropyLoss |  2.56374693\n",
            "Step  234600: eval  CrossEntropyLoss |  3.00646782\n",
            "Step  234600: eval          Accuracy |  0.49038464\n",
            "\n",
            "Step  234700: Ran 100 train steps in 47.37 secs\n",
            "Step  234700: train CrossEntropyLoss |  2.48585057\n",
            "Step  234700: eval  CrossEntropyLoss |  2.93769431\n",
            "Step  234700: eval          Accuracy |  0.48245615\n",
            "\n",
            "Step  234800: Ran 100 train steps in 47.69 secs\n",
            "Step  234800: train CrossEntropyLoss |  2.51269794\n",
            "Step  234800: eval  CrossEntropyLoss |  2.59291673\n",
            "Step  234800: eval          Accuracy |  0.57894742\n",
            "\n",
            "Step  234900: Ran 100 train steps in 47.71 secs\n",
            "Step  234900: train CrossEntropyLoss |  2.47472715\n",
            "Step  234900: eval  CrossEntropyLoss |  2.51688218\n",
            "Step  234900: eval          Accuracy |  0.54385966\n",
            "\n",
            "Step  235000: Ran 100 train steps in 47.66 secs\n",
            "Step  235000: train CrossEntropyLoss |  2.46540117\n",
            "Step  235000: eval  CrossEntropyLoss |  2.22217321\n",
            "Step  235000: eval          Accuracy |  0.53211010\n",
            "\n",
            "Step  235100: Ran 100 train steps in 47.78 secs\n",
            "Step  235100: train CrossEntropyLoss |  2.54567170\n",
            "Step  235100: eval  CrossEntropyLoss |  2.52182508\n",
            "Step  235100: eval          Accuracy |  0.58706468\n",
            "\n",
            "Step  235200: Ran 100 train steps in 47.77 secs\n",
            "Step  235200: train CrossEntropyLoss |  2.47094488\n",
            "Step  235200: eval  CrossEntropyLoss |  2.27537775\n",
            "Step  235200: eval          Accuracy |  0.58653849\n",
            "\n",
            "Step  235300: Ran 100 train steps in 47.74 secs\n",
            "Step  235300: train CrossEntropyLoss |  2.53379512\n",
            "Step  235300: eval  CrossEntropyLoss |  3.35213184\n",
            "Step  235300: eval          Accuracy |  0.42105263\n",
            "\n",
            "Step  235400: Ran 100 train steps in 47.68 secs\n",
            "Step  235400: train CrossEntropyLoss |  2.46776342\n",
            "Step  235400: eval  CrossEntropyLoss |  2.28969502\n",
            "Step  235400: eval          Accuracy |  0.59788364\n",
            "\n",
            "Step  235500: Ran 100 train steps in 47.76 secs\n",
            "Step  235500: train CrossEntropyLoss |  2.51614451\n",
            "Step  235500: eval  CrossEntropyLoss |  2.07904387\n",
            "Step  235500: eval          Accuracy |  0.62365592\n",
            "\n",
            "Step  235600: Ran 100 train steps in 47.69 secs\n",
            "Step  235600: train CrossEntropyLoss |  2.51084638\n",
            "Step  235600: eval  CrossEntropyLoss |  2.43343377\n",
            "Step  235600: eval          Accuracy |  0.60526317\n",
            "\n",
            "Step  235700: Ran 100 train steps in 47.71 secs\n",
            "Step  235700: train CrossEntropyLoss |  2.43443489\n",
            "Step  235700: eval  CrossEntropyLoss |  2.97461367\n",
            "Step  235700: eval          Accuracy |  0.52727270\n",
            "\n",
            "Step  235800: Ran 100 train steps in 47.62 secs\n",
            "Step  235800: train CrossEntropyLoss |  2.42636204\n",
            "Step  235800: eval  CrossEntropyLoss |  2.24277973\n",
            "Step  235800: eval          Accuracy |  0.56880730\n",
            "\n",
            "Step  235900: Ran 100 train steps in 47.59 secs\n",
            "Step  235900: train CrossEntropyLoss |  2.54233432\n",
            "Step  235900: eval  CrossEntropyLoss |  2.94714117\n",
            "Step  235900: eval          Accuracy |  0.47572815\n",
            "\n",
            "Step  236000: Ran 100 train steps in 47.70 secs\n",
            "Step  236000: train CrossEntropyLoss |  2.45726776\n",
            "Step  236000: eval  CrossEntropyLoss |  2.39969492\n",
            "Step  236000: eval          Accuracy |  0.57988167\n",
            "\n",
            "Step  236100: Ran 100 train steps in 48.01 secs\n",
            "Step  236100: train CrossEntropyLoss |  2.49353504\n",
            "Step  236100: eval  CrossEntropyLoss |  2.39546752\n",
            "Step  236100: eval          Accuracy |  0.60606062\n",
            "\n",
            "Step  236200: Ran 100 train steps in 47.87 secs\n",
            "Step  236200: train CrossEntropyLoss |  2.49408984\n",
            "Step  236200: eval  CrossEntropyLoss |  2.99920964\n",
            "Step  236200: eval          Accuracy |  0.55660379\n",
            "\n",
            "Step  236300: Ran 100 train steps in 47.49 secs\n",
            "Step  236300: train CrossEntropyLoss |  2.49305868\n",
            "Step  236300: eval  CrossEntropyLoss |  2.45417547\n",
            "Step  236300: eval          Accuracy |  0.51764709\n",
            "\n",
            "Step  236400: Ran 100 train steps in 47.88 secs\n",
            "Step  236400: train CrossEntropyLoss |  2.49594712\n",
            "Step  236400: eval  CrossEntropyLoss |  2.64256263\n",
            "Step  236400: eval          Accuracy |  0.54838711\n",
            "\n",
            "Step  236500: Ran 100 train steps in 47.44 secs\n",
            "Step  236500: train CrossEntropyLoss |  2.48460531\n",
            "Step  236500: eval  CrossEntropyLoss |  2.73694086\n",
            "Step  236500: eval          Accuracy |  0.57425743\n",
            "\n",
            "Step  236600: Ran 100 train steps in 47.56 secs\n",
            "Step  236600: train CrossEntropyLoss |  2.51138258\n",
            "Step  236600: eval  CrossEntropyLoss |  2.15452528\n",
            "Step  236600: eval          Accuracy |  0.56896549\n",
            "\n",
            "Step  236700: Ran 100 train steps in 47.45 secs\n",
            "Step  236700: train CrossEntropyLoss |  2.47145462\n",
            "Step  236700: eval  CrossEntropyLoss |  3.22273469\n",
            "Step  236700: eval          Accuracy |  0.46153849\n",
            "\n",
            "Step  236800: Ran 100 train steps in 47.64 secs\n",
            "Step  236800: train CrossEntropyLoss |  2.51190233\n",
            "Step  236800: eval  CrossEntropyLoss |  2.09560490\n",
            "Step  236800: eval          Accuracy |  0.62037039\n",
            "\n",
            "Step  236900: Ran 100 train steps in 47.63 secs\n",
            "Step  236900: train CrossEntropyLoss |  2.49219608\n",
            "Step  236900: eval  CrossEntropyLoss |  2.39836860\n",
            "Step  236900: eval          Accuracy |  0.58201063\n",
            "\n",
            "Step  237000: Ran 100 train steps in 47.63 secs\n",
            "Step  237000: train CrossEntropyLoss |  2.48629308\n",
            "Step  237000: eval  CrossEntropyLoss |  2.59080768\n",
            "Step  237000: eval          Accuracy |  0.52845532\n",
            "\n",
            "Step  237100: Ran 100 train steps in 47.63 secs\n",
            "Step  237100: train CrossEntropyLoss |  2.48962665\n",
            "Step  237100: eval  CrossEntropyLoss |  2.61217713\n",
            "Step  237100: eval          Accuracy |  0.58558559\n",
            "\n",
            "Step  237200: Ran 100 train steps in 47.57 secs\n",
            "Step  237200: train CrossEntropyLoss |  2.52303052\n",
            "Step  237200: eval  CrossEntropyLoss |  2.21540260\n",
            "Step  237200: eval          Accuracy |  0.61864406\n",
            "\n",
            "Step  237300: Ran 100 train steps in 47.56 secs\n",
            "Step  237300: train CrossEntropyLoss |  2.51851654\n",
            "Step  237300: eval  CrossEntropyLoss |  2.76471138\n",
            "Step  237300: eval          Accuracy |  0.56989247\n",
            "\n",
            "Step  237400: Ran 100 train steps in 47.65 secs\n",
            "Step  237400: train CrossEntropyLoss |  2.42576337\n",
            "Step  237400: eval  CrossEntropyLoss |  2.76152563\n",
            "Step  237400: eval          Accuracy |  0.52216750\n",
            "\n",
            "Step  237500: Ran 100 train steps in 47.46 secs\n",
            "Step  237500: train CrossEntropyLoss |  2.50232697\n",
            "Step  237500: eval  CrossEntropyLoss |  2.40663934\n",
            "Step  237500: eval          Accuracy |  0.53333336\n",
            "\n",
            "Step  237600: Ran 100 train steps in 47.64 secs\n",
            "Step  237600: train CrossEntropyLoss |  2.46007037\n",
            "Step  237600: eval  CrossEntropyLoss |  3.44976950\n",
            "Step  237600: eval          Accuracy |  0.42391306\n",
            "\n",
            "Step  237700: Ran 100 train steps in 47.42 secs\n",
            "Step  237700: train CrossEntropyLoss |  2.47837496\n",
            "Step  237700: eval  CrossEntropyLoss |  2.98760295\n",
            "Step  237700: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  237800: Ran 100 train steps in 47.46 secs\n",
            "Step  237800: train CrossEntropyLoss |  2.48719192\n",
            "Step  237800: eval  CrossEntropyLoss |  2.72281361\n",
            "Step  237800: eval          Accuracy |  0.50595242\n",
            "\n",
            "Step  237900: Ran 100 train steps in 47.56 secs\n",
            "Step  237900: train CrossEntropyLoss |  2.49127936\n",
            "Step  237900: eval  CrossEntropyLoss |  3.01699066\n",
            "Step  237900: eval          Accuracy |  0.52475244\n",
            "\n",
            "Step  238000: Ran 100 train steps in 47.59 secs\n",
            "Step  238000: train CrossEntropyLoss |  2.54551649\n",
            "Step  238000: eval  CrossEntropyLoss |  2.83501410\n",
            "Step  238000: eval          Accuracy |  0.50980395\n",
            "\n",
            "Step  238100: Ran 100 train steps in 47.55 secs\n",
            "Step  238100: train CrossEntropyLoss |  2.50807333\n",
            "Step  238100: eval  CrossEntropyLoss |  2.41193533\n",
            "Step  238100: eval          Accuracy |  0.60294122\n",
            "\n",
            "Step  238200: Ran 100 train steps in 47.45 secs\n",
            "Step  238200: train CrossEntropyLoss |  2.53716111\n",
            "Step  238200: eval  CrossEntropyLoss |  3.02472925\n",
            "Step  238200: eval          Accuracy |  0.44554454\n",
            "\n",
            "Step  238300: Ran 100 train steps in 47.63 secs\n",
            "Step  238300: train CrossEntropyLoss |  2.47278762\n",
            "Step  238300: eval  CrossEntropyLoss |  2.52298093\n",
            "Step  238300: eval          Accuracy |  0.56250000\n",
            "\n",
            "Step  238400: Ran 100 train steps in 47.75 secs\n",
            "Step  238400: train CrossEntropyLoss |  2.46838307\n",
            "Step  238400: eval  CrossEntropyLoss |  2.28695321\n",
            "Step  238400: eval          Accuracy |  0.57216501\n",
            "\n",
            "Step  238500: Ran 100 train steps in 47.60 secs\n",
            "Step  238500: train CrossEntropyLoss |  2.43367791\n",
            "Step  238500: eval  CrossEntropyLoss |  3.34881926\n",
            "Step  238500: eval          Accuracy |  0.45614034\n",
            "\n",
            "Step  238600: Ran 100 train steps in 47.53 secs\n",
            "Step  238600: train CrossEntropyLoss |  2.48613453\n",
            "Step  238600: eval  CrossEntropyLoss |  2.54180813\n",
            "Step  238600: eval          Accuracy |  0.57142860\n",
            "\n",
            "Step  238700: Ran 100 train steps in 47.53 secs\n",
            "Step  238700: train CrossEntropyLoss |  2.50423551\n",
            "Step  238700: eval  CrossEntropyLoss |  2.54401898\n",
            "Step  238700: eval          Accuracy |  0.54950494\n",
            "\n",
            "Step  238800: Ran 100 train steps in 47.60 secs\n",
            "Step  238800: train CrossEntropyLoss |  2.48061538\n",
            "Step  238800: eval  CrossEntropyLoss |  2.39964008\n",
            "Step  238800: eval          Accuracy |  0.54838705\n",
            "\n",
            "Step  238900: Ran 100 train steps in 47.40 secs\n",
            "Step  238900: train CrossEntropyLoss |  2.44104886\n",
            "Step  238900: eval  CrossEntropyLoss |  3.32429981\n",
            "Step  238900: eval          Accuracy |  0.45535716\n",
            "\n",
            "Step  239000: Ran 100 train steps in 47.35 secs\n",
            "Step  239000: train CrossEntropyLoss |  2.50789547\n",
            "Step  239000: eval  CrossEntropyLoss |  2.31842542\n",
            "Step  239000: eval          Accuracy |  0.53999996\n",
            "\n",
            "Step  239100: Ran 100 train steps in 47.26 secs\n",
            "Step  239100: train CrossEntropyLoss |  2.49379897\n",
            "Step  239100: eval  CrossEntropyLoss |  2.98350024\n",
            "Step  239100: eval          Accuracy |  0.46354169\n",
            "\n",
            "Step  239200: Ran 100 train steps in 47.34 secs\n",
            "Step  239200: train CrossEntropyLoss |  2.47083259\n",
            "Step  239200: eval  CrossEntropyLoss |  2.97881246\n",
            "Step  239200: eval          Accuracy |  0.50925928\n",
            "\n",
            "Step  239300: Ran 100 train steps in 47.35 secs\n",
            "Step  239300: train CrossEntropyLoss |  2.42673302\n",
            "Step  239300: eval  CrossEntropyLoss |  3.29151988\n",
            "Step  239300: eval          Accuracy |  0.48181817\n",
            "\n",
            "Step  239400: Ran 100 train steps in 47.50 secs\n",
            "Step  239400: train CrossEntropyLoss |  2.49573159\n",
            "Step  239400: eval  CrossEntropyLoss |  2.68897605\n",
            "Step  239400: eval          Accuracy |  0.53535354\n",
            "\n",
            "Step  239500: Ran 100 train steps in 47.56 secs\n",
            "Step  239500: train CrossEntropyLoss |  2.45630288\n",
            "Step  239500: eval  CrossEntropyLoss |  2.52605057\n",
            "Step  239500: eval          Accuracy |  0.58653849\n",
            "\n",
            "Step  239600: Ran 100 train steps in 47.54 secs\n",
            "Step  239600: train CrossEntropyLoss |  2.50682712\n",
            "Step  239600: eval  CrossEntropyLoss |  2.14841413\n",
            "Step  239600: eval          Accuracy |  0.60317463\n",
            "\n",
            "Step  239700: Ran 100 train steps in 47.51 secs\n",
            "Step  239700: train CrossEntropyLoss |  2.46065855\n",
            "Step  239700: eval  CrossEntropyLoss |  2.83145452\n",
            "Step  239700: eval          Accuracy |  0.47115386\n",
            "\n",
            "Step  239800: Ran 100 train steps in 47.45 secs\n",
            "Step  239800: train CrossEntropyLoss |  2.46656513\n",
            "Step  239800: eval  CrossEntropyLoss |  3.17068291\n",
            "Step  239800: eval          Accuracy |  0.45283020\n",
            "\n",
            "Step  239900: Ran 100 train steps in 47.45 secs\n",
            "Step  239900: train CrossEntropyLoss |  2.49835587\n",
            "Step  239900: eval  CrossEntropyLoss |  2.28527427\n",
            "Step  239900: eval          Accuracy |  0.62376237\n",
            "\n",
            "Step  240000: Ran 100 train steps in 47.77 secs\n",
            "Step  240000: train CrossEntropyLoss |  2.47300744\n",
            "Step  240000: eval  CrossEntropyLoss |  1.89372170\n",
            "Step  240000: eval          Accuracy |  0.62745100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f17138-59f8-4fdf-b17e-d65cb1538199"
      },
      "source": [
        "eval_article1 = eval_text_pairs[6][0]\r\n",
        "eval_summary1 = eval_text_pairs[6][1]\r\n",
        "print(wrapper.fill(eval_article1))\r\n",
        "print('')\r\n",
        "eval_article2 = eval_text_pairs[999][0]\r\n",
        "eval_summary2 = eval_text_pairs[999][1]\r\n",
        "print(wrapper.fill(eval_article2))\r\n",
        "print('')\r\n",
        "eval_article3 = eval_text_pairs[9][0]\r\n",
        "eval_summary3 = eval_text_pairs[9][1]\r\n",
        "print(wrapper.fill(eval_article3))\r\n",
        "print('')\r\n",
        "eval_article4 = eval_text_pairs[13][0]\r\n",
        "eval_summary4 = eval_text_pairs[13][1]\r\n",
        "print(wrapper.fill(eval_article4))\r\n",
        "print('')\r\n",
        "eval_article5 = eval_text_pairs[116][0]\r\n",
        "eval_summary5 = eval_text_pairs[116][1]\r\n",
        "print(wrapper.fill(eval_article5))\r\n",
        "print('')\r\n",
        "eval_article6 = eval_text_pairs[1113][0]\r\n",
        "eval_summary6 = eval_text_pairs[1113][1]\r\n",
        "print(wrapper.fill(eval_article6))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "швейцарская часовая компания audemars piguet представила новую модель\n",
            "из коллекции royal oak. как сообщает luxurylaunches, речь идет о часах\n",
            "с вечным календарем. официальная презентация пройдет в рамках\n",
            "международного салона высокого часового искусства sihh, который\n",
            "проходит в женеве. часы выполнены из черной керамики с механизмом\n",
            "калибра 5134. примерная стоимость часов составляет порядка 85 тысяч\n",
            "долларов. audemars piguet специализируется на производстве люксовых\n",
            "часов. компания была основана в 1875 году, ее штаб-квартира\n",
            "располагается в женеве. с 1999 года фирма является официальным\n",
            "спонсором скачек queen elizabeth ii cup, а также команды alinghi\n",
            "sailing team по парусному спорту. среди знаменитостей, у кого есть\n",
            "часы бренда: футболисты лионель месси и криштиану роналду, гонщики\n",
            "михаэль шумахер и ярно трулли, а также актеры арнольд шварценеггер и\n",
            "хью джекман.\n",
            "\n",
            "международный валютный фонд (мвф) в среду, 15 мая, утвердил выделение\n",
            "кипру кредита в размере 1,33 миллиарда долларов (миллиард евро). как\n",
            "сообщает agence france-presse, в качестве первого транша кипрское\n",
            "правительство получит 110,7 миллиона долларов. утвержденный 15 мая\n",
            "кредит является частью плана помощи экономике кипра, разработанного\n",
            "мвф и европейским стабилизационным механизмом (esm). общий размер\n",
            "помощи составляет 13 миллиардов долларов (10 миллиардов евро), которые\n",
            "будут направлены на стабилизацию банковского сектора острова в течение\n",
            "трех лет. первый транш помощи от esm в размере трех миллиардов евро\n",
            "был одобрен 13 мая. тогда же были перечислены первые два миллиарда.\n",
            "оставшийся миллиард будет перечислен до 30 июня. банковский сектор\n",
            "кипра пострадал из-за обесценивания греческих гособлигаций. ес\n",
            "согласился выделить кредиты на спасение кипрских банков, при условии,\n",
            "что правительство острова сможет самостоятельно найти 5-7 миллиардов\n",
            "евро. в попытках найти эти деньги власти кипра решили списать\n",
            "единовременный налог с банковских счетов, однако соответствующий\n",
            "законопроект был отклонен парламентом. тогда кипр решил провести\n",
            "реструктуризацию банковского сектора. ожидается, что в результате\n",
            "этого вкладчики двух крупнейших кипрских банков  — bank of cyprus и\n",
            "laiki — потеряют большую часть своих сбережений.\n",
            "\n",
            "на ежегодном фестивале в городе грэхэмстаун, юар, фокусник случайно\n",
            "выстрелил в голову своему напарнику во время представления. об этом\n",
            "сообщает местная газета the daily dispatch. инцидент произошел 30\n",
            "июня. брендон пил (brendon peel) и его ассистент ли лау (li lau)\n",
            "выполняли магический трюк перед многочисленной аудиторией, когда пил\n",
            "по неосторожности пустил в затылок напарника стрелу. директор\n",
            "фестиваля тони ланкестер (tony lankester) отметил, что перепуганных\n",
            "зрителей немедленно эвакуировали. лау доставили в больницу, где ему\n",
            "оперативно обработали раны. по словам ланкестера, стрела не пробила\n",
            "череп фокусника, он находится в сознании и восстанавливается.\n",
            "организаторы фестиваля предоставили травмированным очевидцам\n",
            "психологическую помощь. сотрудники проводят внутреннее расследование и\n",
            "выясняют, что стало причиной произошедшего. один из свидетелей\n",
            "инцидента, журналист информационного портала ilisolezwe унати кондиле\n",
            "(unathi kondile), прокомментировал увиденное на своей странице в\n",
            "facebook. «эти два клоуна стреляли друг в друга, и что-то пошло не\n",
            "так. все это произошло на наших глазах. люди плакали. мы ждали бригаду\n",
            "спасателей», — написал кондиле.\n",
            "\n",
            "автопортрет энди уорхола, выполненный в 1965 году и ранее не\n",
            "выставлявшийся, продадут с аукциона, пишет the new york times.\n",
            "автопортрет более 40 лет хранила бывшая секретарша уорхола кэти нейсо\n",
            "(cathy naso), которая получила картину от художника в оплату ее\n",
            "работы. нейсо работала в студии уорхола в 1965-67 годах. по ее словам,\n",
            "она работала на уорхола бесплатно и посвящала этому все свободное\n",
            "время, так как художник был ее кумиром. \"я просто хотела быть там\", -\n",
            "заявила нейсо, сравнив свою работу в студии художника с возможностью\n",
            "находиться за кулисами во время концерта. подарок уорхола некоторое\n",
            "время висел дома у нейсо, однако впоследствии она сняла его из\n",
            "опасения, что картину могут украсть, и до недавнего времени хранила\n",
            "портрет в шкафу. как заметил представитель аукционного дома sotheby's,\n",
            "организующего торги, благодаря этому цвета на картине очень хорошо\n",
            "сохранились. \"я бы очень хотела оставить этот автопортрет себе, но ему\n",
            "место в мире искусства. мне бы хотелось, чтобы его новый владелец\n",
            "дорожил им так же, как я\", - поделилась нейсо. аукцион, на котором\n",
            "будет продан портрет, состоится 11 ноября в нью-йорке. оценочная\n",
            "стоимость картины составляет 1 - 1,5 миллиона долларов.\n",
            "\n",
            "sony решила выпустить файтинг, который станет \"ответом на игру super\n",
            "smash bros\" от nintendo, пишет vg24/7 со ссылкой на paul gale network\n",
            "и neogaf. в новом проекте, в настоящее время известном под названием\n",
            "title fight, герои из нескольких игр издательства сразятся между\n",
            "собой. среди тех, кто появится в новом проекте, источник называет\n",
            "сладкоежку из twisted metal, кратоса из god of war, толстую принцессу\n",
            "из одноименного проекта, пса-рэпера параппу, слая купера, полковника\n",
            "маела радека из killzone и других персонажей из игр sony. кроме них,\n",
            "как пишет paul gale network, в игре может появиться джеймс бонд из\n",
            "последних частей \"бондианы\". разработкой игры, по данным eurogamer,\n",
            "занимается студия superbot. на ее сайте недавно появилось сообщение о\n",
            "том, что разработчики создают ps3-игру для sony computer entertainment\n",
            "america. на то, что это и есть новый файтинг от sony, указывают\n",
            "вакансии, размещенные на сайте студии: superbot ищет старшего\n",
            "дизайнера по боевой составляющей игры, который должен быть \"хорошо\n",
            "знаком с играми в жанре файтинг\" и ведущего дизайнера, который должен\n",
            "знать особенности сетевого режима и дизайна матчей в консольных играх.\n",
            "пользователи форума neogaf также нашли несколько упоминаний игры в\n",
            "твиттере дизайнера студии криса молины (chris molina), но вскоре после\n",
            "этого он удалил свой аккаунт. sony слухи о разработке игры пока не\n",
            "комментирует.\n",
            "\n",
            "волонтеры-экологи собрали более 120 тонн металлолома во время летних\n",
            "экспедиций на острове вилькицкого в карском море. для полной очистки\n",
            "территории понадобится до четырех лет, сообщает тасс со ссылкой на\n",
            "пресс-службу губернатора ямало-ненецкого автономного округа (янао).\n",
            "«представители российского государственного университета нефти и газа\n",
            "имени губкина взяли пробы грунта с различных участков острова, после\n",
            "проведения лабораторных исследований образцов ими будут предложены\n",
            "варианты средств для очистки почвы острова от разлитых нефтепродуктов\n",
            "и других загрязнений», — говорится в сообщении. работы будут\n",
            "продолжены до полной очистки острова. они проводятся на средства\n",
            "спонсоров, которыми являются «новатэк», «лукойл — западная сибирь» и\n",
            "«транснефть». по словам заместителя губернатора, директора\n",
            "департамента международных и внешнеэкономических связей региона\n",
            "александра мажарова, в состав рабочих групп вошли волонтеры из россии,\n",
            "казахстана, киргизии и израиля. они собрали 2,6 тысяч металлических\n",
            "бочек, из них 800 — с горюче-смазочным материалом. также от металла\n",
            "очищены все постройки, находящиеся на территории полярной станции,\n",
            "уточнили в пресс-службе. по данным тасс, остров вилькицкого,\n",
            "расположенный на востоке карского моря, между обской губой и\n",
            "енисейским заливом, является необитаемым и составляет в длину примерно\n",
            "18 километров. с 1954 года на острове функционировала\n",
            "метеорологическая станция, с 2011 года она переведена в формат\n",
            "автоматической. в 2017 году здесь стартовали экологические\n",
            "мероприятия.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0cebea8-94fc-43e7-88d3-d34a8e32ed4d"
      },
      "source": [
        "print(eval_summary1)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article1, model)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "дом audemars piguet оснастил часы вечным календарем\n",
            "\n",
            "\n",
            "a\n",
            "au\n",
            "aud\n",
            "audem\n",
            "audemars\n",
            "audemars p\n",
            "audemars pig\n",
            "audemars pigu\n",
            "audemars piguet\n",
            "audemars piguet представила\n",
            "audemars piguet представила новую\n",
            "audemars piguet представила новую модель\n",
            "audemars piguet представила новую модель из\n",
            "audemars piguet представила новую модель из коллекции\n",
            "audemars piguet представила новую модель из коллекции ro\n",
            "audemars piguet представила новую модель из коллекции roy\n",
            "audemars piguet представила новую модель из коллекции royal\n",
            "audemars piguet представила новую модель из коллекции royal o\n",
            "audemars piguet представила новую модель из коллекции royal oak\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70382b3e-dc7d-41cb-aff8-79fde58ed5d4"
      },
      "source": [
        "print(eval_summary2)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article2, model)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "мвф выделил кипру миллиард евро\n",
            "\n",
            "\n",
            "мвф\n",
            "мвф утвердил\n",
            "мвф утвердил кредит\n",
            "мвф утвердил кредит на\n",
            "мвф утвердил кредит на кипр\n",
            "мвф утвердил кредит на кипрский\n",
            "мвф утвердил кредит на кипрский кредит\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elJMh60rN5g4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b702f7ea-d280-4c39-fb95-a07d46cc846b"
      },
      "source": [
        "print(eval_summary3)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article3, model)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "фокусник случайно подстрелил ассистента на глазах у зрителей\n",
            "\n",
            "\n",
            "на\n",
            "на фестивале\n",
            "на фестивале в\n",
            "на фестивале в г\n",
            "на фестивале в грэ\n",
            "на фестивале в грэлково\n",
            "на фестивале в грэлково напали\n",
            "на фестивале в грэлково напали с\n",
            "на фестивале в грэлково напали с ножом\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXEb7Uek2KSH",
        "outputId": "3e61698a-0649-4a6b-b8ae-1572f9634988",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(eval_summary4)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article4, model)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "неизвестный автопортрет энди уорхола выставят на торги\n",
            "\n",
            "\n",
            "эн\n",
            "энди\n",
            "энди уор\n",
            "энди уорхо\n",
            "энди уорхола\n",
            "энди уорхола прода\n",
            "энди уорхола продадут\n",
            "энди уорхола продадут с\n",
            "энди уорхола продадут с аукциона\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5884Paz2KaZ",
        "outputId": "668e6379-e546-4084-b690-9eddf8373219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(eval_summary5)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article5, model)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sony приписали разработку нового файтинга\n",
            "\n",
            "\n",
            "sony\n",
            "sony выпустит\n",
            "sony выпустит фай\n",
            "sony выпустит файтинг\n",
            "sony выпустит файтинг от\n",
            "sony выпустит файтинг от nintendo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrFs7Mvw2aFJ",
        "outputId": "a6d8191f-31ee-416a-d978-573202c72b19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(eval_summary6)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article6, model)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "на необитаемом острове в карском море собрали 120 тонн металлолома\n",
            "\n",
            "\n",
            "волон\n",
            "волонтеры\n",
            "волонтеры-\n",
            "волонтеры-э\n",
            "волонтеры-эколо\n",
            "волонтеры-экологи\n",
            "волонтеры-экологи собра\n",
            "волонтеры-экологи собрали\n",
            "волонтеры-экологи собрали более\n",
            "волонтеры-экологи собрали более 120\n",
            "волонтеры-экологи собрали более 120 тонн\n",
            "волонтеры-экологи собрали более 120 тонн метал\n",
            "волонтеры-экологи собрали более 120 тонн металло\n",
            "волонтеры-экологи собрали более 120 тонн металлоло\n",
            "волонтеры-экологи собрали более 120 тонн металлолома\n",
            "волонтеры-экологи собрали более 120 тонн металлолома во\n",
            "волонтеры-экологи собрали более 120 тонн металлолома во время\n",
            "волонтеры-экологи собрали более 120 тонн металлолома во время лет\n",
            "волонтеры-экологи собрали более 120 тонн металлолома во время летних\n",
            "волонтеры-экологи собрали более 120 тонн металлолома во время летних\n",
            "экспеди\n",
            "волонтеры-экологи собрали более 120 тонн металлолома во время летних\n",
            "экспедиций\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdhIoFBbamUs"
      },
      "source": [
        "### Hystory generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zr8tbtwda-R"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrulXb2Jfsh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa0253f-8899-4202-db3b-ecabc0d7b378"
      },
      "source": [
        "print(wrapper.fill(eval_article1[:300]+'...'), '\\n')\r\n",
        "print('Заголовок:', eval_summary1)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "швейцарская часовая компания audemars piguet представила новую модель\n",
            "из коллекции royal oak. как сообщает luxurylaunches, речь идет о часах\n",
            "с вечным календарем. официальная презентация пройдет в рамках\n",
            "международного салона высокого часового искусства sihh, который\n",
            "проходит в женеве. часы выполнены... \n",
            "\n",
            "Заголовок: дом audemars piguet оснастил часы вечным календарем\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XMeYA6CaAUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4777c2fb-302b-48f4-db1c-974a7c74e376"
      },
      "source": [
        "for s in range(20, 260, 20):\r\n",
        "    model = SumTransformer(mode='eval')\r\n",
        "    model.init_from_file('/content/drive/MyDrive/model/model' + str(s) + '.pkl.gz', weights_only=True)\r\n",
        "    clear_output(wait=False)\r\n",
        "    print('Эпоха: {0}, {1} шагов'.format(int(s/20), s*1000))\r\n",
        "    _ = greedy_decode(eval_article1, model)    "
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Эпоха: 12, 240000 шагов\n",
            "\n",
            "a\n",
            "au\n",
            "aud\n",
            "audem\n",
            "audemars\n",
            "audemars p\n",
            "audemars pig\n",
            "audemars pigu\n",
            "audemars piguet\n",
            "audemars piguet представила\n",
            "audemars piguet представила новую\n",
            "audemars piguet представила новую модель\n",
            "audemars piguet представила новую модель из\n",
            "audemars piguet представила новую модель из коллекции\n",
            "audemars piguet представила новую модель из коллекции ro\n",
            "audemars piguet представила новую модель из коллекции roy\n",
            "audemars piguet представила новую модель из коллекции royal\n",
            "audemars piguet представила новую модель из коллекции royal o\n",
            "audemars piguet представила новую модель из коллекции royal oak\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}