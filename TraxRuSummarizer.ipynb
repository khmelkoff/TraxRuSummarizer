{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOAWo4Up77l8SlbvwM+05QI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e333044a-a535-4cc7-ee9f-cbbecc943f00"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 18.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 56.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 56.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 64.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 60.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 55.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 54.0MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "5e1878d6-b915-462b-a2d7-e159d0a8eba0"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "# data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "# data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "# data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "# data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "# text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "# for i in tqdm(range(data.shape[0])):\r\n",
        "    # if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        # text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafd90d3-592e-44a8-d602-30424903033f"
      },
      "source": [
        "# text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "9a443a1c-929d-46b7-a380-16c307c65131"
      },
      "source": [
        "# s0 = text_pairs[10][0]\r\n",
        "# text_list = wrapper.wrap(s0[:300])\r\n",
        "# for line in text_list:\r\n",
        "#     print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLsMmUFFnHA",
        "outputId": "a005addc-ad5c-47e2-cdde-e9a463c36024"
      },
      "source": [
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n",
            "в киеве исключили новый раунд минских переговоров\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "47816206-c61e-4514-ef3a-8638880eff50"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC79r_7EPy6",
        "outputId": "2a1123cf-ccf2-418a-f236-c84d6205fa47"
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4795faf3-f805-435b-e53a-a2c4ad5f3545"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21df2354-47a1-4901-d692-52a6cca9da11"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72518a7a-4ed1-4ba3-c5a9-57f1cc6fdeaa"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfdedd82-663d-42a0-8250-ded11bfc33a8"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2479   554  7702 10186    54  2779 13577  6218 15949     1     0  7832\n",
            "  5082  1162   401   749    17  2101  4496     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "b4fd518d-3e5a-47c2-f7c3-b547048af9cc"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "d2a7ec8b-9305-4710-bdff-64a2e882efde"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9712,   222,  9726,  3001, 15945,  6146,   959,  9050,  3516,\n",
              "       11023,    43,  1017,    60,   337, 15945,  8035,    25,  5578,\n",
              "       12555, 15945,   241,  2085,  8746, 15934,  2021,   358,  6530,\n",
              "       11299,   323, 15945,   258,     5,  6286,  9612,  3062,  8307,\n",
              "        1530,   873,  6185, 15949,     5,  2154,   554,  4934,  9660,\n",
              "          86,  3996,   161, 15945,  7293,  1277, 15945,    79, 11299,\n",
              "          31,   376, 15940,    29,   405,   179,    94, 15945,    41,\n",
              "         277,  1328,  3228,   688,  2116,   459, 15945,    79, 11299,\n",
              "          31,  5760,    65,  5129, 15949, 10950,    16,  6177,  1236,\n",
              "       15945,  8106,  1926,    86,  7128,   843,  4481, 15945,   102,\n",
              "        7559,    48,  8390,   386,  1666, 10244,  2174,  4261,  2523,\n",
              "        2664,    16,  9603, 15949,    25,  6213,  7194,    86,  1466,\n",
              "       15945, 13974,   106,     5,  4899, 15945,  7813, 15945,    79,\n",
              "           5,   532,  9896,  3437,    57,   736, 12628, 11023,     4,\n",
              "        5519, 15974,    81,  1385,    10,   294,   227,    16,  9039,\n",
              "        3519,  9874,    81,   117,  2456,   294,   570,  1552,  3806,\n",
              "           5,  9901,   154,   956, 15949,  2208,   127, 15933,  9050,\n",
              "       15945,  9604,  6901,   303,    64,  5950,  2731,     5,  5544,\n",
              "          60,   337,    16,   133,    30,   169,   161, 15945,   939,\n",
              "           5,  1416,   368, 10333, 14882, 15949,   498,  1995,  5845,\n",
              "        3592,    57,  3881, 12655,  3067,  6596,  1024,  1187,  6128,\n",
              "       15937, 14009, 15945,   505,     5,  1179,     4,   650,  7057,\n",
              "        6449,    56,    43,   994,  5318, 15582,  5185, 15933,  2707,\n",
              "        6711, 15949,  2325,    25,  4227, 11023,   554,  8214,  3213,\n",
              "         329,   213,    48,  1570, 12072, 12225, 15945,   570, 13511,\n",
              "       15945,    79,  6568,  2508,   794,   126, 15944,  8639,   106,\n",
              "        2109,  1356, 13058,     5,  8369,  4453, 15949,  3450,   470,\n",
              "        7248,  6122,  7784,  2248,  3890,    17,  5968,   768,   752,\n",
              "         118, 15949,     5, 14013,  1168,  1200,    88,   180, 10790,\n",
              "       11988, 12632,  6825,   369,  1466, 15945,    25,   570,  1791,\n",
              "         167,    15,   116, 11988, 12632,    52,     6,   917, 15977,\n",
              "         744,  2619,  5163,   309, 15949,  5317, 15945,   939,    71,\n",
              "        7740,  9211, 10923,    20,   189, 15945,   207,  9290, 15945,\n",
              "          86,  1054,  6967,  1666, 10244,    57,   102,  2692, 15945,\n",
              "        2246,  5284,  7049,   165,   532, 14770,    64,  1200,    88,\n",
              "         810, 15949,   207,  1310,   498, 15945,     5,  2222,  6195,\n",
              "       13063,   353, 12469,    17,  4094,  2512,  5305, 10412,   914,\n",
              "       15949,  1319,   918,  9922,    25,  3049, 15960, 10430,     5,\n",
              "        3947,  3756,   685,  9423, 15940,    16,  7305,    18, 15941,\n",
              "       15945,   345,  3073, 15945, 12406, 15945,   432,     9, 15926,\n",
              "       15945,    41,   277,    25,  5133,   719,    16,     5,  3641,\n",
              "         245,  9549,  1321, 15949,     1,     0, 15135, 13856,  9740,\n",
              "        9050,  3516, 11023,     1,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "43ade5b4-f9b4-4f49-9cdf-9220df2422e5"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.0001)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.0001), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "a3fd9011-83b8-4491-b1bc-0dc3b93a41d7"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  80100: Ran 100 train steps in 85.84 secs\n",
            "Step  80100: train CrossEntropyLoss |  5.09019089\n",
            "Step  80100: eval  CrossEntropyLoss |  4.61815548\n",
            "Step  80100: eval          Accuracy |  0.27142859\n",
            "\n",
            "Step  80200: Ran 100 train steps in 47.27 secs\n",
            "Step  80200: train CrossEntropyLoss |  5.13963985\n",
            "Step  80200: eval  CrossEntropyLoss |  5.10823917\n",
            "Step  80200: eval          Accuracy |  0.24347825\n",
            "\n",
            "Step  80300: Ran 100 train steps in 66.18 secs\n",
            "Step  80300: train CrossEntropyLoss |  5.06126547\n",
            "Step  80300: eval  CrossEntropyLoss |  5.08722353\n",
            "Step  80300: eval          Accuracy |  0.25714287\n",
            "\n",
            "Step  80400: Ran 100 train steps in 49.95 secs\n",
            "Step  80400: train CrossEntropyLoss |  4.99495840\n",
            "Step  80400: eval  CrossEntropyLoss |  5.08348227\n",
            "Step  80400: eval          Accuracy |  0.20454545\n",
            "\n",
            "Step  80500: Ran 100 train steps in 49.01 secs\n",
            "Step  80500: train CrossEntropyLoss |  5.01807642\n",
            "Step  80500: eval  CrossEntropyLoss |  5.07576418\n",
            "Step  80500: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  80600: Ran 100 train steps in 49.82 secs\n",
            "Step  80600: train CrossEntropyLoss |  4.92625332\n",
            "Step  80600: eval  CrossEntropyLoss |  5.73977137\n",
            "Step  80600: eval          Accuracy |  0.18103448\n",
            "\n",
            "Step  80700: Ran 100 train steps in 49.59 secs\n",
            "Step  80700: train CrossEntropyLoss |  4.96982098\n",
            "Step  80700: eval  CrossEntropyLoss |  5.45606899\n",
            "Step  80700: eval          Accuracy |  0.22131149\n",
            "\n",
            "Step  80800: Ran 100 train steps in 49.61 secs\n",
            "Step  80800: train CrossEntropyLoss |  4.87731075\n",
            "Step  80800: eval  CrossEntropyLoss |  4.49411774\n",
            "Step  80800: eval          Accuracy |  0.32460734\n",
            "\n",
            "Step  80900: Ran 100 train steps in 49.48 secs\n",
            "Step  80900: train CrossEntropyLoss |  4.89050484\n",
            "Step  80900: eval  CrossEntropyLoss |  5.02500153\n",
            "Step  80900: eval          Accuracy |  0.22950821\n",
            "\n",
            "Step  81000: Ran 100 train steps in 49.78 secs\n",
            "Step  81000: train CrossEntropyLoss |  4.92473269\n",
            "Step  81000: eval  CrossEntropyLoss |  4.91566277\n",
            "Step  81000: eval          Accuracy |  0.26804125\n",
            "\n",
            "Step  81100: Ran 100 train steps in 49.63 secs\n",
            "Step  81100: train CrossEntropyLoss |  4.85753298\n",
            "Step  81100: eval  CrossEntropyLoss |  5.32548189\n",
            "Step  81100: eval          Accuracy |  0.19512197\n",
            "\n",
            "Step  81200: Ran 100 train steps in 49.70 secs\n",
            "Step  81200: train CrossEntropyLoss |  4.87302160\n",
            "Step  81200: eval  CrossEntropyLoss |  5.03491926\n",
            "Step  81200: eval          Accuracy |  0.23232323\n",
            "\n",
            "Step  81300: Ran 100 train steps in 49.46 secs\n",
            "Step  81300: train CrossEntropyLoss |  4.79383612\n",
            "Step  81300: eval  CrossEntropyLoss |  4.89103889\n",
            "Step  81300: eval          Accuracy |  0.25365853\n",
            "\n",
            "Step  81400: Ran 100 train steps in 49.63 secs\n",
            "Step  81400: train CrossEntropyLoss |  4.76175976\n",
            "Step  81400: eval  CrossEntropyLoss |  5.20866776\n",
            "Step  81400: eval          Accuracy |  0.19642858\n",
            "\n",
            "Step  81500: Ran 100 train steps in 49.52 secs\n",
            "Step  81500: train CrossEntropyLoss |  4.82431984\n",
            "Step  81500: eval  CrossEntropyLoss |  5.10917950\n",
            "Step  81500: eval          Accuracy |  0.27551019\n",
            "\n",
            "Step  81600: Ran 100 train steps in 49.77 secs\n",
            "Step  81600: train CrossEntropyLoss |  4.82172918\n",
            "Step  81600: eval  CrossEntropyLoss |  4.79460955\n",
            "Step  81600: eval          Accuracy |  0.31111112\n",
            "\n",
            "Step  81700: Ran 100 train steps in 49.92 secs\n",
            "Step  81700: train CrossEntropyLoss |  4.82685137\n",
            "Step  81700: eval  CrossEntropyLoss |  5.47626877\n",
            "Step  81700: eval          Accuracy |  0.15841584\n",
            "\n",
            "Step  81800: Ran 100 train steps in 49.77 secs\n",
            "Step  81800: train CrossEntropyLoss |  4.83673096\n",
            "Step  81800: eval  CrossEntropyLoss |  4.48755836\n",
            "Step  81800: eval          Accuracy |  0.26804125\n",
            "\n",
            "Step  81900: Ran 100 train steps in 50.00 secs\n",
            "Step  81900: train CrossEntropyLoss |  4.79381704\n",
            "Step  81900: eval  CrossEntropyLoss |  5.62261200\n",
            "Step  81900: eval          Accuracy |  0.19318183\n",
            "\n",
            "Step  82000: Ran 100 train steps in 50.07 secs\n",
            "Step  82000: train CrossEntropyLoss |  4.74770689\n",
            "Step  82000: eval  CrossEntropyLoss |  4.50850058\n",
            "Step  82000: eval          Accuracy |  0.23809525\n",
            "\n",
            "Step  82100: Ran 100 train steps in 49.91 secs\n",
            "Step  82100: train CrossEntropyLoss |  4.78399229\n",
            "Step  82100: eval  CrossEntropyLoss |  4.81814146\n",
            "Step  82100: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  82200: Ran 100 train steps in 50.14 secs\n",
            "Step  82200: train CrossEntropyLoss |  4.81722069\n",
            "Step  82200: eval  CrossEntropyLoss |  4.04432631\n",
            "Step  82200: eval          Accuracy |  0.33495146\n",
            "\n",
            "Step  82300: Ran 100 train steps in 50.16 secs\n",
            "Step  82300: train CrossEntropyLoss |  4.71983862\n",
            "Step  82300: eval  CrossEntropyLoss |  4.95937586\n",
            "Step  82300: eval          Accuracy |  0.27358490\n",
            "\n",
            "Step  82400: Ran 100 train steps in 49.72 secs\n",
            "Step  82400: train CrossEntropyLoss |  4.71352053\n",
            "Step  82400: eval  CrossEntropyLoss |  4.66017389\n",
            "Step  82400: eval          Accuracy |  0.25688073\n",
            "\n",
            "Step  82500: Ran 100 train steps in 50.20 secs\n",
            "Step  82500: train CrossEntropyLoss |  4.81739330\n",
            "Step  82500: eval  CrossEntropyLoss |  4.84743214\n",
            "Step  82500: eval          Accuracy |  0.27678573\n",
            "\n",
            "Step  82600: Ran 100 train steps in 50.01 secs\n",
            "Step  82600: train CrossEntropyLoss |  4.75386906\n",
            "Step  82600: eval  CrossEntropyLoss |  4.82864523\n",
            "Step  82600: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  82700: Ran 100 train steps in 50.13 secs\n",
            "Step  82700: train CrossEntropyLoss |  4.69244623\n",
            "Step  82700: eval  CrossEntropyLoss |  4.87549257\n",
            "Step  82700: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  82800: Ran 100 train steps in 49.96 secs\n",
            "Step  82800: train CrossEntropyLoss |  4.71245670\n",
            "Step  82800: eval  CrossEntropyLoss |  5.19390392\n",
            "Step  82800: eval          Accuracy |  0.26262626\n",
            "\n",
            "Step  82900: Ran 100 train steps in 50.18 secs\n",
            "Step  82900: train CrossEntropyLoss |  4.66858864\n",
            "Step  82900: eval  CrossEntropyLoss |  5.16531515\n",
            "Step  82900: eval          Accuracy |  0.25961539\n",
            "\n",
            "Step  83000: Ran 100 train steps in 50.17 secs\n",
            "Step  83000: train CrossEntropyLoss |  4.76371145\n",
            "Step  83000: eval  CrossEntropyLoss |  4.64430380\n",
            "Step  83000: eval          Accuracy |  0.26732674\n",
            "\n",
            "Step  83100: Ran 100 train steps in 50.15 secs\n",
            "Step  83100: train CrossEntropyLoss |  4.78989935\n",
            "Step  83100: eval  CrossEntropyLoss |  4.09089518\n",
            "Step  83100: eval          Accuracy |  0.32547170\n",
            "\n",
            "Step  83200: Ran 100 train steps in 50.05 secs\n",
            "Step  83200: train CrossEntropyLoss |  4.77032042\n",
            "Step  83200: eval  CrossEntropyLoss |  4.18407774\n",
            "Step  83200: eval          Accuracy |  0.39047620\n",
            "\n",
            "Step  83300: Ran 100 train steps in 50.08 secs\n",
            "Step  83300: train CrossEntropyLoss |  4.75687170\n",
            "Step  83300: eval  CrossEntropyLoss |  4.84486341\n",
            "Step  83300: eval          Accuracy |  0.27642280\n",
            "\n",
            "Step  83400: Ran 100 train steps in 50.05 secs\n",
            "Step  83400: train CrossEntropyLoss |  4.73350000\n",
            "Step  83400: eval  CrossEntropyLoss |  4.54261589\n",
            "Step  83400: eval          Accuracy |  0.35260114\n",
            "\n",
            "Step  83500: Ran 100 train steps in 50.33 secs\n",
            "Step  83500: train CrossEntropyLoss |  4.64873028\n",
            "Step  83500: eval  CrossEntropyLoss |  5.09793520\n",
            "Step  83500: eval          Accuracy |  0.23076925\n",
            "\n",
            "Step  83600: Ran 100 train steps in 50.02 secs\n",
            "Step  83600: train CrossEntropyLoss |  4.71234798\n",
            "Step  83600: eval  CrossEntropyLoss |  5.13850594\n",
            "Step  83600: eval          Accuracy |  0.21904762\n",
            "\n",
            "Step  83700: Ran 100 train steps in 50.08 secs\n",
            "Step  83700: train CrossEntropyLoss |  4.67276812\n",
            "Step  83700: eval  CrossEntropyLoss |  4.59430218\n",
            "Step  83700: eval          Accuracy |  0.33333334\n",
            "\n",
            "Step  83800: Ran 100 train steps in 50.27 secs\n",
            "Step  83800: train CrossEntropyLoss |  4.67717266\n",
            "Step  83800: eval  CrossEntropyLoss |  4.76558447\n",
            "Step  83800: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  83900: Ran 100 train steps in 49.96 secs\n",
            "Step  83900: train CrossEntropyLoss |  4.63757801\n",
            "Step  83900: eval  CrossEntropyLoss |  5.28400755\n",
            "Step  83900: eval          Accuracy |  0.20754717\n",
            "\n",
            "Step  84000: Ran 100 train steps in 50.20 secs\n",
            "Step  84000: train CrossEntropyLoss |  4.65071297\n",
            "Step  84000: eval  CrossEntropyLoss |  4.26577997\n",
            "Step  84000: eval          Accuracy |  0.34042552\n",
            "\n",
            "Step  84100: Ran 100 train steps in 50.30 secs\n",
            "Step  84100: train CrossEntropyLoss |  4.68333387\n",
            "Step  84100: eval  CrossEntropyLoss |  3.98002672\n",
            "Step  84100: eval          Accuracy |  0.34579438\n",
            "\n",
            "Step  84200: Ran 100 train steps in 49.88 secs\n",
            "Step  84200: train CrossEntropyLoss |  4.70752668\n",
            "Step  84200: eval  CrossEntropyLoss |  5.20354795\n",
            "Step  84200: eval          Accuracy |  0.23076925\n",
            "\n",
            "Step  84300: Ran 100 train steps in 50.22 secs\n",
            "Step  84300: train CrossEntropyLoss |  4.65236664\n",
            "Step  84300: eval  CrossEntropyLoss |  4.58085823\n",
            "Step  84300: eval          Accuracy |  0.26865670\n",
            "\n",
            "Step  84400: Ran 100 train steps in 49.93 secs\n",
            "Step  84400: train CrossEntropyLoss |  4.69944334\n",
            "Step  84400: eval  CrossEntropyLoss |  4.94371557\n",
            "Step  84400: eval          Accuracy |  0.20512822\n",
            "\n",
            "Step  84500: Ran 100 train steps in 49.94 secs\n",
            "Step  84500: train CrossEntropyLoss |  4.69122458\n",
            "Step  84500: eval  CrossEntropyLoss |  5.42287779\n",
            "Step  84500: eval          Accuracy |  0.17796610\n",
            "\n",
            "Step  84600: Ran 100 train steps in 50.14 secs\n",
            "Step  84600: train CrossEntropyLoss |  4.70548010\n",
            "Step  84600: eval  CrossEntropyLoss |  4.24682570\n",
            "Step  84600: eval          Accuracy |  0.31553400\n",
            "\n",
            "Step  84700: Ran 100 train steps in 50.05 secs\n",
            "Step  84700: train CrossEntropyLoss |  4.67716789\n",
            "Step  84700: eval  CrossEntropyLoss |  4.11458302\n",
            "Step  84700: eval          Accuracy |  0.26744187\n",
            "\n",
            "Step  84800: Ran 100 train steps in 50.05 secs\n",
            "Step  84800: train CrossEntropyLoss |  4.68304014\n",
            "Step  84800: eval  CrossEntropyLoss |  4.68864012\n",
            "Step  84800: eval          Accuracy |  0.25471699\n",
            "\n",
            "Step  84900: Ran 100 train steps in 49.93 secs\n",
            "Step  84900: train CrossEntropyLoss |  4.61965275\n",
            "Step  84900: eval  CrossEntropyLoss |  4.70288277\n",
            "Step  84900: eval          Accuracy |  0.28021979\n",
            "\n",
            "Step  85000: Ran 100 train steps in 50.14 secs\n",
            "Step  85000: train CrossEntropyLoss |  4.67328310\n",
            "Step  85000: eval  CrossEntropyLoss |  5.08701992\n",
            "Step  85000: eval          Accuracy |  0.20618558\n",
            "\n",
            "Step  85100: Ran 100 train steps in 49.96 secs\n",
            "Step  85100: train CrossEntropyLoss |  4.66622496\n",
            "Step  85100: eval  CrossEntropyLoss |  4.25746012\n",
            "Step  85100: eval          Accuracy |  0.29126215\n",
            "\n",
            "Step  85200: Ran 100 train steps in 49.99 secs\n",
            "Step  85200: train CrossEntropyLoss |  4.67796087\n",
            "Step  85200: eval  CrossEntropyLoss |  4.49309540\n",
            "Step  85200: eval          Accuracy |  0.24770641\n",
            "\n",
            "Step  85300: Ran 100 train steps in 49.84 secs\n",
            "Step  85300: train CrossEntropyLoss |  4.65064287\n",
            "Step  85300: eval  CrossEntropyLoss |  4.87079191\n",
            "Step  85300: eval          Accuracy |  0.29310346\n",
            "\n",
            "Step  85400: Ran 100 train steps in 50.06 secs\n",
            "Step  85400: train CrossEntropyLoss |  4.58305120\n",
            "Step  85400: eval  CrossEntropyLoss |  4.55222368\n",
            "Step  85400: eval          Accuracy |  0.25961539\n",
            "\n",
            "Step  85500: Ran 100 train steps in 50.05 secs\n",
            "Step  85500: train CrossEntropyLoss |  4.62205744\n",
            "Step  85500: eval  CrossEntropyLoss |  4.40718889\n",
            "Step  85500: eval          Accuracy |  0.24365482\n",
            "\n",
            "Step  85600: Ran 100 train steps in 49.89 secs\n",
            "Step  85600: train CrossEntropyLoss |  4.61025095\n",
            "Step  85600: eval  CrossEntropyLoss |  4.57601833\n",
            "Step  85600: eval          Accuracy |  0.22018348\n",
            "\n",
            "Step  85700: Ran 100 train steps in 50.13 secs\n",
            "Step  85700: train CrossEntropyLoss |  4.63538170\n",
            "Step  85700: eval  CrossEntropyLoss |  4.69621801\n",
            "Step  85700: eval          Accuracy |  0.29914531\n",
            "\n",
            "Step  85800: Ran 100 train steps in 50.04 secs\n",
            "Step  85800: train CrossEntropyLoss |  4.60710001\n",
            "Step  85800: eval  CrossEntropyLoss |  4.18628788\n",
            "Step  85800: eval          Accuracy |  0.29901960\n",
            "\n",
            "Step  85900: Ran 100 train steps in 50.07 secs\n",
            "Step  85900: train CrossEntropyLoss |  4.67566681\n",
            "Step  85900: eval  CrossEntropyLoss |  5.25001478\n",
            "Step  85900: eval          Accuracy |  0.24137931\n",
            "\n",
            "Step  86000: Ran 100 train steps in 50.03 secs\n",
            "Step  86000: train CrossEntropyLoss |  4.60561800\n",
            "Step  86000: eval  CrossEntropyLoss |  4.85744286\n",
            "Step  86000: eval          Accuracy |  0.21428573\n",
            "\n",
            "Step  86100: Ran 100 train steps in 50.01 secs\n",
            "Step  86100: train CrossEntropyLoss |  4.58427477\n",
            "Step  86100: eval  CrossEntropyLoss |  4.22633791\n",
            "Step  86100: eval          Accuracy |  0.30434781\n",
            "\n",
            "Step  86200: Ran 100 train steps in 50.08 secs\n",
            "Step  86200: train CrossEntropyLoss |  4.59364748\n",
            "Step  86200: eval  CrossEntropyLoss |  3.86558914\n",
            "Step  86200: eval          Accuracy |  0.32812500\n",
            "\n",
            "Step  86300: Ran 100 train steps in 50.12 secs\n",
            "Step  86300: train CrossEntropyLoss |  4.65285492\n",
            "Step  86300: eval  CrossEntropyLoss |  5.02087688\n",
            "Step  86300: eval          Accuracy |  0.19000000\n",
            "\n",
            "Step  86400: Ran 100 train steps in 50.10 secs\n",
            "Step  86400: train CrossEntropyLoss |  4.58899784\n",
            "Step  86400: eval  CrossEntropyLoss |  5.07026672\n",
            "Step  86400: eval          Accuracy |  0.26373628\n",
            "\n",
            "Step  86500: Ran 100 train steps in 49.93 secs\n",
            "Step  86500: train CrossEntropyLoss |  4.56818390\n",
            "Step  86500: eval  CrossEntropyLoss |  3.90028048\n",
            "Step  86500: eval          Accuracy |  0.38461539\n",
            "\n",
            "Step  86600: Ran 100 train steps in 50.14 secs\n",
            "Step  86600: train CrossEntropyLoss |  4.63968039\n",
            "Step  86600: eval  CrossEntropyLoss |  4.86605740\n",
            "Step  86600: eval          Accuracy |  0.27722773\n",
            "\n",
            "Step  86700: Ran 100 train steps in 50.06 secs\n",
            "Step  86700: train CrossEntropyLoss |  4.60709810\n",
            "Step  86700: eval  CrossEntropyLoss |  4.63829517\n",
            "Step  86700: eval          Accuracy |  0.20560747\n",
            "\n",
            "Step  86800: Ran 100 train steps in 49.98 secs\n",
            "Step  86800: train CrossEntropyLoss |  4.62934113\n",
            "Step  86800: eval  CrossEntropyLoss |  4.64660358\n",
            "Step  86800: eval          Accuracy |  0.27722773\n",
            "\n",
            "Step  86900: Ran 100 train steps in 50.07 secs\n",
            "Step  86900: train CrossEntropyLoss |  4.62270451\n",
            "Step  86900: eval  CrossEntropyLoss |  4.20149231\n",
            "Step  86900: eval          Accuracy |  0.30890054\n",
            "\n",
            "Step  87000: Ran 100 train steps in 49.93 secs\n",
            "Step  87000: train CrossEntropyLoss |  4.57630777\n",
            "Step  87000: eval  CrossEntropyLoss |  4.38734674\n",
            "Step  87000: eval          Accuracy |  0.26999998\n",
            "\n",
            "Step  87100: Ran 100 train steps in 49.84 secs\n",
            "Step  87100: train CrossEntropyLoss |  4.58717823\n",
            "Step  87100: eval  CrossEntropyLoss |  4.54812193\n",
            "Step  87100: eval          Accuracy |  0.29752064\n",
            "\n",
            "Step  87200: Ran 100 train steps in 50.47 secs\n",
            "Step  87200: train CrossEntropyLoss |  4.58321142\n",
            "Step  87200: eval  CrossEntropyLoss |  4.94364405\n",
            "Step  87200: eval          Accuracy |  0.25654450\n",
            "\n",
            "Step  87300: Ran 100 train steps in 50.07 secs\n",
            "Step  87300: train CrossEntropyLoss |  4.67083025\n",
            "Step  87300: eval  CrossEntropyLoss |  4.22076464\n",
            "Step  87300: eval          Accuracy |  0.31067961\n",
            "\n",
            "Step  87400: Ran 100 train steps in 49.85 secs\n",
            "Step  87400: train CrossEntropyLoss |  4.57255888\n",
            "Step  87400: eval  CrossEntropyLoss |  4.19572353\n",
            "Step  87400: eval          Accuracy |  0.33673468\n",
            "\n",
            "Step  87500: Ran 100 train steps in 50.00 secs\n",
            "Step  87500: train CrossEntropyLoss |  4.61732340\n",
            "Step  87500: eval  CrossEntropyLoss |  4.45352793\n",
            "Step  87500: eval          Accuracy |  0.27232143\n",
            "\n",
            "Step  87600: Ran 100 train steps in 49.81 secs\n",
            "Step  87600: train CrossEntropyLoss |  4.53806925\n",
            "Step  87600: eval  CrossEntropyLoss |  4.72528744\n",
            "Step  87600: eval          Accuracy |  0.25806451\n",
            "\n",
            "Step  87700: Ran 100 train steps in 50.01 secs\n",
            "Step  87700: train CrossEntropyLoss |  4.57791662\n",
            "Step  87700: eval  CrossEntropyLoss |  4.92520237\n",
            "Step  87700: eval          Accuracy |  0.26190478\n",
            "\n",
            "Step  87800: Ran 100 train steps in 50.02 secs\n",
            "Step  87800: train CrossEntropyLoss |  4.57903433\n",
            "Step  87800: eval  CrossEntropyLoss |  4.25950527\n",
            "Step  87800: eval          Accuracy |  0.32258064\n",
            "\n",
            "Step  87900: Ran 100 train steps in 49.85 secs\n",
            "Step  87900: train CrossEntropyLoss |  4.61534929\n",
            "Step  87900: eval  CrossEntropyLoss |  4.86908436\n",
            "Step  87900: eval          Accuracy |  0.25806451\n",
            "\n",
            "Step  88000: Ran 100 train steps in 50.04 secs\n",
            "Step  88000: train CrossEntropyLoss |  4.56536436\n",
            "Step  88000: eval  CrossEntropyLoss |  4.69851303\n",
            "Step  88000: eval          Accuracy |  0.22935779\n",
            "\n",
            "Step  88100: Ran 100 train steps in 49.89 secs\n",
            "Step  88100: train CrossEntropyLoss |  4.56767941\n",
            "Step  88100: eval  CrossEntropyLoss |  4.72356892\n",
            "Step  88100: eval          Accuracy |  0.24528302\n",
            "\n",
            "Step  88200: Ran 100 train steps in 49.75 secs\n",
            "Step  88200: train CrossEntropyLoss |  4.53644705\n",
            "Step  88200: eval  CrossEntropyLoss |  4.69634914\n",
            "Step  88200: eval          Accuracy |  0.24770641\n",
            "\n",
            "Step  88300: Ran 100 train steps in 50.16 secs\n",
            "Step  88300: train CrossEntropyLoss |  4.59743357\n",
            "Step  88300: eval  CrossEntropyLoss |  5.13183165\n",
            "Step  88300: eval          Accuracy |  0.23711342\n",
            "\n",
            "Step  88400: Ran 100 train steps in 49.86 secs\n",
            "Step  88400: train CrossEntropyLoss |  4.56961489\n",
            "Step  88400: eval  CrossEntropyLoss |  4.32660198\n",
            "Step  88400: eval          Accuracy |  0.29807693\n",
            "\n",
            "Step  88500: Ran 100 train steps in 49.68 secs\n",
            "Step  88500: train CrossEntropyLoss |  4.60372257\n",
            "Step  88500: eval  CrossEntropyLoss |  4.52391720\n",
            "Step  88500: eval          Accuracy |  0.30000001\n",
            "\n",
            "Step  88600: Ran 100 train steps in 49.88 secs\n",
            "Step  88600: train CrossEntropyLoss |  4.53562260\n",
            "Step  88600: eval  CrossEntropyLoss |  5.43851137\n",
            "Step  88600: eval          Accuracy |  0.22222224\n",
            "\n",
            "Step  88700: Ran 100 train steps in 49.79 secs\n",
            "Step  88700: train CrossEntropyLoss |  4.58338785\n",
            "Step  88700: eval  CrossEntropyLoss |  4.16997147\n",
            "Step  88700: eval          Accuracy |  0.32710278\n",
            "\n",
            "Step  88800: Ran 100 train steps in 50.11 secs\n",
            "Step  88800: train CrossEntropyLoss |  4.57510805\n",
            "Step  88800: eval  CrossEntropyLoss |  4.22168589\n",
            "Step  88800: eval          Accuracy |  0.34234235\n",
            "\n",
            "Step  88900: Ran 100 train steps in 50.02 secs\n",
            "Step  88900: train CrossEntropyLoss |  4.55829906\n",
            "Step  88900: eval  CrossEntropyLoss |  5.09187746\n",
            "Step  88900: eval          Accuracy |  0.17647059\n",
            "\n",
            "Step  89000: Ran 100 train steps in 49.97 secs\n",
            "Step  89000: train CrossEntropyLoss |  4.53587198\n",
            "Step  89000: eval  CrossEntropyLoss |  4.64373446\n",
            "Step  89000: eval          Accuracy |  0.26886794\n",
            "\n",
            "Step  89100: Ran 100 train steps in 50.03 secs\n",
            "Step  89100: train CrossEntropyLoss |  4.60962343\n",
            "Step  89100: eval  CrossEntropyLoss |  4.35265541\n",
            "Step  89100: eval          Accuracy |  0.28409091\n",
            "\n",
            "Step  89200: Ran 100 train steps in 49.80 secs\n",
            "Step  89200: train CrossEntropyLoss |  4.56347322\n",
            "Step  89200: eval  CrossEntropyLoss |  4.94358015\n",
            "Step  89200: eval          Accuracy |  0.23684211\n",
            "\n",
            "Step  89300: Ran 100 train steps in 50.11 secs\n",
            "Step  89300: train CrossEntropyLoss |  4.64240122\n",
            "Step  89300: eval  CrossEntropyLoss |  4.42816591\n",
            "Step  89300: eval          Accuracy |  0.27722773\n",
            "\n",
            "Step  89400: Ran 100 train steps in 50.28 secs\n",
            "Step  89400: train CrossEntropyLoss |  4.58287954\n",
            "Step  89400: eval  CrossEntropyLoss |  4.80841208\n",
            "Step  89400: eval          Accuracy |  0.24761906\n",
            "\n",
            "Step  89500: Ran 100 train steps in 50.03 secs\n",
            "Step  89500: train CrossEntropyLoss |  4.62012815\n",
            "Step  89500: eval  CrossEntropyLoss |  4.33473396\n",
            "Step  89500: eval          Accuracy |  0.27751198\n",
            "\n",
            "Step  89600: Ran 100 train steps in 50.25 secs\n",
            "Step  89600: train CrossEntropyLoss |  4.56523132\n",
            "Step  89600: eval  CrossEntropyLoss |  4.25789547\n",
            "Step  89600: eval          Accuracy |  0.30851063\n",
            "\n",
            "Step  89700: Ran 100 train steps in 50.13 secs\n",
            "Step  89700: train CrossEntropyLoss |  4.57862759\n",
            "Step  89700: eval  CrossEntropyLoss |  4.48404169\n",
            "Step  89700: eval          Accuracy |  0.28431374\n",
            "\n",
            "Step  89800: Ran 100 train steps in 50.22 secs\n",
            "Step  89800: train CrossEntropyLoss |  4.60272551\n",
            "Step  89800: eval  CrossEntropyLoss |  4.93068027\n",
            "Step  89800: eval          Accuracy |  0.24137931\n",
            "\n",
            "Step  89900: Ran 100 train steps in 50.05 secs\n",
            "Step  89900: train CrossEntropyLoss |  4.52464390\n",
            "Step  89900: eval  CrossEntropyLoss |  4.32461166\n",
            "Step  89900: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  90000: Ran 100 train steps in 50.12 secs\n",
            "Step  90000: train CrossEntropyLoss |  4.55768681\n",
            "Step  90000: eval  CrossEntropyLoss |  4.47007561\n",
            "Step  90000: eval          Accuracy |  0.27368423\n",
            "\n",
            "Step  90100: Ran 100 train steps in 50.14 secs\n",
            "Step  90100: train CrossEntropyLoss |  4.60075903\n",
            "Step  90100: eval  CrossEntropyLoss |  4.66242123\n",
            "Step  90100: eval          Accuracy |  0.27522933\n",
            "\n",
            "Step  90200: Ran 100 train steps in 50.12 secs\n",
            "Step  90200: train CrossEntropyLoss |  4.60695839\n",
            "Step  90200: eval  CrossEntropyLoss |  4.79817438\n",
            "Step  90200: eval          Accuracy |  0.27184466\n",
            "\n",
            "Step  90300: Ran 100 train steps in 50.36 secs\n",
            "Step  90300: train CrossEntropyLoss |  4.47747278\n",
            "Step  90300: eval  CrossEntropyLoss |  4.56932449\n",
            "Step  90300: eval          Accuracy |  0.27692309\n",
            "\n",
            "Step  90400: Ran 100 train steps in 50.67 secs\n",
            "Step  90400: train CrossEntropyLoss |  4.63812351\n",
            "Step  90400: eval  CrossEntropyLoss |  4.93408871\n",
            "Step  90400: eval          Accuracy |  0.21782178\n",
            "\n",
            "Step  90500: Ran 100 train steps in 50.04 secs\n",
            "Step  90500: train CrossEntropyLoss |  4.57740307\n",
            "Step  90500: eval  CrossEntropyLoss |  4.69712210\n",
            "Step  90500: eval          Accuracy |  0.28225806\n",
            "\n",
            "Step  90600: Ran 100 train steps in 50.10 secs\n",
            "Step  90600: train CrossEntropyLoss |  4.58717346\n",
            "Step  90600: eval  CrossEntropyLoss |  4.80011320\n",
            "Step  90600: eval          Accuracy |  0.27049181\n",
            "\n",
            "Step  90700: Ran 100 train steps in 49.88 secs\n",
            "Step  90700: train CrossEntropyLoss |  4.51295042\n",
            "Step  90700: eval  CrossEntropyLoss |  4.74320412\n",
            "Step  90700: eval          Accuracy |  0.21465969\n",
            "\n",
            "Step  90800: Ran 100 train steps in 50.05 secs\n",
            "Step  90800: train CrossEntropyLoss |  4.54194069\n",
            "Step  90800: eval  CrossEntropyLoss |  5.05573893\n",
            "Step  90800: eval          Accuracy |  0.27027029\n",
            "\n",
            "Step  90900: Ran 100 train steps in 49.39 secs\n",
            "Step  90900: train CrossEntropyLoss |  4.52476025\n",
            "Step  90900: eval  CrossEntropyLoss |  3.88747764\n",
            "Step  90900: eval          Accuracy |  0.36363637\n",
            "\n",
            "Step  91000: Ran 100 train steps in 48.52 secs\n",
            "Step  91000: train CrossEntropyLoss |  4.55929756\n",
            "Step  91000: eval  CrossEntropyLoss |  4.73407507\n",
            "Step  91000: eval          Accuracy |  0.25628141\n",
            "\n",
            "Step  91100: Ran 100 train steps in 47.80 secs\n",
            "Step  91100: train CrossEntropyLoss |  4.55541515\n",
            "Step  91100: eval  CrossEntropyLoss |  4.47399855\n",
            "Step  91100: eval          Accuracy |  0.21212122\n",
            "\n",
            "Step  91200: Ran 100 train steps in 47.73 secs\n",
            "Step  91200: train CrossEntropyLoss |  4.56556273\n",
            "Step  91200: eval  CrossEntropyLoss |  4.63862610\n",
            "Step  91200: eval          Accuracy |  0.29999998\n",
            "\n",
            "Step  91300: Ran 100 train steps in 47.60 secs\n",
            "Step  91300: train CrossEntropyLoss |  4.54749727\n",
            "Step  91300: eval  CrossEntropyLoss |  4.35346794\n",
            "Step  91300: eval          Accuracy |  0.30051816\n",
            "\n",
            "Step  91400: Ran 100 train steps in 47.64 secs\n",
            "Step  91400: train CrossEntropyLoss |  4.52863503\n",
            "Step  91400: eval  CrossEntropyLoss |  4.68294382\n",
            "Step  91400: eval          Accuracy |  0.23584905\n",
            "\n",
            "Step  91500: Ran 100 train steps in 47.42 secs\n",
            "Step  91500: train CrossEntropyLoss |  4.53920460\n",
            "Step  91500: eval  CrossEntropyLoss |  4.16285896\n",
            "Step  91500: eval          Accuracy |  0.29090908\n",
            "\n",
            "Step  91600: Ran 100 train steps in 47.51 secs\n",
            "Step  91600: train CrossEntropyLoss |  4.51347685\n",
            "Step  91600: eval  CrossEntropyLoss |  4.95655537\n",
            "Step  91600: eval          Accuracy |  0.19607843\n",
            "\n",
            "Step  91700: Ran 100 train steps in 47.39 secs\n",
            "Step  91700: train CrossEntropyLoss |  4.52710247\n",
            "Step  91700: eval  CrossEntropyLoss |  4.47718573\n",
            "Step  91700: eval          Accuracy |  0.29353234\n",
            "\n",
            "Step  91800: Ran 100 train steps in 47.42 secs\n",
            "Step  91800: train CrossEntropyLoss |  4.53733492\n",
            "Step  91800: eval  CrossEntropyLoss |  4.27789259\n",
            "Step  91800: eval          Accuracy |  0.27027029\n",
            "\n",
            "Step  91900: Ran 100 train steps in 47.80 secs\n",
            "Step  91900: train CrossEntropyLoss |  4.49854565\n",
            "Step  91900: eval  CrossEntropyLoss |  4.51180267\n",
            "Step  91900: eval          Accuracy |  0.28571430\n",
            "\n",
            "Step  92000: Ran 100 train steps in 47.27 secs\n",
            "Step  92000: train CrossEntropyLoss |  4.51613903\n",
            "Step  92000: eval  CrossEntropyLoss |  4.85833216\n",
            "Step  92000: eval          Accuracy |  0.22807017\n",
            "\n",
            "Step  92100: Ran 100 train steps in 47.45 secs\n",
            "Step  92100: train CrossEntropyLoss |  4.53645182\n",
            "Step  92100: eval  CrossEntropyLoss |  5.23165703\n",
            "Step  92100: eval          Accuracy |  0.19090909\n",
            "\n",
            "Step  92200: Ran 100 train steps in 47.41 secs\n",
            "Step  92200: train CrossEntropyLoss |  4.47082186\n",
            "Step  92200: eval  CrossEntropyLoss |  4.34491730\n",
            "Step  92200: eval          Accuracy |  0.32487309\n",
            "\n",
            "Step  92300: Ran 100 train steps in 47.42 secs\n",
            "Step  92300: train CrossEntropyLoss |  4.46217442\n",
            "Step  92300: eval  CrossEntropyLoss |  4.35240221\n",
            "Step  92300: eval          Accuracy |  0.33628318\n",
            "\n",
            "Step  92400: Ran 100 train steps in 47.56 secs\n",
            "Step  92400: train CrossEntropyLoss |  4.44358158\n",
            "Step  92400: eval  CrossEntropyLoss |  5.01182556\n",
            "Step  92400: eval          Accuracy |  0.25619835\n",
            "\n",
            "Step  92500: Ran 100 train steps in 47.33 secs\n",
            "Step  92500: train CrossEntropyLoss |  4.49274969\n",
            "Step  92500: eval  CrossEntropyLoss |  4.06176186\n",
            "Step  92500: eval          Accuracy |  0.33333334\n",
            "\n",
            "Step  92600: Ran 100 train steps in 47.70 secs\n",
            "Step  92600: train CrossEntropyLoss |  4.50224161\n",
            "Step  92600: eval  CrossEntropyLoss |  4.62457609\n",
            "Step  92600: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  92700: Ran 100 train steps in 47.20 secs\n",
            "Step  92700: train CrossEntropyLoss |  4.52535582\n",
            "Step  92700: eval  CrossEntropyLoss |  4.16999483\n",
            "Step  92700: eval          Accuracy |  0.25641027\n",
            "\n",
            "Step  92800: Ran 100 train steps in 47.45 secs\n",
            "Step  92800: train CrossEntropyLoss |  4.47138977\n",
            "Step  92800: eval  CrossEntropyLoss |  4.56381750\n",
            "Step  92800: eval          Accuracy |  0.22869956\n",
            "\n",
            "Step  92900: Ran 100 train steps in 47.36 secs\n",
            "Step  92900: train CrossEntropyLoss |  4.53293324\n",
            "Step  92900: eval  CrossEntropyLoss |  4.29356718\n",
            "Step  92900: eval          Accuracy |  0.30000001\n",
            "\n",
            "Step  93000: Ran 100 train steps in 47.45 secs\n",
            "Step  93000: train CrossEntropyLoss |  4.50359297\n",
            "Step  93000: eval  CrossEntropyLoss |  3.85282826\n",
            "Step  93000: eval          Accuracy |  0.34999999\n",
            "\n",
            "Step  93100: Ran 100 train steps in 47.56 secs\n",
            "Step  93100: train CrossEntropyLoss |  4.42333889\n",
            "Step  93100: eval  CrossEntropyLoss |  4.55342913\n",
            "Step  93100: eval          Accuracy |  0.25396827\n",
            "\n",
            "Step  93200: Ran 100 train steps in 47.12 secs\n",
            "Step  93200: train CrossEntropyLoss |  4.52223730\n",
            "Step  93200: eval  CrossEntropyLoss |  4.59505415\n",
            "Step  93200: eval          Accuracy |  0.27659574\n",
            "\n",
            "Step  93300: Ran 100 train steps in 47.69 secs\n",
            "Step  93300: train CrossEntropyLoss |  4.46114016\n",
            "Step  93300: eval  CrossEntropyLoss |  4.41266298\n",
            "Step  93300: eval          Accuracy |  0.28985506\n",
            "\n",
            "Step  93400: Ran 100 train steps in 47.62 secs\n",
            "Step  93400: train CrossEntropyLoss |  4.51003218\n",
            "Step  93400: eval  CrossEntropyLoss |  4.94664574\n",
            "Step  93400: eval          Accuracy |  0.20535715\n",
            "\n",
            "Step  93500: Ran 100 train steps in 47.65 secs\n",
            "Step  93500: train CrossEntropyLoss |  4.44842434\n",
            "Step  93500: eval  CrossEntropyLoss |  4.39906120\n",
            "Step  93500: eval          Accuracy |  0.27380952\n",
            "\n",
            "Step  93600: Ran 100 train steps in 47.53 secs\n",
            "Step  93600: train CrossEntropyLoss |  4.52098417\n",
            "Step  93600: eval  CrossEntropyLoss |  4.47908926\n",
            "Step  93600: eval          Accuracy |  0.24590166\n",
            "\n",
            "Step  93700: Ran 100 train steps in 47.21 secs\n",
            "Step  93700: train CrossEntropyLoss |  4.46148539\n",
            "Step  93700: eval  CrossEntropyLoss |  4.64450264\n",
            "Step  93700: eval          Accuracy |  0.30630630\n",
            "\n",
            "Step  93800: Ran 100 train steps in 47.49 secs\n",
            "Step  93800: train CrossEntropyLoss |  4.53950024\n",
            "Step  93800: eval  CrossEntropyLoss |  4.06328869\n",
            "Step  93800: eval          Accuracy |  0.34759358\n",
            "\n",
            "Step  93900: Ran 100 train steps in 47.53 secs\n",
            "Step  93900: train CrossEntropyLoss |  4.47876263\n",
            "Step  93900: eval  CrossEntropyLoss |  4.36061335\n",
            "Step  93900: eval          Accuracy |  0.30392158\n",
            "\n",
            "Step  94000: Ran 100 train steps in 47.30 secs\n",
            "Step  94000: train CrossEntropyLoss |  4.48802423\n",
            "Step  94000: eval  CrossEntropyLoss |  4.23059464\n",
            "Step  94000: eval          Accuracy |  0.30769232\n",
            "\n",
            "Step  94100: Ran 100 train steps in 47.50 secs\n",
            "Step  94100: train CrossEntropyLoss |  4.51114511\n",
            "Step  94100: eval  CrossEntropyLoss |  5.39919424\n",
            "Step  94100: eval          Accuracy |  0.16037735\n",
            "\n",
            "Step  94200: Ran 100 train steps in 47.45 secs\n",
            "Step  94200: train CrossEntropyLoss |  4.47173405\n",
            "Step  94200: eval  CrossEntropyLoss |  4.37000227\n",
            "Step  94200: eval          Accuracy |  0.29787233\n",
            "\n",
            "Step  94300: Ran 100 train steps in 47.57 secs\n",
            "Step  94300: train CrossEntropyLoss |  4.48344851\n",
            "Step  94300: eval  CrossEntropyLoss |  5.39179373\n",
            "Step  94300: eval          Accuracy |  0.18421052\n",
            "\n",
            "Step  94400: Ran 100 train steps in 47.62 secs\n",
            "Step  94400: train CrossEntropyLoss |  4.47734070\n",
            "Step  94400: eval  CrossEntropyLoss |  4.52136660\n",
            "Step  94400: eval          Accuracy |  0.26168224\n",
            "\n",
            "Step  94500: Ran 100 train steps in 47.39 secs\n",
            "Step  94500: train CrossEntropyLoss |  4.47178173\n",
            "Step  94500: eval  CrossEntropyLoss |  4.19969177\n",
            "Step  94500: eval          Accuracy |  0.32460734\n",
            "\n",
            "Step  94600: Ran 100 train steps in 47.18 secs\n",
            "Step  94600: train CrossEntropyLoss |  4.45122194\n",
            "Step  94600: eval  CrossEntropyLoss |  4.47625732\n",
            "Step  94600: eval          Accuracy |  0.28440365\n",
            "\n",
            "Step  94700: Ran 100 train steps in 47.38 secs\n",
            "Step  94700: train CrossEntropyLoss |  4.50763273\n",
            "Step  94700: eval  CrossEntropyLoss |  4.55060673\n",
            "Step  94700: eval          Accuracy |  0.24299064\n",
            "\n",
            "Step  94800: Ran 100 train steps in 47.75 secs\n",
            "Step  94800: train CrossEntropyLoss |  4.45019388\n",
            "Step  94800: eval  CrossEntropyLoss |  4.61367750\n",
            "Step  94800: eval          Accuracy |  0.22123894\n",
            "\n",
            "Step  94900: Ran 100 train steps in 47.46 secs\n",
            "Step  94900: train CrossEntropyLoss |  4.43411112\n",
            "Step  94900: eval  CrossEntropyLoss |  4.82633448\n",
            "Step  94900: eval          Accuracy |  0.24757282\n",
            "\n",
            "Step  95000: Ran 100 train steps in 47.47 secs\n",
            "Step  95000: train CrossEntropyLoss |  4.50588560\n",
            "Step  95000: eval  CrossEntropyLoss |  4.13109159\n",
            "Step  95000: eval          Accuracy |  0.32631579\n",
            "\n",
            "Step  95100: Ran 100 train steps in 47.34 secs\n",
            "Step  95100: train CrossEntropyLoss |  4.52354097\n",
            "Step  95100: eval  CrossEntropyLoss |  4.94818687\n",
            "Step  95100: eval          Accuracy |  0.24509805\n",
            "\n",
            "Step  95200: Ran 100 train steps in 47.32 secs\n",
            "Step  95200: train CrossEntropyLoss |  4.44753838\n",
            "Step  95200: eval  CrossEntropyLoss |  5.22993946\n",
            "Step  95200: eval          Accuracy |  0.23893805\n",
            "\n",
            "Step  95300: Ran 100 train steps in 47.62 secs\n",
            "Step  95300: train CrossEntropyLoss |  4.49209404\n",
            "Step  95300: eval  CrossEntropyLoss |  4.69708252\n",
            "Step  95300: eval          Accuracy |  0.25500000\n",
            "\n",
            "Step  95400: Ran 100 train steps in 47.37 secs\n",
            "Step  95400: train CrossEntropyLoss |  4.49241734\n",
            "Step  95400: eval  CrossEntropyLoss |  4.61472702\n",
            "Step  95400: eval          Accuracy |  0.28703704\n",
            "\n",
            "Step  95500: Ran 100 train steps in 47.37 secs\n",
            "Step  95500: train CrossEntropyLoss |  4.47093630\n",
            "Step  95500: eval  CrossEntropyLoss |  4.84747744\n",
            "Step  95500: eval          Accuracy |  0.20388350\n",
            "\n",
            "Step  95600: Ran 100 train steps in 47.39 secs\n",
            "Step  95600: train CrossEntropyLoss |  4.47532225\n",
            "Step  95600: eval  CrossEntropyLoss |  4.23494577\n",
            "Step  95600: eval          Accuracy |  0.28448275\n",
            "\n",
            "Step  95700: Ran 100 train steps in 47.48 secs\n",
            "Step  95700: train CrossEntropyLoss |  4.49219179\n",
            "Step  95700: eval  CrossEntropyLoss |  4.34312439\n",
            "Step  95700: eval          Accuracy |  0.26605505\n",
            "\n",
            "Step  95800: Ran 100 train steps in 47.55 secs\n",
            "Step  95800: train CrossEntropyLoss |  4.48154449\n",
            "Step  95800: eval  CrossEntropyLoss |  4.88881445\n",
            "Step  95800: eval          Accuracy |  0.19444445\n",
            "\n",
            "Step  95900: Ran 100 train steps in 47.53 secs\n",
            "Step  95900: train CrossEntropyLoss |  4.47020054\n",
            "Step  95900: eval  CrossEntropyLoss |  4.92317438\n",
            "Step  95900: eval          Accuracy |  0.25409839\n",
            "\n",
            "Step  96000: Ran 100 train steps in 47.42 secs\n",
            "Step  96000: train CrossEntropyLoss |  4.41719961\n",
            "Step  96000: eval  CrossEntropyLoss |  5.11501789\n",
            "Step  96000: eval          Accuracy |  0.23364486\n",
            "\n",
            "Step  96100: Ran 100 train steps in 47.56 secs\n",
            "Step  96100: train CrossEntropyLoss |  4.51830244\n",
            "Step  96100: eval  CrossEntropyLoss |  4.42673588\n",
            "Step  96100: eval          Accuracy |  0.28571427\n",
            "\n",
            "Step  96200: Ran 100 train steps in 47.66 secs\n",
            "Step  96200: train CrossEntropyLoss |  4.41839790\n",
            "Step  96200: eval  CrossEntropyLoss |  4.78406096\n",
            "Step  96200: eval          Accuracy |  0.30081302\n",
            "\n",
            "Step  96300: Ran 100 train steps in 47.80 secs\n",
            "Step  96300: train CrossEntropyLoss |  4.44796467\n",
            "Step  96300: eval  CrossEntropyLoss |  4.25129700\n",
            "Step  96300: eval          Accuracy |  0.27272728\n",
            "\n",
            "Step  96400: Ran 100 train steps in 47.84 secs\n",
            "Step  96400: train CrossEntropyLoss |  4.50279760\n",
            "Step  96400: eval  CrossEntropyLoss |  4.38447142\n",
            "Step  96400: eval          Accuracy |  0.30303031\n",
            "\n",
            "Step  96500: Ran 100 train steps in 47.53 secs\n",
            "Step  96500: train CrossEntropyLoss |  4.47597027\n",
            "Step  96500: eval  CrossEntropyLoss |  5.11295938\n",
            "Step  96500: eval          Accuracy |  0.21153846\n",
            "\n",
            "Step  96600: Ran 100 train steps in 47.67 secs\n",
            "Step  96600: train CrossEntropyLoss |  4.41140938\n",
            "Step  96600: eval  CrossEntropyLoss |  4.64819336\n",
            "Step  96600: eval          Accuracy |  0.31067961\n",
            "\n",
            "Step  96700: Ran 100 train steps in 47.62 secs\n",
            "Step  96700: train CrossEntropyLoss |  4.46781778\n",
            "Step  96700: eval  CrossEntropyLoss |  4.64179850\n",
            "Step  96700: eval          Accuracy |  0.25961539\n",
            "\n",
            "Step  96800: Ran 100 train steps in 47.68 secs\n",
            "Step  96800: train CrossEntropyLoss |  4.40001202\n",
            "Step  96800: eval  CrossEntropyLoss |  4.87135887\n",
            "Step  96800: eval          Accuracy |  0.18691587\n",
            "\n",
            "Step  96900: Ran 100 train steps in 47.55 secs\n",
            "Step  96900: train CrossEntropyLoss |  4.45799541\n",
            "Step  96900: eval  CrossEntropyLoss |  4.67432308\n",
            "Step  96900: eval          Accuracy |  0.23979591\n",
            "\n",
            "Step  97000: Ran 100 train steps in 47.76 secs\n",
            "Step  97000: train CrossEntropyLoss |  4.42757273\n",
            "Step  97000: eval  CrossEntropyLoss |  4.52297449\n",
            "Step  97000: eval          Accuracy |  0.28282827\n",
            "\n",
            "Step  97100: Ran 100 train steps in 47.74 secs\n",
            "Step  97100: train CrossEntropyLoss |  4.40552092\n",
            "Step  97100: eval  CrossEntropyLoss |  4.12765217\n",
            "Step  97100: eval          Accuracy |  0.28723404\n",
            "\n",
            "Step  97200: Ran 100 train steps in 47.67 secs\n",
            "Step  97200: train CrossEntropyLoss |  4.52955055\n",
            "Step  97200: eval  CrossEntropyLoss |  4.26511621\n",
            "Step  97200: eval          Accuracy |  0.33333334\n",
            "\n",
            "Step  97300: Ran 100 train steps in 47.78 secs\n",
            "Step  97300: train CrossEntropyLoss |  4.46582890\n",
            "Step  97300: eval  CrossEntropyLoss |  4.18539524\n",
            "Step  97300: eval          Accuracy |  0.29545456\n",
            "\n",
            "Step  97400: Ran 100 train steps in 47.63 secs\n",
            "Step  97400: train CrossEntropyLoss |  4.41443586\n",
            "Step  97400: eval  CrossEntropyLoss |  4.15707302\n",
            "Step  97400: eval          Accuracy |  0.33980584\n",
            "\n",
            "Step  97500: Ran 100 train steps in 47.79 secs\n",
            "Step  97500: train CrossEntropyLoss |  4.45701838\n",
            "Step  97500: eval  CrossEntropyLoss |  3.85035205\n",
            "Step  97500: eval          Accuracy |  0.37500000\n",
            "\n",
            "Step  97600: Ran 100 train steps in 47.66 secs\n",
            "Step  97600: train CrossEntropyLoss |  4.39180946\n",
            "Step  97600: eval  CrossEntropyLoss |  4.62806129\n",
            "Step  97600: eval          Accuracy |  0.23809525\n",
            "\n",
            "Step  97700: Ran 100 train steps in 47.73 secs\n",
            "Step  97700: train CrossEntropyLoss |  4.45768261\n",
            "Step  97700: eval  CrossEntropyLoss |  4.74730444\n",
            "Step  97700: eval          Accuracy |  0.23364486\n",
            "\n",
            "Step  97800: Ran 100 train steps in 47.77 secs\n",
            "Step  97800: train CrossEntropyLoss |  4.43241739\n",
            "Step  97800: eval  CrossEntropyLoss |  4.47211742\n",
            "Step  97800: eval          Accuracy |  0.30303031\n",
            "\n",
            "Step  97900: Ran 100 train steps in 47.66 secs\n",
            "Step  97900: train CrossEntropyLoss |  4.40409422\n",
            "Step  97900: eval  CrossEntropyLoss |  4.63162994\n",
            "Step  97900: eval          Accuracy |  0.25454545\n",
            "\n",
            "Step  98000: Ran 100 train steps in 47.70 secs\n",
            "Step  98000: train CrossEntropyLoss |  4.40396118\n",
            "Step  98000: eval  CrossEntropyLoss |  4.33201838\n",
            "Step  98000: eval          Accuracy |  0.30701753\n",
            "\n",
            "Step  98100: Ran 100 train steps in 47.75 secs\n",
            "Step  98100: train CrossEntropyLoss |  4.39668798\n",
            "Step  98100: eval  CrossEntropyLoss |  4.39474392\n",
            "Step  98100: eval          Accuracy |  0.29999998\n",
            "\n",
            "Step  98200: Ran 100 train steps in 47.82 secs\n",
            "Step  98200: train CrossEntropyLoss |  4.42507410\n",
            "Step  98200: eval  CrossEntropyLoss |  4.70587444\n",
            "Step  98200: eval          Accuracy |  0.29464287\n",
            "\n",
            "Step  98300: Ran 100 train steps in 47.74 secs\n",
            "Step  98300: train CrossEntropyLoss |  4.46235895\n",
            "Step  98300: eval  CrossEntropyLoss |  4.55545521\n",
            "Step  98300: eval          Accuracy |  0.26940641\n",
            "\n",
            "Step  98400: Ran 100 train steps in 47.60 secs\n",
            "Step  98400: train CrossEntropyLoss |  4.40608549\n",
            "Step  98400: eval  CrossEntropyLoss |  4.64041996\n",
            "Step  98400: eval          Accuracy |  0.25225225\n",
            "\n",
            "Step  98500: Ran 100 train steps in 47.55 secs\n",
            "Step  98500: train CrossEntropyLoss |  4.45227814\n",
            "Step  98500: eval  CrossEntropyLoss |  4.28898430\n",
            "Step  98500: eval          Accuracy |  0.29357797\n",
            "\n",
            "Step  98600: Ran 100 train steps in 47.68 secs\n",
            "Step  98600: train CrossEntropyLoss |  4.40783596\n",
            "Step  98600: eval  CrossEntropyLoss |  4.10115814\n",
            "Step  98600: eval          Accuracy |  0.30769232\n",
            "\n",
            "Step  98700: Ran 100 train steps in 47.92 secs\n",
            "Step  98700: train CrossEntropyLoss |  4.46570826\n",
            "Step  98700: eval  CrossEntropyLoss |  4.21083355\n",
            "Step  98700: eval          Accuracy |  0.26455027\n",
            "\n",
            "Step  98800: Ran 100 train steps in 47.70 secs\n",
            "Step  98800: train CrossEntropyLoss |  4.49366808\n",
            "Step  98800: eval  CrossEntropyLoss |  4.06143045\n",
            "Step  98800: eval          Accuracy |  0.25531915\n",
            "\n",
            "Step  98900: Ran 100 train steps in 47.73 secs\n",
            "Step  98900: train CrossEntropyLoss |  4.45755434\n",
            "Step  98900: eval  CrossEntropyLoss |  4.74471092\n",
            "Step  98900: eval          Accuracy |  0.21698114\n",
            "\n",
            "Step  99000: Ran 100 train steps in 47.53 secs\n",
            "Step  99000: train CrossEntropyLoss |  4.44813299\n",
            "Step  99000: eval  CrossEntropyLoss |  4.69375610\n",
            "Step  99000: eval          Accuracy |  0.19607843\n",
            "\n",
            "Step  99100: Ran 100 train steps in 47.74 secs\n",
            "Step  99100: train CrossEntropyLoss |  4.40752745\n",
            "Step  99100: eval  CrossEntropyLoss |  4.62053442\n",
            "Step  99100: eval          Accuracy |  0.26470590\n",
            "\n",
            "Step  99200: Ran 100 train steps in 48.00 secs\n",
            "Step  99200: train CrossEntropyLoss |  4.41081333\n",
            "Step  99200: eval  CrossEntropyLoss |  4.88046837\n",
            "Step  99200: eval          Accuracy |  0.23636363\n",
            "\n",
            "Step  99300: Ran 100 train steps in 47.89 secs\n",
            "Step  99300: train CrossEntropyLoss |  4.40598154\n",
            "Step  99300: eval  CrossEntropyLoss |  4.38268709\n",
            "Step  99300: eval          Accuracy |  0.31460676\n",
            "\n",
            "Step  99400: Ran 100 train steps in 47.94 secs\n",
            "Step  99400: train CrossEntropyLoss |  4.45497513\n",
            "Step  99400: eval  CrossEntropyLoss |  4.47857904\n",
            "Step  99400: eval          Accuracy |  0.27678573\n",
            "\n",
            "Step  99500: Ran 100 train steps in 48.09 secs\n",
            "Step  99500: train CrossEntropyLoss |  4.38762331\n",
            "Step  99500: eval  CrossEntropyLoss |  4.03185749\n",
            "Step  99500: eval          Accuracy |  0.31775701\n",
            "\n",
            "Step  99600: Ran 100 train steps in 47.86 secs\n",
            "Step  99600: train CrossEntropyLoss |  4.45272017\n",
            "Step  99600: eval  CrossEntropyLoss |  4.21196079\n",
            "Step  99600: eval          Accuracy |  0.29499999\n",
            "\n",
            "Step  99700: Ran 100 train steps in 48.04 secs\n",
            "Step  99700: train CrossEntropyLoss |  4.39612341\n",
            "Step  99700: eval  CrossEntropyLoss |  4.85822821\n",
            "Step  99700: eval          Accuracy |  0.18333334\n",
            "\n",
            "Step  99800: Ran 100 train steps in 47.77 secs\n",
            "Step  99800: train CrossEntropyLoss |  4.38990641\n",
            "Step  99800: eval  CrossEntropyLoss |  4.84713984\n",
            "Step  99800: eval          Accuracy |  0.26470590\n",
            "\n",
            "Step  99900: Ran 100 train steps in 47.81 secs\n",
            "Step  99900: train CrossEntropyLoss |  4.41159391\n",
            "Step  99900: eval  CrossEntropyLoss |  4.37955952\n",
            "Step  99900: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  100000: Ran 100 train steps in 47.95 secs\n",
            "Step  100000: train CrossEntropyLoss |  4.47178078\n",
            "Step  100000: eval  CrossEntropyLoss |  4.70313168\n",
            "Step  100000: eval          Accuracy |  0.25773197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "# !rsync -a /content/drive/MyDrive/model2/ ~/\r\n",
        "\r\n",
        "# copy the model to Google Drive\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model/\r\n",
        "\r\n",
        "# sync Google Drive dir with the train dir\r\n",
        "# !rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RKDL96cg-Y8"
      },
      "source": [
        "model = loop.model\r\n",
        "empty_state = model.state"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e95fba-abb1-4881-b71e-991f4aab5281"
      },
      "source": [
        "train_article = train_text_pairs[5][0]\r\n",
        "train_summary = train_text_pairs[5][1]\r\n",
        "print(wrapper.fill(train_article))\r\n",
        "print('')\r\n",
        "eval_article = eval_text_pairs[1][0]\r\n",
        "eval_summary = eval_text_pairs[1][1]\r\n",
        "print(wrapper.fill(eval_article))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые придумали новый способ взаимодействия с графеном, который\n",
            "позволяет избавиться от \"слипающихся\" листов. статья ученых появилась\n",
            "в журнале acs nano, а ее краткое изложение приводится на сайте северо-\n",
            "западного университета, сотрудники которого принимали участие в\n",
            "работе. известно, что основной трудностью при работе с графеновыми\n",
            "листами является то, что при соприкосновении они слипаются под\n",
            "воздействием сил ван-дер-ваальса между собой при наложении друг на\n",
            "друга. это приводит к потере большинства уникальных свойств материала.\n",
            "для решения подобной проблемы, например, некоторые исследователи\n",
            "кладут между листами прокладки из другого материала, однако такое\n",
            "решение часто не слишком эффективно - атомы прокладки могут\n",
            "образовывать связи с атомами углерода в графене, что снова приводит к\n",
            "появлению дефектов в материале. в рамках нового исследования ученые\n",
            "предложили использовать графен не в виде ровных листов, а в виде\n",
            "смятых в комок листов. по словам исследователей, в подобном виде\n",
            "графен ведет себя как бумажные комки в мусорной корзине - несмотря на\n",
            "достаточно плотное расположение, поверхности листов, из которых они\n",
            "состоят, не соприкасаются. расчеты показывают, что при подобной\n",
            "упаковке листов графен сохраняет около 45 процентов исходной площади\n",
            "поверхности. для сравнения, при других способах организации удается\n",
            "спасти не более 16 процентов площади. графен как теоретическая\n",
            "абстракция рассматривался еще в конце 20-х годов прошлого века.\n",
            "начиная с 1960-х годов, он выступал в качестве удобной математической\n",
            "модели для расчетов в квантовой механике. впервые графен получили на\n",
            "практике константин новоселов и андрей гейм в 2004 году.\n",
            "\n",
            "сша планируют сократить численность военного контингента в южной\n",
            "корее. по информации корейского министерства иностранных дел, к концу\n",
            "2005 года из страны будет выведена треть американского контингента,\n",
            "составляющего в настоящее время 37500 военнослужащих, сообщает\n",
            "reuters. всего к концу 2005 года страну покинут 12500 американских\n",
            "солдат. 3600 из них продолжат службу в ираке. глава корейского мид\n",
            "отметил, что сша подходят к выводу войск очень внимательно, так как\n",
            "ситуация на полуострове остается напряженной. тем не менее, сша пошли\n",
            "навстречу желанию властей южной кореи иметь более независимую армию, и\n",
            "обещают оказать им в этом всяческое содействие. собственные силы южной\n",
            "кореи составляют на сегодняшний день 690 000 человек. армия северной\n",
            "кореи насчитывает 1 100 000 военнослужащих.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3a7497-bca2-4155-a684-81d0d1364e8a"
      },
      "source": [
        "# checking first symbol generation\r\n",
        "print(detokenize([next_symbol(tokenize(train_article)+[0], model)]))\r\n",
        "print(detokenize([next_symbol(tokenize(eval_article)+[0], model)]))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые\n",
            "сша\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        \r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa5oX3s-EoVR",
        "outputId": "abe17b8b-e5fe-4a55-b20c-9800f5e4a125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenize('.')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10097, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyH4tSPOFJoz",
        "outputId": "13e6bf3b-9e63-40b3-8c25-e21ec8a968a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "detokenize([2])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' ⁇'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc70a4fe-fa8d-4103-fdd7-b97ee63a2751"
      },
      "source": [
        "print(train_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(train_article, model)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые предложили использовать графен в мятом виде\n",
            "\n",
            "\n",
            "ученые\n",
            "ученые обнаружили\n",
            "ученые обнаружили способ\n",
            "ученые обнаружили способ от\n",
            "ученые обнаружили способ отпеча\n",
            "ученые обнаружили способ отпечатки\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b02f09d-466f-45a8-b9fb-3db2d896418c"
      },
      "source": [
        "print(eval_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article, model)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша на треть сократят корейскую группировку\n",
            "\n",
            "\n",
            "сша\n",
            "сша провели\n",
            "сша провели учения\n",
            "сша провели учения в\n",
            "сша провели учения в секторе\n",
            "сша провели учения в секторе газа\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f11ac1f-a934-467a-bc7b-91e7e301d113"
      },
      "source": [
        "model.state = empty_state\r\n",
        "\r\n",
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(tokenize(eval_summary))[None, :],\r\n",
        "                                        temperature=0.2, max_length=20)\r\n",
        "print(wrapper.fill(detokenize(output[0])))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша кубометров кубометров кубометров кубометров кубометров кубометров\n",
            "кубометров кубометров кубометров кубометров кубометров кубометров\n",
            "кубометров кубометров кубометров кубометров кубометров кубометров\n",
            "кубометров\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}