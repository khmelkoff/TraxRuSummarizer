{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM4mzQ966zPv84nB8Bwa9G2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f97fa8a-5b8e-441c-83aa-b6bcb7065c73"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 15.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 16.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 59.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 56.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 57.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 44.9MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "677328f3-b090-4478-c532-5c9ec9889c58"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "# data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "# data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "# data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "# data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "# text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "# for i in tqdm(range(data.shape[0])):\r\n",
        "    # if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        # text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafd90d3-592e-44a8-d602-30424903033f"
      },
      "source": [
        "# text_pairs[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "9a443a1c-929d-46b7-a380-16c307c65131"
      },
      "source": [
        "# s0 = text_pairs[10][0]\r\n",
        "# text_list = wrapper.wrap(s0[:300])\r\n",
        "# for line in text_list:\r\n",
        "#     print(line)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLsMmUFFnHA",
        "outputId": "1f934de7-8177-4624-dbda-8ad1b7ea1aaf"
      },
      "source": [
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n",
            "в киеве исключили новый раунд минских переговоров\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "5ffaab05-4181-49c6-d7fc-1360f5d020a3"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC79r_7EPy6",
        "outputId": "3834b52c-4e02-498d-f22d-c466071cd59f"
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0a617b-1be6-4eef-a4f9-d948541bdf1a"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8314134b-35aa-4990-88ab-9f75206e2ffd"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58cd5309-1586-4f9e-a870-96fa7b378708"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966b745b-c540-47b2-9516-8899d165e1d9"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2473  6336     5  4645 15949     1     0  2917   369  2528  1820   173\n",
            "  5338     5 10050 15960    73 15960 12436     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "9f8780ec-a66e-49f7-fefb-a27a8719a83b"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "44cbbe39-d287-497d-d311-15bc5b142621"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1885,  5488, 15960,   214,   437, 13528,  9008, 15931,  5857,\n",
              "        1004,  7240,   125,   365, 15945,   183,   647, 13258,  4234,\n",
              "        1193,  2730,   173,     5,  2893,  2663,  1753, 15945,   227,\n",
              "        7186,   647, 14900,   415,  5533,  1193, 15945,  4281,  7418,\n",
              "           5,  2405,  2196, 15949,  1004,  7240,   125,   365, 15945,\n",
              "          43,  1194,   241,  4234,  1193,   657, 11166, 15916,    30,\n",
              "        9293, 15945,  1537,    75,  8729,  7623,  4281,  5751,  4891,\n",
              "        4651, 15949,    63,   226,    81,  5557, 15949,    53, 15957,\n",
              "        2901,   725, 15960,  1873,    29,   157,  2650,   154,  2732,\n",
              "         278,    25,  1214, 15940,  2500,   712, 15945,   976, 12985,\n",
              "       15945,    79,  1004,  7240,   125,   365,    78, 12986,    25,\n",
              "       13924,  2153,   970,  2663,  2196,     5,   280,  1392, 15949,\n",
              "        1985,  4772,  1236, 11245,  1501,  9542,  6812,    64,   798,\n",
              "         768,  1059,   768,  2052,    53,   673,  4178, 15924,     2,\n",
              "        1174,   158,  2757, 15945,   533,  2859,     5,     3,   182,\n",
              "          61,    97,     5,  3583,  1111,  8480,   173, 15949,  1671,\n",
              "       15945,    79,  5208,  2848,   173,  1004,  7240,   125,   365,\n",
              "          57,   348,  1739,   305,     5,  2405,  1753,   278,    25,\n",
              "        5533,  1193, 15945,   533,  5082,     5,  1829,   462,   105,\n",
              "        1038, 15940,    35, 15949,  3081,  3426,  2663,  2196,  1824,\n",
              "         559, 15941,  1779, 15935,   901,  1555, 15945,    79,  6115,\n",
              "        1038,    43,  4328, 13528,  9008,   161,   657,  6612,  1307,\n",
              "         241, 13892,   351,    52,  2399,    30,   824,   180, 15945,\n",
              "        2049,  4474,  3936,  2144,  1753, 15949,   241,  2629,  1348,\n",
              "        2383, 12179, 14700, 15363,  3120, 15916,    18,  1004,  7240,\n",
              "         125,   365,  4281,  1537,  7623,  4891,  4582, 15945,  3638,\n",
              "        7000,   368,  2273,  6447, 15934,  2112,    11,   538, 15945,\n",
              "          79,  1217,  5219,  9284,  2938,  9325,  5854, 15949,     1,\n",
              "           0,  8725,  4328, 13528,  9008, 15931,  1753,   278,  2880,\n",
              "        7503,   241, 15916,    18,  9293,     1,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "4689ff6e-148f-4552-df0e-f2425288af5d"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.00015)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.00015), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "dde22798-d33a-446c-ccdc-762c709798b0"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  60100: Ran 100 train steps in 75.42 secs\n",
            "Step  60100: train CrossEntropyLoss |  5.74634504\n",
            "Step  60100: eval  CrossEntropyLoss |  5.81962395\n",
            "Step  60100: eval          Accuracy |  0.14035088\n",
            "\n",
            "Step  60200: Ran 100 train steps in 37.33 secs\n",
            "Step  60200: train CrossEntropyLoss |  5.71195889\n",
            "Step  60200: eval  CrossEntropyLoss |  4.87589073\n",
            "Step  60200: eval          Accuracy |  0.28323698\n",
            "\n",
            "Step  60300: Ran 100 train steps in 37.10 secs\n",
            "Step  60300: train CrossEntropyLoss |  5.59981775\n",
            "Step  60300: eval  CrossEntropyLoss |  5.06067896\n",
            "Step  60300: eval          Accuracy |  0.25252524\n",
            "\n",
            "Step  60400: Ran 100 train steps in 54.77 secs\n",
            "Step  60400: train CrossEntropyLoss |  5.58086491\n",
            "Step  60400: eval  CrossEntropyLoss |  5.47390556\n",
            "Step  60400: eval          Accuracy |  0.19642858\n",
            "\n",
            "Step  60500: Ran 100 train steps in 37.11 secs\n",
            "Step  60500: train CrossEntropyLoss |  5.53092575\n",
            "Step  60500: eval  CrossEntropyLoss |  5.47379684\n",
            "Step  60500: eval          Accuracy |  0.20183486\n",
            "\n",
            "Step  60600: Ran 100 train steps in 37.10 secs\n",
            "Step  60600: train CrossEntropyLoss |  5.46442270\n",
            "Step  60600: eval  CrossEntropyLoss |  5.29006004\n",
            "Step  60600: eval          Accuracy |  0.20879121\n",
            "\n",
            "Step  60700: Ran 100 train steps in 37.47 secs\n",
            "Step  60700: train CrossEntropyLoss |  5.47223997\n",
            "Step  60700: eval  CrossEntropyLoss |  5.31366873\n",
            "Step  60700: eval          Accuracy |  0.25252524\n",
            "\n",
            "Step  60800: Ran 100 train steps in 37.44 secs\n",
            "Step  60800: train CrossEntropyLoss |  5.42652225\n",
            "Step  60800: eval  CrossEntropyLoss |  5.06797409\n",
            "Step  60800: eval          Accuracy |  0.22488040\n",
            "\n",
            "Step  60900: Ran 100 train steps in 37.65 secs\n",
            "Step  60900: train CrossEntropyLoss |  5.38418245\n",
            "Step  60900: eval  CrossEntropyLoss |  5.59240723\n",
            "Step  60900: eval          Accuracy |  0.19047619\n",
            "\n",
            "Step  61000: Ran 100 train steps in 37.65 secs\n",
            "Step  61000: train CrossEntropyLoss |  5.34827375\n",
            "Step  61000: eval  CrossEntropyLoss |  6.03657293\n",
            "Step  61000: eval          Accuracy |  0.14035088\n",
            "\n",
            "Step  61100: Ran 100 train steps in 37.73 secs\n",
            "Step  61100: train CrossEntropyLoss |  5.32628536\n",
            "Step  61100: eval  CrossEntropyLoss |  5.57944870\n",
            "Step  61100: eval          Accuracy |  0.20192309\n",
            "\n",
            "Step  61200: Ran 100 train steps in 37.73 secs\n",
            "Step  61200: train CrossEntropyLoss |  5.31511593\n",
            "Step  61200: eval  CrossEntropyLoss |  4.91685057\n",
            "Step  61200: eval          Accuracy |  0.23076925\n",
            "\n",
            "Step  61300: Ran 100 train steps in 37.80 secs\n",
            "Step  61300: train CrossEntropyLoss |  5.34345341\n",
            "Step  61300: eval  CrossEntropyLoss |  4.86280775\n",
            "Step  61300: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  61400: Ran 100 train steps in 37.72 secs\n",
            "Step  61400: train CrossEntropyLoss |  5.29996157\n",
            "Step  61400: eval  CrossEntropyLoss |  5.54606009\n",
            "Step  61400: eval          Accuracy |  0.15887851\n",
            "\n",
            "Step  61500: Ran 100 train steps in 38.01 secs\n",
            "Step  61500: train CrossEntropyLoss |  5.30920887\n",
            "Step  61500: eval  CrossEntropyLoss |  5.21832275\n",
            "Step  61500: eval          Accuracy |  0.20192309\n",
            "\n",
            "Step  61600: Ran 100 train steps in 37.85 secs\n",
            "Step  61600: train CrossEntropyLoss |  5.23401642\n",
            "Step  61600: eval  CrossEntropyLoss |  5.05457306\n",
            "Step  61600: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  61700: Ran 100 train steps in 38.12 secs\n",
            "Step  61700: train CrossEntropyLoss |  5.23364019\n",
            "Step  61700: eval  CrossEntropyLoss |  5.19093990\n",
            "Step  61700: eval          Accuracy |  0.21465969\n",
            "\n",
            "Step  61800: Ran 100 train steps in 38.23 secs\n",
            "Step  61800: train CrossEntropyLoss |  5.29642439\n",
            "Step  61800: eval  CrossEntropyLoss |  5.80892372\n",
            "Step  61800: eval          Accuracy |  0.19298247\n",
            "\n",
            "Step  61900: Ran 100 train steps in 37.94 secs\n",
            "Step  61900: train CrossEntropyLoss |  5.26774693\n",
            "Step  61900: eval  CrossEntropyLoss |  5.66142416\n",
            "Step  61900: eval          Accuracy |  0.18918920\n",
            "\n",
            "Step  62000: Ran 100 train steps in 37.94 secs\n",
            "Step  62000: train CrossEntropyLoss |  5.22671795\n",
            "Step  62000: eval  CrossEntropyLoss |  5.75435591\n",
            "Step  62000: eval          Accuracy |  0.14814815\n",
            "\n",
            "Step  62100: Ran 100 train steps in 38.06 secs\n",
            "Step  62100: train CrossEntropyLoss |  5.28645086\n",
            "Step  62100: eval  CrossEntropyLoss |  5.18390751\n",
            "Step  62100: eval          Accuracy |  0.24864866\n",
            "\n",
            "Step  62200: Ran 100 train steps in 38.03 secs\n",
            "Step  62200: train CrossEntropyLoss |  5.21035814\n",
            "Step  62200: eval  CrossEntropyLoss |  5.05922890\n",
            "Step  62200: eval          Accuracy |  0.23364486\n",
            "\n",
            "Step  62300: Ran 100 train steps in 37.92 secs\n",
            "Step  62300: train CrossEntropyLoss |  5.26615810\n",
            "Step  62300: eval  CrossEntropyLoss |  5.59635353\n",
            "Step  62300: eval          Accuracy |  0.17999999\n",
            "\n",
            "Step  62400: Ran 100 train steps in 37.97 secs\n",
            "Step  62400: train CrossEntropyLoss |  5.18086958\n",
            "Step  62400: eval  CrossEntropyLoss |  5.60216427\n",
            "Step  62400: eval          Accuracy |  0.16666667\n",
            "\n",
            "Step  62500: Ran 100 train steps in 37.81 secs\n",
            "Step  62500: train CrossEntropyLoss |  5.21945667\n",
            "Step  62500: eval  CrossEntropyLoss |  5.39140844\n",
            "Step  62500: eval          Accuracy |  0.21153846\n",
            "\n",
            "Step  62600: Ran 100 train steps in 37.81 secs\n",
            "Step  62600: train CrossEntropyLoss |  5.17082834\n",
            "Step  62600: eval  CrossEntropyLoss |  5.02091217\n",
            "Step  62600: eval          Accuracy |  0.16504854\n",
            "\n",
            "Step  62700: Ran 100 train steps in 37.47 secs\n",
            "Step  62700: train CrossEntropyLoss |  5.18648529\n",
            "Step  62700: eval  CrossEntropyLoss |  5.52419281\n",
            "Step  62700: eval          Accuracy |  0.18644068\n",
            "\n",
            "Step  62800: Ran 100 train steps in 37.69 secs\n",
            "Step  62800: train CrossEntropyLoss |  5.18440485\n",
            "Step  62800: eval  CrossEntropyLoss |  5.30925322\n",
            "Step  62800: eval          Accuracy |  0.19211823\n",
            "\n",
            "Step  62900: Ran 100 train steps in 38.02 secs\n",
            "Step  62900: train CrossEntropyLoss |  5.16358328\n",
            "Step  62900: eval  CrossEntropyLoss |  6.15594530\n",
            "Step  62900: eval          Accuracy |  0.22727272\n",
            "\n",
            "Step  63000: Ran 100 train steps in 38.09 secs\n",
            "Step  63000: train CrossEntropyLoss |  5.17468548\n",
            "Step  63000: eval  CrossEntropyLoss |  4.93130350\n",
            "Step  63000: eval          Accuracy |  0.22549020\n",
            "\n",
            "Step  63100: Ran 100 train steps in 37.95 secs\n",
            "Step  63100: train CrossEntropyLoss |  5.09061146\n",
            "Step  63100: eval  CrossEntropyLoss |  5.13535690\n",
            "Step  63100: eval          Accuracy |  0.21153846\n",
            "\n",
            "Step  63200: Ran 100 train steps in 37.93 secs\n",
            "Step  63200: train CrossEntropyLoss |  5.16486979\n",
            "Step  63200: eval  CrossEntropyLoss |  5.10972166\n",
            "Step  63200: eval          Accuracy |  0.20212765\n",
            "\n",
            "Step  63300: Ran 100 train steps in 38.03 secs\n",
            "Step  63300: train CrossEntropyLoss |  5.17311573\n",
            "Step  63300: eval  CrossEntropyLoss |  4.97440243\n",
            "Step  63300: eval          Accuracy |  0.23853210\n",
            "\n",
            "Step  63400: Ran 100 train steps in 38.03 secs\n",
            "Step  63400: train CrossEntropyLoss |  5.12604189\n",
            "Step  63400: eval  CrossEntropyLoss |  5.10282326\n",
            "Step  63400: eval          Accuracy |  0.20999999\n",
            "\n",
            "Step  63500: Ran 100 train steps in 38.08 secs\n",
            "Step  63500: train CrossEntropyLoss |  5.16124821\n",
            "Step  63500: eval  CrossEntropyLoss |  5.16147375\n",
            "Step  63500: eval          Accuracy |  0.23931625\n",
            "\n",
            "Step  63600: Ran 100 train steps in 38.37 secs\n",
            "Step  63600: train CrossEntropyLoss |  5.09458494\n",
            "Step  63600: eval  CrossEntropyLoss |  5.23264837\n",
            "Step  63600: eval          Accuracy |  0.27083334\n",
            "\n",
            "Step  63700: Ran 100 train steps in 38.03 secs\n",
            "Step  63700: train CrossEntropyLoss |  5.12340879\n",
            "Step  63700: eval  CrossEntropyLoss |  5.09324598\n",
            "Step  63700: eval          Accuracy |  0.23636363\n",
            "\n",
            "Step  63800: Ran 100 train steps in 38.01 secs\n",
            "Step  63800: train CrossEntropyLoss |  5.08815718\n",
            "Step  63800: eval  CrossEntropyLoss |  4.82203388\n",
            "Step  63800: eval          Accuracy |  0.27722773\n",
            "\n",
            "Step  63900: Ran 100 train steps in 38.19 secs\n",
            "Step  63900: train CrossEntropyLoss |  5.10768604\n",
            "Step  63900: eval  CrossEntropyLoss |  5.04507542\n",
            "Step  63900: eval          Accuracy |  0.15625000\n",
            "\n",
            "Step  64000: Ran 100 train steps in 37.94 secs\n",
            "Step  64000: train CrossEntropyLoss |  5.14713955\n",
            "Step  64000: eval  CrossEntropyLoss |  4.94308138\n",
            "Step  64000: eval          Accuracy |  0.22641510\n",
            "\n",
            "Step  64100: Ran 100 train steps in 38.18 secs\n",
            "Step  64100: train CrossEntropyLoss |  5.06149101\n",
            "Step  64100: eval  CrossEntropyLoss |  5.28219271\n",
            "Step  64100: eval          Accuracy |  0.22689077\n",
            "\n",
            "Step  64200: Ran 100 train steps in 37.74 secs\n",
            "Step  64200: train CrossEntropyLoss |  5.07535458\n",
            "Step  64200: eval  CrossEntropyLoss |  4.36078644\n",
            "Step  64200: eval          Accuracy |  0.31958765\n",
            "\n",
            "Step  64300: Ran 100 train steps in 37.72 secs\n",
            "Step  64300: train CrossEntropyLoss |  5.08804274\n",
            "Step  64300: eval  CrossEntropyLoss |  5.25463486\n",
            "Step  64300: eval          Accuracy |  0.15315315\n",
            "\n",
            "Step  64400: Ran 100 train steps in 37.84 secs\n",
            "Step  64400: train CrossEntropyLoss |  5.09728670\n",
            "Step  64400: eval  CrossEntropyLoss |  5.21960926\n",
            "Step  64400: eval          Accuracy |  0.19191919\n",
            "\n",
            "Step  64500: Ran 100 train steps in 37.77 secs\n",
            "Step  64500: train CrossEntropyLoss |  5.07586002\n",
            "Step  64500: eval  CrossEntropyLoss |  4.85858202\n",
            "Step  64500: eval          Accuracy |  0.24401915\n",
            "\n",
            "Step  64600: Ran 100 train steps in 37.76 secs\n",
            "Step  64600: train CrossEntropyLoss |  5.01896715\n",
            "Step  64600: eval  CrossEntropyLoss |  4.59206200\n",
            "Step  64600: eval          Accuracy |  0.25892860\n",
            "\n",
            "Step  64700: Ran 100 train steps in 37.90 secs\n",
            "Step  64700: train CrossEntropyLoss |  5.09055376\n",
            "Step  64700: eval  CrossEntropyLoss |  4.99441051\n",
            "Step  64700: eval          Accuracy |  0.21719459\n",
            "\n",
            "Step  64800: Ran 100 train steps in 37.79 secs\n",
            "Step  64800: train CrossEntropyLoss |  5.08545399\n",
            "Step  64800: eval  CrossEntropyLoss |  5.61153221\n",
            "Step  64800: eval          Accuracy |  0.16363636\n",
            "\n",
            "Step  64900: Ran 100 train steps in 37.97 secs\n",
            "Step  64900: train CrossEntropyLoss |  5.06494045\n",
            "Step  64900: eval  CrossEntropyLoss |  5.48365021\n",
            "Step  64900: eval          Accuracy |  0.17543860\n",
            "\n",
            "Step  65000: Ran 100 train steps in 38.12 secs\n",
            "Step  65000: train CrossEntropyLoss |  5.02869749\n",
            "Step  65000: eval  CrossEntropyLoss |  5.38197708\n",
            "Step  65000: eval          Accuracy |  0.18446602\n",
            "\n",
            "Step  65100: Ran 100 train steps in 38.14 secs\n",
            "Step  65100: train CrossEntropyLoss |  5.04407167\n",
            "Step  65100: eval  CrossEntropyLoss |  5.00571251\n",
            "Step  65100: eval          Accuracy |  0.23300971\n",
            "\n",
            "Step  65200: Ran 100 train steps in 38.31 secs\n",
            "Step  65200: train CrossEntropyLoss |  5.06214762\n",
            "Step  65200: eval  CrossEntropyLoss |  4.84037781\n",
            "Step  65200: eval          Accuracy |  0.21818182\n",
            "\n",
            "Step  65300: Ran 100 train steps in 37.99 secs\n",
            "Step  65300: train CrossEntropyLoss |  5.03679848\n",
            "Step  65300: eval  CrossEntropyLoss |  5.04592323\n",
            "Step  65300: eval          Accuracy |  0.24509805\n",
            "\n",
            "Step  65400: Ran 100 train steps in 37.71 secs\n",
            "Step  65400: train CrossEntropyLoss |  5.05033922\n",
            "Step  65400: eval  CrossEntropyLoss |  4.96538353\n",
            "Step  65400: eval          Accuracy |  0.20833334\n",
            "\n",
            "Step  65500: Ran 100 train steps in 38.02 secs\n",
            "Step  65500: train CrossEntropyLoss |  5.02438259\n",
            "Step  65500: eval  CrossEntropyLoss |  4.91316080\n",
            "Step  65500: eval          Accuracy |  0.24509805\n",
            "\n",
            "Step  65600: Ran 100 train steps in 38.83 secs\n",
            "Step  65600: train CrossEntropyLoss |  5.09900618\n",
            "Step  65600: eval  CrossEntropyLoss |  4.50775433\n",
            "Step  65600: eval          Accuracy |  0.28712872\n",
            "\n",
            "Step  65700: Ran 100 train steps in 37.71 secs\n",
            "Step  65700: train CrossEntropyLoss |  5.00085735\n",
            "Step  65700: eval  CrossEntropyLoss |  5.43968105\n",
            "Step  65700: eval          Accuracy |  0.21428572\n",
            "\n",
            "Step  65800: Ran 100 train steps in 37.98 secs\n",
            "Step  65800: train CrossEntropyLoss |  5.03234673\n",
            "Step  65800: eval  CrossEntropyLoss |  4.67105150\n",
            "Step  65800: eval          Accuracy |  0.25274727\n",
            "\n",
            "Step  65900: Ran 100 train steps in 37.81 secs\n",
            "Step  65900: train CrossEntropyLoss |  5.05992985\n",
            "Step  65900: eval  CrossEntropyLoss |  5.23282433\n",
            "Step  65900: eval          Accuracy |  0.19626167\n",
            "\n",
            "Step  66000: Ran 100 train steps in 38.07 secs\n",
            "Step  66000: train CrossEntropyLoss |  5.04652500\n",
            "Step  66000: eval  CrossEntropyLoss |  4.93525362\n",
            "Step  66000: eval          Accuracy |  0.22115386\n",
            "\n",
            "Step  66100: Ran 100 train steps in 37.04 secs\n",
            "Step  66100: train CrossEntropyLoss |  5.05234909\n",
            "Step  66100: eval  CrossEntropyLoss |  4.07394218\n",
            "Step  66100: eval          Accuracy |  0.29166669\n",
            "\n",
            "Step  66200: Ran 100 train steps in 37.50 secs\n",
            "Step  66200: train CrossEntropyLoss |  5.04451084\n",
            "Step  66200: eval  CrossEntropyLoss |  4.83689499\n",
            "Step  66200: eval          Accuracy |  0.21929824\n",
            "\n",
            "Step  66300: Ran 100 train steps in 37.85 secs\n",
            "Step  66300: train CrossEntropyLoss |  5.07290173\n",
            "Step  66300: eval  CrossEntropyLoss |  5.12201929\n",
            "Step  66300: eval          Accuracy |  0.20720722\n",
            "\n",
            "Step  66400: Ran 100 train steps in 38.40 secs\n",
            "Step  66400: train CrossEntropyLoss |  4.95046425\n",
            "Step  66400: eval  CrossEntropyLoss |  4.77805710\n",
            "Step  66400: eval          Accuracy |  0.21904762\n",
            "\n",
            "Step  66500: Ran 100 train steps in 37.99 secs\n",
            "Step  66500: train CrossEntropyLoss |  4.94129181\n",
            "Step  66500: eval  CrossEntropyLoss |  4.68505669\n",
            "Step  66500: eval          Accuracy |  0.24000001\n",
            "\n",
            "Step  66600: Ran 100 train steps in 37.91 secs\n",
            "Step  66600: train CrossEntropyLoss |  4.96663427\n",
            "Step  66600: eval  CrossEntropyLoss |  5.51252508\n",
            "Step  66600: eval          Accuracy |  0.13333334\n",
            "\n",
            "Step  66700: Ran 100 train steps in 37.66 secs\n",
            "Step  66700: train CrossEntropyLoss |  5.01569223\n",
            "Step  66700: eval  CrossEntropyLoss |  5.05346346\n",
            "Step  66700: eval          Accuracy |  0.16190477\n",
            "\n",
            "Step  66800: Ran 100 train steps in 37.83 secs\n",
            "Step  66800: train CrossEntropyLoss |  4.97283792\n",
            "Step  66800: eval  CrossEntropyLoss |  5.49557400\n",
            "Step  66800: eval          Accuracy |  0.15053764\n",
            "\n",
            "Step  66900: Ran 100 train steps in 37.72 secs\n",
            "Step  66900: train CrossEntropyLoss |  5.03847599\n",
            "Step  66900: eval  CrossEntropyLoss |  5.01928663\n",
            "Step  66900: eval          Accuracy |  0.22727273\n",
            "\n",
            "Step  67000: Ran 100 train steps in 37.75 secs\n",
            "Step  67000: train CrossEntropyLoss |  4.98227024\n",
            "Step  67000: eval  CrossEntropyLoss |  5.74390316\n",
            "Step  67000: eval          Accuracy |  0.16129032\n",
            "\n",
            "Step  67100: Ran 100 train steps in 37.65 secs\n",
            "Step  67100: train CrossEntropyLoss |  4.99884272\n",
            "Step  67100: eval  CrossEntropyLoss |  5.05967426\n",
            "Step  67100: eval          Accuracy |  0.17000000\n",
            "\n",
            "Step  67200: Ran 100 train steps in 37.63 secs\n",
            "Step  67200: train CrossEntropyLoss |  5.03203297\n",
            "Step  67200: eval  CrossEntropyLoss |  5.16563463\n",
            "Step  67200: eval          Accuracy |  0.23809525\n",
            "\n",
            "Step  67300: Ran 100 train steps in 37.67 secs\n",
            "Step  67300: train CrossEntropyLoss |  4.98413849\n",
            "Step  67300: eval  CrossEntropyLoss |  5.63241768\n",
            "Step  67300: eval          Accuracy |  0.18390805\n",
            "\n",
            "Step  67400: Ran 100 train steps in 37.83 secs\n",
            "Step  67400: train CrossEntropyLoss |  4.95713377\n",
            "Step  67400: eval  CrossEntropyLoss |  5.10586119\n",
            "Step  67400: eval          Accuracy |  0.24561404\n",
            "\n",
            "Step  67500: Ran 100 train steps in 37.79 secs\n",
            "Step  67500: train CrossEntropyLoss |  4.94040108\n",
            "Step  67500: eval  CrossEntropyLoss |  4.70642662\n",
            "Step  67500: eval          Accuracy |  0.26267281\n",
            "\n",
            "Step  67600: Ran 100 train steps in 37.98 secs\n",
            "Step  67600: train CrossEntropyLoss |  5.01310253\n",
            "Step  67600: eval  CrossEntropyLoss |  4.68190241\n",
            "Step  67600: eval          Accuracy |  0.22302157\n",
            "\n",
            "Step  67700: Ran 100 train steps in 38.26 secs\n",
            "Step  67700: train CrossEntropyLoss |  4.97596169\n",
            "Step  67700: eval  CrossEntropyLoss |  4.75388765\n",
            "Step  67700: eval          Accuracy |  0.21153846\n",
            "\n",
            "Step  67800: Ran 100 train steps in 38.10 secs\n",
            "Step  67800: train CrossEntropyLoss |  5.00382614\n",
            "Step  67800: eval  CrossEntropyLoss |  4.78751421\n",
            "Step  67800: eval          Accuracy |  0.25555557\n",
            "\n",
            "Step  67900: Ran 100 train steps in 38.12 secs\n",
            "Step  67900: train CrossEntropyLoss |  4.99416780\n",
            "Step  67900: eval  CrossEntropyLoss |  5.10254812\n",
            "Step  67900: eval          Accuracy |  0.23696683\n",
            "\n",
            "Step  68000: Ran 100 train steps in 38.04 secs\n",
            "Step  68000: train CrossEntropyLoss |  4.98626232\n",
            "Step  68000: eval  CrossEntropyLoss |  5.44438076\n",
            "Step  68000: eval          Accuracy |  0.14782608\n",
            "\n",
            "Step  68100: Ran 100 train steps in 37.87 secs\n",
            "Step  68100: train CrossEntropyLoss |  4.97194862\n",
            "Step  68100: eval  CrossEntropyLoss |  5.34975719\n",
            "Step  68100: eval          Accuracy |  0.20833334\n",
            "\n",
            "Step  68200: Ran 100 train steps in 37.96 secs\n",
            "Step  68200: train CrossEntropyLoss |  4.96069098\n",
            "Step  68200: eval  CrossEntropyLoss |  5.52728415\n",
            "Step  68200: eval          Accuracy |  0.18396227\n",
            "\n",
            "Step  68300: Ran 100 train steps in 38.00 secs\n",
            "Step  68300: train CrossEntropyLoss |  4.92888117\n",
            "Step  68300: eval  CrossEntropyLoss |  4.90806723\n",
            "Step  68300: eval          Accuracy |  0.23214287\n",
            "\n",
            "Step  68400: Ran 100 train steps in 37.70 secs\n",
            "Step  68400: train CrossEntropyLoss |  5.00015163\n",
            "Step  68400: eval  CrossEntropyLoss |  4.82631207\n",
            "Step  68400: eval          Accuracy |  0.21495326\n",
            "\n",
            "Step  68500: Ran 100 train steps in 37.99 secs\n",
            "Step  68500: train CrossEntropyLoss |  5.00401926\n",
            "Step  68500: eval  CrossEntropyLoss |  5.58917189\n",
            "Step  68500: eval          Accuracy |  0.18181819\n",
            "\n",
            "Step  68600: Ran 100 train steps in 37.86 secs\n",
            "Step  68600: train CrossEntropyLoss |  4.99351835\n",
            "Step  68600: eval  CrossEntropyLoss |  4.58639050\n",
            "Step  68600: eval          Accuracy |  0.28787878\n",
            "\n",
            "Step  68700: Ran 100 train steps in 37.86 secs\n",
            "Step  68700: train CrossEntropyLoss |  4.89230824\n",
            "Step  68700: eval  CrossEntropyLoss |  5.18996382\n",
            "Step  68700: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  68800: Ran 100 train steps in 37.95 secs\n",
            "Step  68800: train CrossEntropyLoss |  5.00539875\n",
            "Step  68800: eval  CrossEntropyLoss |  5.22592688\n",
            "Step  68800: eval          Accuracy |  0.19148935\n",
            "\n",
            "Step  68900: Ran 100 train steps in 38.07 secs\n",
            "Step  68900: train CrossEntropyLoss |  4.95868206\n",
            "Step  68900: eval  CrossEntropyLoss |  4.55761719\n",
            "Step  68900: eval          Accuracy |  0.21875000\n",
            "\n",
            "Step  69000: Ran 100 train steps in 38.46 secs\n",
            "Step  69000: train CrossEntropyLoss |  4.93909931\n",
            "Step  69000: eval  CrossEntropyLoss |  5.45222902\n",
            "Step  69000: eval          Accuracy |  0.16535433\n",
            "\n",
            "Step  69100: Ran 100 train steps in 38.17 secs\n",
            "Step  69100: train CrossEntropyLoss |  4.93638659\n",
            "Step  69100: eval  CrossEntropyLoss |  5.21510983\n",
            "Step  69100: eval          Accuracy |  0.21495326\n",
            "\n",
            "Step  69200: Ran 100 train steps in 38.03 secs\n",
            "Step  69200: train CrossEntropyLoss |  4.99428463\n",
            "Step  69200: eval  CrossEntropyLoss |  4.66495752\n",
            "Step  69200: eval          Accuracy |  0.26495728\n",
            "\n",
            "Step  69300: Ran 100 train steps in 37.90 secs\n",
            "Step  69300: train CrossEntropyLoss |  4.93584538\n",
            "Step  69300: eval  CrossEntropyLoss |  5.03133392\n",
            "Step  69300: eval          Accuracy |  0.20792079\n",
            "\n",
            "Step  69400: Ran 100 train steps in 38.03 secs\n",
            "Step  69400: train CrossEntropyLoss |  4.93327665\n",
            "Step  69400: eval  CrossEntropyLoss |  5.07407665\n",
            "Step  69400: eval          Accuracy |  0.23913044\n",
            "\n",
            "Step  69500: Ran 100 train steps in 37.65 secs\n",
            "Step  69500: train CrossEntropyLoss |  4.94472361\n",
            "Step  69500: eval  CrossEntropyLoss |  4.81537390\n",
            "Step  69500: eval          Accuracy |  0.25999999\n",
            "\n",
            "Step  69600: Ran 100 train steps in 37.90 secs\n",
            "Step  69600: train CrossEntropyLoss |  4.95310020\n",
            "Step  69600: eval  CrossEntropyLoss |  4.46121168\n",
            "Step  69600: eval          Accuracy |  0.28318584\n",
            "\n",
            "Step  69700: Ran 100 train steps in 37.75 secs\n",
            "Step  69700: train CrossEntropyLoss |  4.92352295\n",
            "Step  69700: eval  CrossEntropyLoss |  5.34378910\n",
            "Step  69700: eval          Accuracy |  0.20370370\n",
            "\n",
            "Step  69800: Ran 100 train steps in 37.96 secs\n",
            "Step  69800: train CrossEntropyLoss |  4.91865206\n",
            "Step  69800: eval  CrossEntropyLoss |  4.78071356\n",
            "Step  69800: eval          Accuracy |  0.22522523\n",
            "\n",
            "Step  69900: Ran 100 train steps in 37.50 secs\n",
            "Step  69900: train CrossEntropyLoss |  4.92205238\n",
            "Step  69900: eval  CrossEntropyLoss |  4.51714849\n",
            "Step  69900: eval          Accuracy |  0.23316063\n",
            "\n",
            "Step  70000: Ran 100 train steps in 37.63 secs\n",
            "Step  70000: train CrossEntropyLoss |  4.95402622\n",
            "Step  70000: eval  CrossEntropyLoss |  4.66018391\n",
            "Step  70000: eval          Accuracy |  0.23762375\n",
            "\n",
            "Step  70100: Ran 100 train steps in 37.74 secs\n",
            "Step  70100: train CrossEntropyLoss |  4.94922829\n",
            "Step  70100: eval  CrossEntropyLoss |  4.87359428\n",
            "Step  70100: eval          Accuracy |  0.21311477\n",
            "\n",
            "Step  70200: Ran 100 train steps in 37.84 secs\n",
            "Step  70200: train CrossEntropyLoss |  4.97881842\n",
            "Step  70200: eval  CrossEntropyLoss |  4.65268660\n",
            "Step  70200: eval          Accuracy |  0.21717171\n",
            "\n",
            "Step  70300: Ran 100 train steps in 37.91 secs\n",
            "Step  70300: train CrossEntropyLoss |  4.87644529\n",
            "Step  70300: eval  CrossEntropyLoss |  5.41611385\n",
            "Step  70300: eval          Accuracy |  0.19008264\n",
            "\n",
            "Step  70400: Ran 100 train steps in 37.82 secs\n",
            "Step  70400: train CrossEntropyLoss |  4.92110300\n",
            "Step  70400: eval  CrossEntropyLoss |  5.04008770\n",
            "Step  70400: eval          Accuracy |  0.24074075\n",
            "\n",
            "Step  70500: Ran 100 train steps in 37.93 secs\n",
            "Step  70500: train CrossEntropyLoss |  4.94174290\n",
            "Step  70500: eval  CrossEntropyLoss |  4.51592207\n",
            "Step  70500: eval          Accuracy |  0.27678573\n",
            "\n",
            "Step  70600: Ran 100 train steps in 37.70 secs\n",
            "Step  70600: train CrossEntropyLoss |  4.91805220\n",
            "Step  70600: eval  CrossEntropyLoss |  4.69502735\n",
            "Step  70600: eval          Accuracy |  0.26760563\n",
            "\n",
            "Step  70700: Ran 100 train steps in 37.72 secs\n",
            "Step  70700: train CrossEntropyLoss |  4.92592955\n",
            "Step  70700: eval  CrossEntropyLoss |  4.96756029\n",
            "Step  70700: eval          Accuracy |  0.22330098\n",
            "\n",
            "Step  70800: Ran 100 train steps in 37.70 secs\n",
            "Step  70800: train CrossEntropyLoss |  4.90976572\n",
            "Step  70800: eval  CrossEntropyLoss |  5.37286282\n",
            "Step  70800: eval          Accuracy |  0.20588236\n",
            "\n",
            "Step  70900: Ran 100 train steps in 37.95 secs\n",
            "Step  70900: train CrossEntropyLoss |  4.93401146\n",
            "Step  70900: eval  CrossEntropyLoss |  4.47589540\n",
            "Step  70900: eval          Accuracy |  0.28947368\n",
            "\n",
            "Step  71000: Ran 100 train steps in 37.73 secs\n",
            "Step  71000: train CrossEntropyLoss |  4.91633368\n",
            "Step  71000: eval  CrossEntropyLoss |  5.44469023\n",
            "Step  71000: eval          Accuracy |  0.18644068\n",
            "\n",
            "Step  71100: Ran 100 train steps in 38.04 secs\n",
            "Step  71100: train CrossEntropyLoss |  4.88095284\n",
            "Step  71100: eval  CrossEntropyLoss |  4.66590118\n",
            "Step  71100: eval          Accuracy |  0.24107143\n",
            "\n",
            "Step  71200: Ran 100 train steps in 37.75 secs\n",
            "Step  71200: train CrossEntropyLoss |  4.98614073\n",
            "Step  71200: eval  CrossEntropyLoss |  4.39680958\n",
            "Step  71200: eval          Accuracy |  0.31132075\n",
            "\n",
            "Step  71300: Ran 100 train steps in 37.96 secs\n",
            "Step  71300: train CrossEntropyLoss |  4.87476254\n",
            "Step  71300: eval  CrossEntropyLoss |  4.86073589\n",
            "Step  71300: eval          Accuracy |  0.22277227\n",
            "\n",
            "Step  71400: Ran 100 train steps in 37.76 secs\n",
            "Step  71400: train CrossEntropyLoss |  4.88946867\n",
            "Step  71400: eval  CrossEntropyLoss |  4.62309217\n",
            "Step  71400: eval          Accuracy |  0.18918920\n",
            "\n",
            "Step  71500: Ran 100 train steps in 37.88 secs\n",
            "Step  71500: train CrossEntropyLoss |  4.90269136\n",
            "Step  71500: eval  CrossEntropyLoss |  4.69259453\n",
            "Step  71500: eval          Accuracy |  0.25862068\n",
            "\n",
            "Step  71600: Ran 100 train steps in 37.83 secs\n",
            "Step  71600: train CrossEntropyLoss |  4.81715393\n",
            "Step  71600: eval  CrossEntropyLoss |  4.98216629\n",
            "Step  71600: eval          Accuracy |  0.22935779\n",
            "\n",
            "Step  71700: Ran 100 train steps in 37.49 secs\n",
            "Step  71700: train CrossEntropyLoss |  4.89913416\n",
            "Step  71700: eval  CrossEntropyLoss |  4.66240072\n",
            "Step  71700: eval          Accuracy |  0.30337080\n",
            "\n",
            "Step  71800: Ran 100 train steps in 38.33 secs\n",
            "Step  71800: train CrossEntropyLoss |  4.84702015\n",
            "Step  71800: eval  CrossEntropyLoss |  4.97627592\n",
            "Step  71800: eval          Accuracy |  0.22705314\n",
            "\n",
            "Step  71900: Ran 100 train steps in 37.92 secs\n",
            "Step  71900: train CrossEntropyLoss |  4.84122849\n",
            "Step  71900: eval  CrossEntropyLoss |  5.10634756\n",
            "Step  71900: eval          Accuracy |  0.22413793\n",
            "\n",
            "Step  72000: Ran 100 train steps in 37.81 secs\n",
            "Step  72000: train CrossEntropyLoss |  4.88243103\n",
            "Step  72000: eval  CrossEntropyLoss |  4.62143326\n",
            "Step  72000: eval          Accuracy |  0.29914531\n",
            "\n",
            "Step  72100: Ran 100 train steps in 37.83 secs\n",
            "Step  72100: train CrossEntropyLoss |  4.90032530\n",
            "Step  72100: eval  CrossEntropyLoss |  4.98134899\n",
            "Step  72100: eval          Accuracy |  0.23902439\n",
            "\n",
            "Step  72200: Ran 100 train steps in 37.71 secs\n",
            "Step  72200: train CrossEntropyLoss |  4.88264465\n",
            "Step  72200: eval  CrossEntropyLoss |  4.95469141\n",
            "Step  72200: eval          Accuracy |  0.22641510\n",
            "\n",
            "Step  72300: Ran 100 train steps in 37.54 secs\n",
            "Step  72300: train CrossEntropyLoss |  4.90141582\n",
            "Step  72300: eval  CrossEntropyLoss |  5.13493252\n",
            "Step  72300: eval          Accuracy |  0.22222222\n",
            "\n",
            "Step  72400: Ran 100 train steps in 37.95 secs\n",
            "Step  72400: train CrossEntropyLoss |  4.88568830\n",
            "Step  72400: eval  CrossEntropyLoss |  4.82405186\n",
            "Step  72400: eval          Accuracy |  0.28500000\n",
            "\n",
            "Step  72500: Ran 100 train steps in 38.49 secs\n",
            "Step  72500: train CrossEntropyLoss |  4.91432810\n",
            "Step  72500: eval  CrossEntropyLoss |  5.53524303\n",
            "Step  72500: eval          Accuracy |  0.15789473\n",
            "\n",
            "Step  72600: Ran 100 train steps in 38.13 secs\n",
            "Step  72600: train CrossEntropyLoss |  4.85301828\n",
            "Step  72600: eval  CrossEntropyLoss |  4.92553759\n",
            "Step  72600: eval          Accuracy |  0.19298247\n",
            "\n",
            "Step  72700: Ran 100 train steps in 37.77 secs\n",
            "Step  72700: train CrossEntropyLoss |  4.81027508\n",
            "Step  72700: eval  CrossEntropyLoss |  5.09685755\n",
            "Step  72700: eval          Accuracy |  0.21238938\n",
            "\n",
            "Step  72800: Ran 100 train steps in 38.20 secs\n",
            "Step  72800: train CrossEntropyLoss |  4.85514832\n",
            "Step  72800: eval  CrossEntropyLoss |  5.42989969\n",
            "Step  72800: eval          Accuracy |  0.19090909\n",
            "\n",
            "Step  72900: Ran 100 train steps in 37.80 secs\n",
            "Step  72900: train CrossEntropyLoss |  4.88722467\n",
            "Step  72900: eval  CrossEntropyLoss |  4.87235355\n",
            "Step  72900: eval          Accuracy |  0.24752475\n",
            "\n",
            "Step  73000: Ran 100 train steps in 38.07 secs\n",
            "Step  73000: train CrossEntropyLoss |  4.93551731\n",
            "Step  73000: eval  CrossEntropyLoss |  4.60183287\n",
            "Step  73000: eval          Accuracy |  0.23469387\n",
            "\n",
            "Step  73100: Ran 100 train steps in 38.01 secs\n",
            "Step  73100: train CrossEntropyLoss |  4.87355995\n",
            "Step  73100: eval  CrossEntropyLoss |  4.98685837\n",
            "Step  73100: eval          Accuracy |  0.21138214\n",
            "\n",
            "Step  73200: Ran 100 train steps in 38.02 secs\n",
            "Step  73200: train CrossEntropyLoss |  4.93871593\n",
            "Step  73200: eval  CrossEntropyLoss |  5.40948486\n",
            "Step  73200: eval          Accuracy |  0.20192309\n",
            "\n",
            "Step  73300: Ran 100 train steps in 38.08 secs\n",
            "Step  73300: train CrossEntropyLoss |  4.92315197\n",
            "Step  73300: eval  CrossEntropyLoss |  5.32461739\n",
            "Step  73300: eval          Accuracy |  0.16037735\n",
            "\n",
            "Step  73400: Ran 100 train steps in 38.24 secs\n",
            "Step  73400: train CrossEntropyLoss |  4.83196354\n",
            "Step  73400: eval  CrossEntropyLoss |  4.31052208\n",
            "Step  73400: eval          Accuracy |  0.23232323\n",
            "\n",
            "Step  73500: Ran 100 train steps in 37.97 secs\n",
            "Step  73500: train CrossEntropyLoss |  4.86156130\n",
            "Step  73500: eval  CrossEntropyLoss |  4.07550812\n",
            "Step  73500: eval          Accuracy |  0.31219512\n",
            "\n",
            "Step  73600: Ran 100 train steps in 38.03 secs\n",
            "Step  73600: train CrossEntropyLoss |  4.92152309\n",
            "Step  73600: eval  CrossEntropyLoss |  4.78053474\n",
            "Step  73600: eval          Accuracy |  0.24793388\n",
            "\n",
            "Step  73700: Ran 100 train steps in 38.27 secs\n",
            "Step  73700: train CrossEntropyLoss |  4.90001678\n",
            "Step  73700: eval  CrossEntropyLoss |  5.33381891\n",
            "Step  73700: eval          Accuracy |  0.18548387\n",
            "\n",
            "Step  73800: Ran 100 train steps in 38.12 secs\n",
            "Step  73800: train CrossEntropyLoss |  4.91232586\n",
            "Step  73800: eval  CrossEntropyLoss |  4.87607050\n",
            "Step  73800: eval          Accuracy |  0.21634616\n",
            "\n",
            "Step  73900: Ran 100 train steps in 38.14 secs\n",
            "Step  73900: train CrossEntropyLoss |  4.81335115\n",
            "Step  73900: eval  CrossEntropyLoss |  4.54010248\n",
            "Step  73900: eval          Accuracy |  0.24242425\n",
            "\n",
            "Step  74000: Ran 100 train steps in 38.05 secs\n",
            "Step  74000: train CrossEntropyLoss |  4.86466074\n",
            "Step  74000: eval  CrossEntropyLoss |  5.23967552\n",
            "Step  74000: eval          Accuracy |  0.17647059\n",
            "\n",
            "Step  74100: Ran 100 train steps in 37.92 secs\n",
            "Step  74100: train CrossEntropyLoss |  4.81725168\n",
            "Step  74100: eval  CrossEntropyLoss |  4.62161160\n",
            "Step  74100: eval          Accuracy |  0.25471699\n",
            "\n",
            "Step  74200: Ran 100 train steps in 38.09 secs\n",
            "Step  74200: train CrossEntropyLoss |  4.81361008\n",
            "Step  74200: eval  CrossEntropyLoss |  5.19884014\n",
            "Step  74200: eval          Accuracy |  0.21649486\n",
            "\n",
            "Step  74300: Ran 100 train steps in 38.26 secs\n",
            "Step  74300: train CrossEntropyLoss |  4.84114170\n",
            "Step  74300: eval  CrossEntropyLoss |  5.13887978\n",
            "Step  74300: eval          Accuracy |  0.24175824\n",
            "\n",
            "Step  74400: Ran 100 train steps in 37.98 secs\n",
            "Step  74400: train CrossEntropyLoss |  4.85360909\n",
            "Step  74400: eval  CrossEntropyLoss |  4.83034611\n",
            "Step  74400: eval          Accuracy |  0.21904762\n",
            "\n",
            "Step  74500: Ran 100 train steps in 38.05 secs\n",
            "Step  74500: train CrossEntropyLoss |  4.88162231\n",
            "Step  74500: eval  CrossEntropyLoss |  5.38507891\n",
            "Step  74500: eval          Accuracy |  0.19083969\n",
            "\n",
            "Step  74600: Ran 100 train steps in 37.84 secs\n",
            "Step  74600: train CrossEntropyLoss |  4.88124990\n",
            "Step  74600: eval  CrossEntropyLoss |  5.26858616\n",
            "Step  74600: eval          Accuracy |  0.21359223\n",
            "\n",
            "Step  74700: Ran 100 train steps in 37.97 secs\n",
            "Step  74700: train CrossEntropyLoss |  4.79111862\n",
            "Step  74700: eval  CrossEntropyLoss |  4.70741796\n",
            "Step  74700: eval          Accuracy |  0.25405407\n",
            "\n",
            "Step  74800: Ran 100 train steps in 38.06 secs\n",
            "Step  74800: train CrossEntropyLoss |  4.84125376\n",
            "Step  74800: eval  CrossEntropyLoss |  5.10799170\n",
            "Step  74800: eval          Accuracy |  0.12173913\n",
            "\n",
            "Step  74900: Ran 100 train steps in 37.97 secs\n",
            "Step  74900: train CrossEntropyLoss |  4.88874006\n",
            "Step  74900: eval  CrossEntropyLoss |  5.30155516\n",
            "Step  74900: eval          Accuracy |  0.17000000\n",
            "\n",
            "Step  75000: Ran 100 train steps in 38.09 secs\n",
            "Step  75000: train CrossEntropyLoss |  4.84239101\n",
            "Step  75000: eval  CrossEntropyLoss |  4.50846672\n",
            "Step  75000: eval          Accuracy |  0.29145730\n",
            "\n",
            "Step  75100: Ran 100 train steps in 37.95 secs\n",
            "Step  75100: train CrossEntropyLoss |  4.84148216\n",
            "Step  75100: eval  CrossEntropyLoss |  4.97422695\n",
            "Step  75100: eval          Accuracy |  0.23333335\n",
            "\n",
            "Step  75200: Ran 100 train steps in 37.86 secs\n",
            "Step  75200: train CrossEntropyLoss |  4.80912876\n",
            "Step  75200: eval  CrossEntropyLoss |  5.71883869\n",
            "Step  75200: eval          Accuracy |  0.19642858\n",
            "\n",
            "Step  75300: Ran 100 train steps in 38.21 secs\n",
            "Step  75300: train CrossEntropyLoss |  4.83849335\n",
            "Step  75300: eval  CrossEntropyLoss |  4.30361986\n",
            "Step  75300: eval          Accuracy |  0.27619049\n",
            "\n",
            "Step  75400: Ran 100 train steps in 38.18 secs\n",
            "Step  75400: train CrossEntropyLoss |  4.82820320\n",
            "Step  75400: eval  CrossEntropyLoss |  5.18313837\n",
            "Step  75400: eval          Accuracy |  0.20754717\n",
            "\n",
            "Step  75500: Ran 100 train steps in 38.04 secs\n",
            "Step  75500: train CrossEntropyLoss |  4.82382202\n",
            "Step  75500: eval  CrossEntropyLoss |  4.55484200\n",
            "Step  75500: eval          Accuracy |  0.26315790\n",
            "\n",
            "Step  75600: Ran 100 train steps in 38.18 secs\n",
            "Step  75600: train CrossEntropyLoss |  4.83799076\n",
            "Step  75600: eval  CrossEntropyLoss |  5.08308649\n",
            "Step  75600: eval          Accuracy |  0.23529413\n",
            "\n",
            "Step  75700: Ran 100 train steps in 38.07 secs\n",
            "Step  75700: train CrossEntropyLoss |  4.86398077\n",
            "Step  75700: eval  CrossEntropyLoss |  4.95234537\n",
            "Step  75700: eval          Accuracy |  0.21296297\n",
            "\n",
            "Step  75800: Ran 100 train steps in 38.07 secs\n",
            "Step  75800: train CrossEntropyLoss |  4.83132267\n",
            "Step  75800: eval  CrossEntropyLoss |  4.70705700\n",
            "Step  75800: eval          Accuracy |  0.29999998\n",
            "\n",
            "Step  75900: Ran 100 train steps in 37.90 secs\n",
            "Step  75900: train CrossEntropyLoss |  4.77178144\n",
            "Step  75900: eval  CrossEntropyLoss |  4.82866573\n",
            "Step  75900: eval          Accuracy |  0.25668448\n",
            "\n",
            "Step  76000: Ran 100 train steps in 37.85 secs\n",
            "Step  76000: train CrossEntropyLoss |  4.84962606\n",
            "Step  76000: eval  CrossEntropyLoss |  4.59232950\n",
            "Step  76000: eval          Accuracy |  0.28037381\n",
            "\n",
            "Step  76100: Ran 100 train steps in 38.06 secs\n",
            "Step  76100: train CrossEntropyLoss |  4.83022785\n",
            "Step  76100: eval  CrossEntropyLoss |  5.11928177\n",
            "Step  76100: eval          Accuracy |  0.18260869\n",
            "\n",
            "Step  76200: Ran 100 train steps in 37.90 secs\n",
            "Step  76200: train CrossEntropyLoss |  4.83299637\n",
            "Step  76200: eval  CrossEntropyLoss |  5.30452967\n",
            "Step  76200: eval          Accuracy |  0.20689654\n",
            "\n",
            "Step  76300: Ran 100 train steps in 37.69 secs\n",
            "Step  76300: train CrossEntropyLoss |  4.82105303\n",
            "Step  76300: eval  CrossEntropyLoss |  4.41464949\n",
            "Step  76300: eval          Accuracy |  0.25490198\n",
            "\n",
            "Step  76400: Ran 100 train steps in 37.81 secs\n",
            "Step  76400: train CrossEntropyLoss |  4.82698393\n",
            "Step  76400: eval  CrossEntropyLoss |  4.63813972\n",
            "Step  76400: eval          Accuracy |  0.24285716\n",
            "\n",
            "Step  76500: Ran 100 train steps in 37.89 secs\n",
            "Step  76500: train CrossEntropyLoss |  4.78640413\n",
            "Step  76500: eval  CrossEntropyLoss |  5.15406942\n",
            "Step  76500: eval          Accuracy |  0.21311477\n",
            "\n",
            "Step  76600: Ran 100 train steps in 37.63 secs\n",
            "Step  76600: train CrossEntropyLoss |  4.84042072\n",
            "Step  76600: eval  CrossEntropyLoss |  4.94042826\n",
            "Step  76600: eval          Accuracy |  0.29885057\n",
            "\n",
            "Step  76700: Ran 100 train steps in 37.83 secs\n",
            "Step  76700: train CrossEntropyLoss |  4.81615019\n",
            "Step  76700: eval  CrossEntropyLoss |  5.19974232\n",
            "Step  76700: eval          Accuracy |  0.21081081\n",
            "\n",
            "Step  76800: Ran 100 train steps in 37.95 secs\n",
            "Step  76800: train CrossEntropyLoss |  4.79912138\n",
            "Step  76800: eval  CrossEntropyLoss |  5.17123413\n",
            "Step  76800: eval          Accuracy |  0.21705426\n",
            "\n",
            "Step  76900: Ran 100 train steps in 38.06 secs\n",
            "Step  76900: train CrossEntropyLoss |  4.83115721\n",
            "Step  76900: eval  CrossEntropyLoss |  4.45330000\n",
            "Step  76900: eval          Accuracy |  0.25531915\n",
            "\n",
            "Step  77000: Ran 100 train steps in 37.92 secs\n",
            "Step  77000: train CrossEntropyLoss |  4.81936646\n",
            "Step  77000: eval  CrossEntropyLoss |  4.89835596\n",
            "Step  77000: eval          Accuracy |  0.26785716\n",
            "\n",
            "Step  77100: Ran 100 train steps in 37.93 secs\n",
            "Step  77100: train CrossEntropyLoss |  4.78991365\n",
            "Step  77100: eval  CrossEntropyLoss |  4.68555832\n",
            "Step  77100: eval          Accuracy |  0.22058824\n",
            "\n",
            "Step  77200: Ran 100 train steps in 37.69 secs\n",
            "Step  77200: train CrossEntropyLoss |  4.84802246\n",
            "Step  77200: eval  CrossEntropyLoss |  5.22315598\n",
            "Step  77200: eval          Accuracy |  0.19444445\n",
            "\n",
            "Step  77300: Ran 100 train steps in 37.88 secs\n",
            "Step  77300: train CrossEntropyLoss |  4.82335138\n",
            "Step  77300: eval  CrossEntropyLoss |  4.79876184\n",
            "Step  77300: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  77400: Ran 100 train steps in 37.58 secs\n",
            "Step  77400: train CrossEntropyLoss |  4.74025917\n",
            "Step  77400: eval  CrossEntropyLoss |  4.64075899\n",
            "Step  77400: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  77500: Ran 100 train steps in 37.96 secs\n",
            "Step  77500: train CrossEntropyLoss |  4.78660917\n",
            "Step  77500: eval  CrossEntropyLoss |  4.56329107\n",
            "Step  77500: eval          Accuracy |  0.26213592\n",
            "\n",
            "Step  77600: Ran 100 train steps in 37.54 secs\n",
            "Step  77600: train CrossEntropyLoss |  4.78906441\n",
            "Step  77600: eval  CrossEntropyLoss |  4.44607925\n",
            "Step  77600: eval          Accuracy |  0.28021979\n",
            "\n",
            "Step  77700: Ran 100 train steps in 37.85 secs\n",
            "Step  77700: train CrossEntropyLoss |  4.79365492\n",
            "Step  77700: eval  CrossEntropyLoss |  4.53926706\n",
            "Step  77700: eval          Accuracy |  0.25961539\n",
            "\n",
            "Step  77800: Ran 100 train steps in 37.94 secs\n",
            "Step  77800: train CrossEntropyLoss |  4.77291298\n",
            "Step  77800: eval  CrossEntropyLoss |  5.09293175\n",
            "Step  77800: eval          Accuracy |  0.23148148\n",
            "\n",
            "Step  77900: Ran 100 train steps in 38.23 secs\n",
            "Step  77900: train CrossEntropyLoss |  4.77671909\n",
            "Step  77900: eval  CrossEntropyLoss |  4.82357693\n",
            "Step  77900: eval          Accuracy |  0.22999999\n",
            "\n",
            "Step  78000: Ran 100 train steps in 37.96 secs\n",
            "Step  78000: train CrossEntropyLoss |  4.80054569\n",
            "Step  78000: eval  CrossEntropyLoss |  4.95703840\n",
            "Step  78000: eval          Accuracy |  0.28125000\n",
            "\n",
            "Step  78100: Ran 100 train steps in 38.21 secs\n",
            "Step  78100: train CrossEntropyLoss |  4.82925510\n",
            "Step  78100: eval  CrossEntropyLoss |  4.53881598\n",
            "Step  78100: eval          Accuracy |  0.25252524\n",
            "\n",
            "Step  78200: Ran 100 train steps in 38.17 secs\n",
            "Step  78200: train CrossEntropyLoss |  4.73810291\n",
            "Step  78200: eval  CrossEntropyLoss |  4.64572811\n",
            "Step  78200: eval          Accuracy |  0.25688073\n",
            "\n",
            "Step  78300: Ran 100 train steps in 38.40 secs\n",
            "Step  78300: train CrossEntropyLoss |  4.82775784\n",
            "Step  78300: eval  CrossEntropyLoss |  4.70516062\n",
            "Step  78300: eval          Accuracy |  0.26999998\n",
            "\n",
            "Step  78400: Ran 100 train steps in 38.07 secs\n",
            "Step  78400: train CrossEntropyLoss |  4.71663475\n",
            "Step  78400: eval  CrossEntropyLoss |  4.46823597\n",
            "Step  78400: eval          Accuracy |  0.27102804\n",
            "\n",
            "Step  78500: Ran 100 train steps in 37.87 secs\n",
            "Step  78500: train CrossEntropyLoss |  4.76170015\n",
            "Step  78500: eval  CrossEntropyLoss |  4.77169609\n",
            "Step  78500: eval          Accuracy |  0.30508474\n",
            "\n",
            "Step  78600: Ran 100 train steps in 38.05 secs\n",
            "Step  78600: train CrossEntropyLoss |  4.82439327\n",
            "Step  78600: eval  CrossEntropyLoss |  5.27138090\n",
            "Step  78600: eval          Accuracy |  0.21428572\n",
            "\n",
            "Step  78700: Ran 100 train steps in 37.97 secs\n",
            "Step  78700: train CrossEntropyLoss |  4.77441740\n",
            "Step  78700: eval  CrossEntropyLoss |  4.73850822\n",
            "Step  78700: eval          Accuracy |  0.21674876\n",
            "\n",
            "Step  78800: Ran 100 train steps in 38.17 secs\n",
            "Step  78800: train CrossEntropyLoss |  4.82468939\n",
            "Step  78800: eval  CrossEntropyLoss |  4.32953262\n",
            "Step  78800: eval          Accuracy |  0.29069766\n",
            "\n",
            "Step  78900: Ran 100 train steps in 37.86 secs\n",
            "Step  78900: train CrossEntropyLoss |  4.75485945\n",
            "Step  78900: eval  CrossEntropyLoss |  4.48722553\n",
            "Step  78900: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  79000: Ran 100 train steps in 38.54 secs\n",
            "Step  79000: train CrossEntropyLoss |  4.86637163\n",
            "Step  79000: eval  CrossEntropyLoss |  4.85017157\n",
            "Step  79000: eval          Accuracy |  0.22580644\n",
            "\n",
            "Step  79100: Ran 100 train steps in 38.41 secs\n",
            "Step  79100: train CrossEntropyLoss |  4.80781937\n",
            "Step  79100: eval  CrossEntropyLoss |  4.82196426\n",
            "Step  79100: eval          Accuracy |  0.24742270\n",
            "\n",
            "Step  79200: Ran 100 train steps in 38.34 secs\n",
            "Step  79200: train CrossEntropyLoss |  4.77990675\n",
            "Step  79200: eval  CrossEntropyLoss |  5.28396797\n",
            "Step  79200: eval          Accuracy |  0.22131149\n",
            "\n",
            "Step  79300: Ran 100 train steps in 38.42 secs\n",
            "Step  79300: train CrossEntropyLoss |  4.83911085\n",
            "Step  79300: eval  CrossEntropyLoss |  5.00868797\n",
            "Step  79300: eval          Accuracy |  0.26415095\n",
            "\n",
            "Step  79400: Ran 100 train steps in 38.63 secs\n",
            "Step  79400: train CrossEntropyLoss |  4.75594330\n",
            "Step  79400: eval  CrossEntropyLoss |  4.83792639\n",
            "Step  79400: eval          Accuracy |  0.25714287\n",
            "\n",
            "Step  79500: Ran 100 train steps in 38.53 secs\n",
            "Step  79500: train CrossEntropyLoss |  4.76333046\n",
            "Step  79500: eval  CrossEntropyLoss |  4.94326878\n",
            "Step  79500: eval          Accuracy |  0.28431374\n",
            "\n",
            "Step  79600: Ran 100 train steps in 38.48 secs\n",
            "Step  79600: train CrossEntropyLoss |  4.75468016\n",
            "Step  79600: eval  CrossEntropyLoss |  4.58131361\n",
            "Step  79600: eval          Accuracy |  0.23369566\n",
            "\n",
            "Step  79700: Ran 100 train steps in 38.53 secs\n",
            "Step  79700: train CrossEntropyLoss |  4.77581072\n",
            "Step  79700: eval  CrossEntropyLoss |  5.33081865\n",
            "Step  79700: eval          Accuracy |  0.20238096\n",
            "\n",
            "Step  79800: Ran 100 train steps in 38.32 secs\n",
            "Step  79800: train CrossEntropyLoss |  4.76075506\n",
            "Step  79800: eval  CrossEntropyLoss |  4.81174564\n",
            "Step  79800: eval          Accuracy |  0.22881356\n",
            "\n",
            "Step  79900: Ran 100 train steps in 38.40 secs\n",
            "Step  79900: train CrossEntropyLoss |  4.83031273\n",
            "Step  79900: eval  CrossEntropyLoss |  5.07159805\n",
            "Step  79900: eval          Accuracy |  0.20909090\n",
            "\n",
            "Step  80000: Ran 100 train steps in 38.38 secs\n",
            "Step  80000: train CrossEntropyLoss |  4.70973396\n",
            "Step  80000: eval  CrossEntropyLoss |  5.10788631\n",
            "Step  80000: eval          Accuracy |  0.21666668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "# !rsync -a /content/drive/MyDrive/model2/ ~/\r\n",
        "\r\n",
        "# copy the model to Google Drive\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model/\r\n",
        "\r\n",
        "# sync Google Drive dir with the train dir\r\n",
        "# !rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e771de8b-9d1d-4f07-9c75-6a608bbe91c0"
      },
      "source": [
        "train_text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('свидетелям обвинения по делу об убийстве оппозиционера бориса немцова поступали угрозы во время разбирательства в московском окружном военном суде. об этом агентству тасс рассказала прокурор по уголовному делу мария семененко. «да, было. по крайней мере, нам достоверно известно о нескольких таких случаях в отношении свидетелей», — отметила она. семененко не стала отвечать на вопрос, находится ли кто-то из свидетелей под государственной защитой. по ее словам, такая информация является закрытой. в четверг, 13 июля, суд приговорил заура дадаева, признанного виновным в убийстве политика, к 20 годам лишения свободы в колонии строгого режима. помимо дадаева, по делу проходят темирлан эскерханов, а также братья анзор и шадид губашевы и хамзат бахаев. соучастники преступления получили сроки от 11 до 19 лет. немцова застрелили на большом москворецком мосту в центре столицы 27 февраля 2015 года. по версии следствия, руслан мухудинов, предполагаемый организатор преступления, предложил исполнителям 15 миллионов рублей за убийство, он объявлен в международный розыск. киллером следствие и суд сочли дадаева.',\n",
              " 'прокурор рассказала про угрозы свидетелям по\\xa0делу об\\xa0убийстве немцова')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYS0jHDJkAd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166f1956-081e-46fb-bc93-e59e6c884b33"
      },
      "source": [
        "eval_text_pairs[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('московский «локомотив» сыграл вничью с тульским «арсеналом» в матче 9-го тура российской футбольной премьер-лиги (рфпл). об этом в субботу, 1 октября, сообщается на сайте турнира. встреча, прошедшая на стадионе «локомотив» в москве, завершилась со счетом 1:1. гости вышли вперед на 44-й минуте, отличился фелицио браун форбс. железнодорожники ушли от поражения на последней минуте матча — александр самедов реализовал пенальти на ведране чорлуке. железнодорожники набрали  8-е очко и остались на 13-м месте в турнирной таблице. «арсенал» имеет на одно очко меньше и располагается строчкой ниже. подопечные юрия семина прервали серию из трех поражений в рфпл подряд. ранее «локомотив» последовательно уступал «спартаку», «уфе» и «ростову» с одинаковым счетом 0:1. 29 сентября на встрече с болельщиками столичного клуба семин извинился перед поклонниками за неудовлетворительный результат команды в последнее время. семин был назначен главным тренером «локомотива» 26 августа. с тех пор клуб одержал одну победу и уступил трижды при одной ничьей.',\n",
              " '«локомотив» сыграл вничью с «арсеналом» и\\xa0прервал серию из\\xa0трех поражений кряду')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU9OvGMPiVLR"
      },
      "source": [
        "# fixed input for comparison purposes\r\n",
        "train_input = \"свидетелям обвинения по делу об убийстве оппозиционера бориса немцова поступали угрозы во время разбирательства в московском окружном военном суде. об этом агентству тасс рассказала прокурор по уголовному делу мария семененко. «да, было. по крайней мере, нам достоверно известно о нескольких таких случаях в отношении свидетелей», — отметила она. семененко не стала отвечать на вопрос, находится ли кто-то из свидетелей под государственной защитой. по ее словам, такая информация является закрытой. в четверг, 13 июля, суд приговорил заура дадаева, признанного виновным в убийстве политика, к 20 годам лишения свободы в колонии строгого режима. помимо дадаева, по делу проходят темирлан эскерханов, а также братья анзор и шадид губашевы и хамзат бахаев. соучастники преступления получили сроки от 11 до 19 лет. немцова застрелили на большом москворецком мосту в центре столицы 27 февраля 2015 года. по версии следствия, руслан мухудинов, предполагаемый организатор преступления, предложил исполнителям 15 миллионов рублей за убийство, он объявлен в международный розыск. киллером следствие и суд сочли дадаева.\"\r\n",
        "eval_input = \"московский «локомотив» сыграл вничью с тульским «арсеналом» в матче 9-го тура российской футбольной премьер-лиги (рфпл). об этом в субботу, 1 октября, сообщается на сайте турнира. встреча, прошедшая на стадионе «локомотив» в москве, завершилась со счетом 1:1. гости вышли вперед на 44-й минуте, отличился фелицио браун форбс. железнодорожники ушли от поражения на последней минуте матча — александр самедов реализовал пенальти на ведране чорлуке. железнодорожники набрали  8-е очко и остались на 13-м месте в турнирной таблице. «арсенал» имеет на одно очко меньше и располагается строчкой ниже. подопечные юрия семина прервали серию из трех поражений в рфпл подряд. ранее «локомотив» последовательно уступал «спартаку», «уфе» и «ростову» с одинаковым счетом 0:1. 29 сентября на встрече с болельщиками столичного клуба семин извинился перед поклонниками за неудовлетворительный результат команды в последнее время. семин был назначен главным тренером «локомотива» 26 августа. с тех пор клуб одержал одну победу и уступил трижды при одной ничьей.\""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV-2j7AKT1pT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c60cf1-4667-4d45-bcf7-afc13ecae60a"
      },
      "source": [
        "print('прокурор рассказала про угрозы свидетелям по\\xa0делу об\\xa0убийстве немцова')\r\n",
        "print('')\r\n",
        "print(wrapper.fill(train_input))\r\n",
        "print('')\r\n",
        "print('«локомотив» сыграл вничью с «арсеналом» и\\xa0прервал серию из\\xa0трех поражений кряду')\r\n",
        "print('')\r\n",
        "print(wrapper.fill(eval_input))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "прокурор рассказала про угрозы свидетелям по делу об убийстве немцова\n",
            "\n",
            "свидетелям обвинения по делу об убийстве оппозиционера бориса немцова\n",
            "поступали угрозы во время разбирательства в московском окружном\n",
            "военном суде. об этом агентству тасс рассказала прокурор по уголовному\n",
            "делу мария семененко. «да, было. по крайней мере, нам достоверно\n",
            "известно о нескольких таких случаях в отношении свидетелей», —\n",
            "отметила она. семененко не стала отвечать на вопрос, находится ли кто-\n",
            "то из свидетелей под государственной защитой. по ее словам, такая\n",
            "информация является закрытой. в четверг, 13 июля, суд приговорил заура\n",
            "дадаева, признанного виновным в убийстве политика, к 20 годам лишения\n",
            "свободы в колонии строгого режима. помимо дадаева, по делу проходят\n",
            "темирлан эскерханов, а также братья анзор и шадид губашевы и хамзат\n",
            "бахаев. соучастники преступления получили сроки от 11 до 19 лет.\n",
            "немцова застрелили на большом москворецком мосту в центре столицы 27\n",
            "февраля 2015 года. по версии следствия, руслан мухудинов,\n",
            "предполагаемый организатор преступления, предложил исполнителям 15\n",
            "миллионов рублей за убийство, он объявлен в международный розыск.\n",
            "киллером следствие и суд сочли дадаева.\n",
            "\n",
            "«локомотив» сыграл вничью с «арсеналом» и прервал серию из трех поражений кряду\n",
            "\n",
            "московский «локомотив» сыграл вничью с тульским «арсеналом» в матче\n",
            "9-го тура российской футбольной премьер-лиги (рфпл). об этом в\n",
            "субботу, 1 октября, сообщается на сайте турнира. встреча, прошедшая на\n",
            "стадионе «локомотив» в москве, завершилась со счетом 1:1. гости вышли\n",
            "вперед на 44-й минуте, отличился фелицио браун форбс. железнодорожники\n",
            "ушли от поражения на последней минуте матча — александр самедов\n",
            "реализовал пенальти на ведране чорлуке. железнодорожники набрали  8-е\n",
            "очко и остались на 13-м месте в турнирной таблице. «арсенал» имеет на\n",
            "одно очко меньше и располагается строчкой ниже. подопечные юрия семина\n",
            "прервали серию из трех поражений в рфпл подряд. ранее «локомотив»\n",
            "последовательно уступал «спартаку», «уфе» и «ростову» с одинаковым\n",
            "счетом 0:1. 29 сентября на встрече с болельщиками столичного клуба\n",
            "семин извинился перед поклонниками за неудовлетворительный результат\n",
            "команды в последнее время. семин был назначен главным тренером\n",
            "«локомотива» 26 августа. с тех пор клуб одержал одну победу и уступил\n",
            "трижды при одной ничьей.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ad0861-0c99-4b2f-a6a7-1272a80051a3"
      },
      "source": [
        "print(detokenize([next_symbol(tokenize(train_input)+[0], model)]))\r\n",
        "print(detokenize([next_symbol(tokenize(eval_input)+[0], model)]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "следствие\n",
            "«\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "        if len(generated_output) >= 20:\r\n",
        "            print(detokenize(generated_output))\r\n",
        "            break\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "    \r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a19894-8542-41ed-e950-4c9b002dfa1f"
      },
      "source": [
        "print(greedy_decode(train_input, model))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "следствие\n",
            "следствие по\n",
            "следствие по делу\n",
            "следствие по делу о\n",
            "следствие по делу о убийстве\n",
            "следствие по делу о убийстве нем\n",
            "следствие по делу о убийстве немцова\n",
            "следствие по делу о убийстве немцова обвинили\n",
            "следствие по делу о убийстве немцова обвинили в\n",
            "следствие по делу о убийстве немцова обвинили в убийстве\n",
            "следствие по делу о убийстве немцова обвинили в убийстве\n",
            "следствие по делу о убийстве немцова обвинили в убийстве\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28115f13-da8c-42bf-c722-bb5af0399bf4"
      },
      "source": [
        "print(greedy_decode(eval_input, model))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "«\n",
            "«локомотив\n",
            "«локомотив»\n",
            "«локомотив» проиграл\n",
            "«локомотив» проиграл «\n",
            "«локомотив» проиграл «локомотив\n",
            "«локомотив» проиграл «локомотив»\n",
            "«локомотив» проиграл «локомотив» в\n",
            "«локомотив» проиграл «локомотив» в матче\n",
            "«локомотив» проиграл «локомотив» в матче «\n",
            "«локомотив» проиграл «локомотив» в матче «спартак\n",
            "«локомотив» проиграл «локомотив» в матче «спартак»\n",
            "«локомотив» проиграл «локомотив» в матче «спартак»\n",
            "«локомотив» проиграл «локомотив» в матче «спартак»\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7c94ce-537c-4c1a-8f2f-88ced7a47c66"
      },
      "source": [
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(tokenize(eval_input))[None, :],\r\n",
        "                                        temperature=0.5, max_length=20)\r\n",
        "print(wrapper.fill(detokenize(output[0])))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ку под вный туристов медведев один военный стра евро казахстан\n",
            "полицейский лидер индий вный гол обстрелялиный у\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}