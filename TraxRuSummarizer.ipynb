{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNF9WgsGuILvAv9Tg1+UFPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5492f5fd-0155-4d9b-87e5-00cf7eeda6b7"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▋                               | 10kB 28.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 33.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 23.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 40kB 27.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51kB 25.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 61kB 28.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 71kB 23.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 81kB 24.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 92kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 102kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 112kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 122kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 133kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 143kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 153kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 163kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 174kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 184kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 194kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 204kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 215kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 225kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 235kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 245kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 256kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 266kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 276kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 286kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 296kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 307kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 317kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 327kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 337kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 348kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 358kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 368kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 378kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 389kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 399kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 409kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 419kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 430kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 440kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 450kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 460kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 471kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 481kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 491kB 23.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 501kB 23.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 512kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 522kB 23.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 51.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 51.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 51.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 51.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 12.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 49.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 64.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 50.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 51.3MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "d0373007-1610-4bb4-e12c-d6685b443195"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "# data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "# data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "# data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "# data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "# text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "# for i in tqdm(range(data.shape[0])):\r\n",
        "    # if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        # text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafd90d3-592e-44a8-d602-30424903033f"
      },
      "source": [
        "# text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "9a443a1c-929d-46b7-a380-16c307c65131"
      },
      "source": [
        "# s0 = text_pairs[10][0]\r\n",
        "# text_list = wrapper.wrap(s0[:300])\r\n",
        "# for line in text_list:\r\n",
        "#     print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLsMmUFFnHA",
        "outputId": "84dbfa31-a44a-4e1c-b12f-12182e0f6a30"
      },
      "source": [
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n",
            "в киеве исключили новый раунд минских переговоров\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "beb2a0eb-0935-49ad-9b17-437c882e6b81"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC79r_7EPy6",
        "outputId": "a94dce36-16b4-4015-a9c7-6832e21eed6d"
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35f83a0-2373-4b81-835a-7ae20d9e58dd"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa57d5d-2259-453d-ebcb-3b90135003c5"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e024f9a-68b3-44cc-dc39-75a0b018d9e8"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        "    trax.data.FilterByLength(2048)\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a06f27d-78fd-413c-c8e1-ab0494c2c60b"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4563 15941  1068  1621   193   122   269 15949     1     0  6136  1328\n",
            " 12205  4563 15941   861     5  1759  4652     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "6c03f5a1-1ad2-4665-c282-5a51d2b9e640"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "50a2ea8a-de59-4d24-fe9e-537239b35a61"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  620,  3389,  6958,  1995,  3481,  5184,  5481,   110, 10774,\n",
              "          36,   291,   110,   221,    54,    38, 15933, 11819,   291,\n",
              "         110, 15939,   482, 15972,    16,   110, 15956,   743, 15930,\n",
              "         240,  4008, 15972, 11121,   303,     5,  3704,  9465,  1621,\n",
              "          26,    62, 15945,  5138, 13002,  6498,  7263,  2103, 15949,\n",
              "          63,   226,  6744,   110,  7066,   369,  1006,   371,  7424,\n",
              "        8161,  7996,  4366,  1017,  4151,    11,    36,    33,    96,\n",
              "       15949,  3771,   381,  3855, 10760,    43,  2668,  2478,    46,\n",
              "         692,    96,   161,    70,  1872,  8266, 15949,   586,  3786,\n",
              "          61,  1506, 12184,   136, 15996,   110, 15925,    36,    33,\n",
              "       15948, 15945, 13372, 15946,  7644,  5322,   270,   291,   110,\n",
              "       10435,  9475, 15956,   286, 15930,    82,    53, 15345, 15945,\n",
              "          41, 13563, 14269,    59, 15936,  1255,  1519,   291,   110,\n",
              "         795,    26,  5689,   210,    57,   882,   159,  3229,   249,\n",
              "          11,    36,    33,    96,   371, 14662,  1277,  7903,    59,\n",
              "        2151,   309,  2292,   508, 15945,   227,   532,    57,   375,\n",
              "       10905,   620,  1083, 10316,     5, 13381,  4316,    16,  2392,\n",
              "          35,   119,  1931, 15950, 15949,  3196, 11474,  2046,   314,\n",
              "         493,  1498,    59, 15932,   729,   882,  4896, 15949,   207,\n",
              "        1203,  1694,   110, 15936,    98,   304,   324,   291,  1995,\n",
              "        5845,  6114,  1506,  7231,   127,  2103, 15949,  7799, 11104,\n",
              "         915,  6576,   203,  1865,    38,   110, 10774,    36, 15972,\n",
              "        4163,   154, 15930,  5740, 15945,  2529, 15933,    75,  1650,\n",
              "        1017,  9644,  8264,   945,    63,  8310,   262,  8766,  1852,\n",
              "         415, 15949,  3415,   824,   647,    25,  3929,  1937,    11,\n",
              "          36,    33, 15948,   455, 15945,    79,   249, 11041,   673,\n",
              "        9855, 14662,  1277,  1605,  9334,  7231,   127, 15949,  1995,\n",
              "        5845,   237, 10020,   328,  2583,    33,  1506, 15949,    70,\n",
              "        1051,   841,  1471,   819,  7263,  2103,    57,  3763, 12605,\n",
              "       15949,     5,  2222, 11619,    25,  2491,   895,   303, 11212,\n",
              "       15945,   875,  8002,  1995,  3481,  2384,  1154,    18,  6596,\n",
              "        1597,  9465,  4622,  5938, 15945,  1715,  8317,  1206,  8618,\n",
              "        2957,    56,  3948,  2964, 15949,   208, 15945,     5,  2999,\n",
              "        2848,   173,  7424,   110, 10774,   264, 15972,    59,  2151,\n",
              "         303,     5,  3704,  2478,  2871,  5689, 15945,  2529, 15933,\n",
              "        8264,   945,    11, 10912,    58,   212,  2629, 10882, 15949,\n",
              "           5,  3087,  1995,  5845,  1740,   493, 13758, 10604,  1327,\n",
              "        6498,  3227,   204,  7263,  2103, 15945,    25,  1598,  1605,\n",
              "         452,  3559,  2505,  1017,    25,  2608,  1538,  1036, 15949,\n",
              "           1,     0,  6054,  1995,  5845,  8156,  6596,  1597,  1424,\n",
              "        1836,  1621,    26,    62,     1,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "b304a916-34e9-4b6e-95c1-8993f68ec799"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.0001)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.0001), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "dfee67dd-eb85-4606-c57a-c5a3fef3968a"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  160100: Ran 100 train steps in 84.79 secs\n",
            "Step  160100: train CrossEntropyLoss |  4.00795841\n",
            "Step  160100: eval  CrossEntropyLoss |  3.98525572\n",
            "Step  160100: eval          Accuracy |  0.38536584\n",
            "\n",
            "Step  160200: Ran 100 train steps in 46.77 secs\n",
            "Step  160200: train CrossEntropyLoss |  3.83672452\n",
            "Step  160200: eval  CrossEntropyLoss |  3.56351542\n",
            "Step  160200: eval          Accuracy |  0.41346157\n",
            "\n",
            "Step  160300: Ran 100 train steps in 64.33 secs\n",
            "Step  160300: train CrossEntropyLoss |  3.78694940\n",
            "Step  160300: eval  CrossEntropyLoss |  4.71436977\n",
            "Step  160300: eval          Accuracy |  0.32380953\n",
            "\n",
            "Step  160400: Ran 100 train steps in 47.64 secs\n",
            "Step  160400: train CrossEntropyLoss |  3.76034784\n",
            "Step  160400: eval  CrossEntropyLoss |  3.81426024\n",
            "Step  160400: eval          Accuracy |  0.43103448\n",
            "\n",
            "Step  160500: Ran 100 train steps in 47.39 secs\n",
            "Step  160500: train CrossEntropyLoss |  3.70814300\n",
            "Step  160500: eval  CrossEntropyLoss |  3.78402114\n",
            "Step  160500: eval          Accuracy |  0.35294119\n",
            "\n",
            "Step  160600: Ran 100 train steps in 47.34 secs\n",
            "Step  160600: train CrossEntropyLoss |  3.64640522\n",
            "Step  160600: eval  CrossEntropyLoss |  3.97803307\n",
            "Step  160600: eval          Accuracy |  0.42708334\n",
            "\n",
            "Step  160700: Ran 100 train steps in 47.43 secs\n",
            "Step  160700: train CrossEntropyLoss |  3.65262818\n",
            "Step  160700: eval  CrossEntropyLoss |  3.37251925\n",
            "Step  160700: eval          Accuracy |  0.40277779\n",
            "\n",
            "Step  160800: Ran 100 train steps in 47.70 secs\n",
            "Step  160800: train CrossEntropyLoss |  3.58885288\n",
            "Step  160800: eval  CrossEntropyLoss |  3.31752896\n",
            "Step  160800: eval          Accuracy |  0.45192310\n",
            "\n",
            "Step  160900: Ran 100 train steps in 47.61 secs\n",
            "Step  160900: train CrossEntropyLoss |  3.59214616\n",
            "Step  160900: eval  CrossEntropyLoss |  3.16790271\n",
            "Step  160900: eval          Accuracy |  0.50505048\n",
            "\n",
            "Step  161000: Ran 100 train steps in 47.51 secs\n",
            "Step  161000: train CrossEntropyLoss |  3.53161573\n",
            "Step  161000: eval  CrossEntropyLoss |  4.06975937\n",
            "Step  161000: eval          Accuracy |  0.34513274\n",
            "\n",
            "Step  161100: Ran 100 train steps in 47.70 secs\n",
            "Step  161100: train CrossEntropyLoss |  3.53709841\n",
            "Step  161100: eval  CrossEntropyLoss |  3.50603747\n",
            "Step  161100: eval          Accuracy |  0.41798943\n",
            "\n",
            "Step  161200: Ran 100 train steps in 47.59 secs\n",
            "Step  161200: train CrossEntropyLoss |  3.46064997\n",
            "Step  161200: eval  CrossEntropyLoss |  3.66824079\n",
            "Step  161200: eval          Accuracy |  0.45871559\n",
            "\n",
            "Step  161300: Ran 100 train steps in 47.79 secs\n",
            "Step  161300: train CrossEntropyLoss |  3.52822328\n",
            "Step  161300: eval  CrossEntropyLoss |  4.13813972\n",
            "Step  161300: eval          Accuracy |  0.34375000\n",
            "\n",
            "Step  161400: Ran 100 train steps in 47.61 secs\n",
            "Step  161400: train CrossEntropyLoss |  3.48424649\n",
            "Step  161400: eval  CrossEntropyLoss |  3.50480986\n",
            "Step  161400: eval          Accuracy |  0.48958334\n",
            "\n",
            "Step  161500: Ran 100 train steps in 47.84 secs\n",
            "Step  161500: train CrossEntropyLoss |  3.49440122\n",
            "Step  161500: eval  CrossEntropyLoss |  3.05823302\n",
            "Step  161500: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  161600: Ran 100 train steps in 47.72 secs\n",
            "Step  161600: train CrossEntropyLoss |  3.47949219\n",
            "Step  161600: eval  CrossEntropyLoss |  4.17319059\n",
            "Step  161600: eval          Accuracy |  0.34745762\n",
            "\n",
            "Step  161700: Ran 100 train steps in 47.61 secs\n",
            "Step  161700: train CrossEntropyLoss |  3.43339229\n",
            "Step  161700: eval  CrossEntropyLoss |  3.64593196\n",
            "Step  161700: eval          Accuracy |  0.41836733\n",
            "\n",
            "Step  161800: Ran 100 train steps in 47.73 secs\n",
            "Step  161800: train CrossEntropyLoss |  3.42625523\n",
            "Step  161800: eval  CrossEntropyLoss |  3.93997502\n",
            "Step  161800: eval          Accuracy |  0.36206895\n",
            "\n",
            "Step  161900: Ran 100 train steps in 47.81 secs\n",
            "Step  161900: train CrossEntropyLoss |  3.46052551\n",
            "Step  161900: eval  CrossEntropyLoss |  3.03206873\n",
            "Step  161900: eval          Accuracy |  0.46354169\n",
            "\n",
            "Step  162000: Ran 100 train steps in 47.92 secs\n",
            "Step  162000: train CrossEntropyLoss |  3.39504194\n",
            "Step  162000: eval  CrossEntropyLoss |  3.53662872\n",
            "Step  162000: eval          Accuracy |  0.41818181\n",
            "\n",
            "Step  162100: Ran 100 train steps in 47.99 secs\n",
            "Step  162100: train CrossEntropyLoss |  3.38537621\n",
            "Step  162100: eval  CrossEntropyLoss |  3.77000475\n",
            "Step  162100: eval          Accuracy |  0.37864077\n",
            "\n",
            "Step  162200: Ran 100 train steps in 47.61 secs\n",
            "Step  162200: train CrossEntropyLoss |  3.40364099\n",
            "Step  162200: eval  CrossEntropyLoss |  3.83952904\n",
            "Step  162200: eval          Accuracy |  0.38888893\n",
            "\n",
            "Step  162300: Ran 100 train steps in 47.82 secs\n",
            "Step  162300: train CrossEntropyLoss |  3.38824725\n",
            "Step  162300: eval  CrossEntropyLoss |  3.02066922\n",
            "Step  162300: eval          Accuracy |  0.46728972\n",
            "\n",
            "Step  162400: Ran 100 train steps in 47.86 secs\n",
            "Step  162400: train CrossEntropyLoss |  3.34562564\n",
            "Step  162400: eval  CrossEntropyLoss |  3.51712918\n",
            "Step  162400: eval          Accuracy |  0.45771143\n",
            "\n",
            "Step  162500: Ran 100 train steps in 47.70 secs\n",
            "Step  162500: train CrossEntropyLoss |  3.42992234\n",
            "Step  162500: eval  CrossEntropyLoss |  3.32107377\n",
            "Step  162500: eval          Accuracy |  0.42592594\n",
            "\n",
            "Step  162600: Ran 100 train steps in 47.82 secs\n",
            "Step  162600: train CrossEntropyLoss |  3.35342121\n",
            "Step  162600: eval  CrossEntropyLoss |  3.34058213\n",
            "Step  162600: eval          Accuracy |  0.43298972\n",
            "\n",
            "Step  162700: Ran 100 train steps in 47.81 secs\n",
            "Step  162700: train CrossEntropyLoss |  3.33621550\n",
            "Step  162700: eval  CrossEntropyLoss |  3.26657391\n",
            "Step  162700: eval          Accuracy |  0.50549453\n",
            "\n",
            "Step  162800: Ran 100 train steps in 47.88 secs\n",
            "Step  162800: train CrossEntropyLoss |  3.32050490\n",
            "Step  162800: eval  CrossEntropyLoss |  3.22136354\n",
            "Step  162800: eval          Accuracy |  0.50375938\n",
            "\n",
            "Step  162900: Ran 100 train steps in 47.85 secs\n",
            "Step  162900: train CrossEntropyLoss |  3.31826186\n",
            "Step  162900: eval  CrossEntropyLoss |  3.30924559\n",
            "Step  162900: eval          Accuracy |  0.44162434\n",
            "\n",
            "Step  163000: Ran 100 train steps in 47.72 secs\n",
            "Step  163000: train CrossEntropyLoss |  3.32002473\n",
            "Step  163000: eval  CrossEntropyLoss |  3.01580667\n",
            "Step  163000: eval          Accuracy |  0.51546395\n",
            "\n",
            "Step  163100: Ran 100 train steps in 48.15 secs\n",
            "Step  163100: train CrossEntropyLoss |  3.32828403\n",
            "Step  163100: eval  CrossEntropyLoss |  3.66602612\n",
            "Step  163100: eval          Accuracy |  0.34999999\n",
            "\n",
            "Step  163200: Ran 100 train steps in 47.71 secs\n",
            "Step  163200: train CrossEntropyLoss |  3.28628159\n",
            "Step  163200: eval  CrossEntropyLoss |  2.90665030\n",
            "Step  163200: eval          Accuracy |  0.50999999\n",
            "\n",
            "Step  163300: Ran 100 train steps in 47.87 secs\n",
            "Step  163300: train CrossEntropyLoss |  3.32295585\n",
            "Step  163300: eval  CrossEntropyLoss |  3.65073371\n",
            "Step  163300: eval          Accuracy |  0.44166669\n",
            "\n",
            "Step  163400: Ran 100 train steps in 47.89 secs\n",
            "Step  163400: train CrossEntropyLoss |  3.29572630\n",
            "Step  163400: eval  CrossEntropyLoss |  3.41088104\n",
            "Step  163400: eval          Accuracy |  0.38532108\n",
            "\n",
            "Step  163500: Ran 100 train steps in 47.70 secs\n",
            "Step  163500: train CrossEntropyLoss |  3.30713153\n",
            "Step  163500: eval  CrossEntropyLoss |  3.95344925\n",
            "Step  163500: eval          Accuracy |  0.40650409\n",
            "\n",
            "Step  163600: Ran 100 train steps in 47.74 secs\n",
            "Step  163600: train CrossEntropyLoss |  3.29046631\n",
            "Step  163600: eval  CrossEntropyLoss |  3.03964305\n",
            "Step  163600: eval          Accuracy |  0.48356807\n",
            "\n",
            "Step  163700: Ran 100 train steps in 47.69 secs\n",
            "Step  163700: train CrossEntropyLoss |  3.28862453\n",
            "Step  163700: eval  CrossEntropyLoss |  3.20772791\n",
            "Step  163700: eval          Accuracy |  0.46226415\n",
            "\n",
            "Step  163800: Ran 100 train steps in 47.90 secs\n",
            "Step  163800: train CrossEntropyLoss |  3.23393679\n",
            "Step  163800: eval  CrossEntropyLoss |  3.74129629\n",
            "Step  163800: eval          Accuracy |  0.44545454\n",
            "\n",
            "Step  163900: Ran 100 train steps in 47.80 secs\n",
            "Step  163900: train CrossEntropyLoss |  3.33257318\n",
            "Step  163900: eval  CrossEntropyLoss |  3.12806988\n",
            "Step  163900: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  164000: Ran 100 train steps in 47.62 secs\n",
            "Step  164000: train CrossEntropyLoss |  3.38392472\n",
            "Step  164000: eval  CrossEntropyLoss |  3.00384569\n",
            "Step  164000: eval          Accuracy |  0.50999999\n",
            "\n",
            "Step  164100: Ran 100 train steps in 47.70 secs\n",
            "Step  164100: train CrossEntropyLoss |  3.25491166\n",
            "Step  164100: eval  CrossEntropyLoss |  3.54902816\n",
            "Step  164100: eval          Accuracy |  0.37614676\n",
            "\n",
            "Step  164200: Ran 100 train steps in 47.88 secs\n",
            "Step  164200: train CrossEntropyLoss |  3.29457903\n",
            "Step  164200: eval  CrossEntropyLoss |  3.13071799\n",
            "Step  164200: eval          Accuracy |  0.49253729\n",
            "\n",
            "Step  164300: Ran 100 train steps in 47.92 secs\n",
            "Step  164300: train CrossEntropyLoss |  3.22293043\n",
            "Step  164300: eval  CrossEntropyLoss |  3.83997226\n",
            "Step  164300: eval          Accuracy |  0.38834953\n",
            "\n",
            "Step  164400: Ran 100 train steps in 47.48 secs\n",
            "Step  164400: train CrossEntropyLoss |  3.26898885\n",
            "Step  164400: eval  CrossEntropyLoss |  3.14486718\n",
            "Step  164400: eval          Accuracy |  0.44444445\n",
            "\n",
            "Step  164500: Ran 100 train steps in 47.16 secs\n",
            "Step  164500: train CrossEntropyLoss |  3.25336885\n",
            "Step  164500: eval  CrossEntropyLoss |  3.80632067\n",
            "Step  164500: eval          Accuracy |  0.42222223\n",
            "\n",
            "Step  164600: Ran 100 train steps in 47.21 secs\n",
            "Step  164600: train CrossEntropyLoss |  3.28526092\n",
            "Step  164600: eval  CrossEntropyLoss |  2.66213584\n",
            "Step  164600: eval          Accuracy |  0.54913294\n",
            "\n",
            "Step  164700: Ran 100 train steps in 47.98 secs\n",
            "Step  164700: train CrossEntropyLoss |  3.24124527\n",
            "Step  164700: eval  CrossEntropyLoss |  3.31518364\n",
            "Step  164700: eval          Accuracy |  0.44736841\n",
            "\n",
            "Step  164800: Ran 100 train steps in 47.60 secs\n",
            "Step  164800: train CrossEntropyLoss |  3.31510735\n",
            "Step  164800: eval  CrossEntropyLoss |  3.94486976\n",
            "Step  164800: eval          Accuracy |  0.37068966\n",
            "\n",
            "Step  164900: Ran 100 train steps in 47.67 secs\n",
            "Step  164900: train CrossEntropyLoss |  3.27585769\n",
            "Step  164900: eval  CrossEntropyLoss |  3.32668471\n",
            "Step  164900: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  165000: Ran 100 train steps in 47.70 secs\n",
            "Step  165000: train CrossEntropyLoss |  3.24868441\n",
            "Step  165000: eval  CrossEntropyLoss |  3.70669961\n",
            "Step  165000: eval          Accuracy |  0.44919786\n",
            "\n",
            "Step  165100: Ran 100 train steps in 47.76 secs\n",
            "Step  165100: train CrossEntropyLoss |  3.31857777\n",
            "Step  165100: eval  CrossEntropyLoss |  3.80264378\n",
            "Step  165100: eval          Accuracy |  0.39682543\n",
            "\n",
            "Step  165200: Ran 100 train steps in 47.78 secs\n",
            "Step  165200: train CrossEntropyLoss |  3.27039909\n",
            "Step  165200: eval  CrossEntropyLoss |  3.35994649\n",
            "Step  165200: eval          Accuracy |  0.48999998\n",
            "\n",
            "Step  165300: Ran 100 train steps in 47.82 secs\n",
            "Step  165300: train CrossEntropyLoss |  3.21757793\n",
            "Step  165300: eval  CrossEntropyLoss |  3.53018665\n",
            "Step  165300: eval          Accuracy |  0.39267015\n",
            "\n",
            "Step  165400: Ran 100 train steps in 47.77 secs\n",
            "Step  165400: train CrossEntropyLoss |  3.22222185\n",
            "Step  165400: eval  CrossEntropyLoss |  2.84440017\n",
            "Step  165400: eval          Accuracy |  0.46078432\n",
            "\n",
            "Step  165500: Ran 100 train steps in 47.84 secs\n",
            "Step  165500: train CrossEntropyLoss |  3.32080960\n",
            "Step  165500: eval  CrossEntropyLoss |  3.37902331\n",
            "Step  165500: eval          Accuracy |  0.39215687\n",
            "\n",
            "Step  165600: Ran 100 train steps in 47.81 secs\n",
            "Step  165600: train CrossEntropyLoss |  3.19429135\n",
            "Step  165600: eval  CrossEntropyLoss |  3.76015258\n",
            "Step  165600: eval          Accuracy |  0.36885247\n",
            "\n",
            "Step  165700: Ran 100 train steps in 47.62 secs\n",
            "Step  165700: train CrossEntropyLoss |  3.23416853\n",
            "Step  165700: eval  CrossEntropyLoss |  2.73426080\n",
            "Step  165700: eval          Accuracy |  0.55440420\n",
            "\n",
            "Step  165800: Ran 100 train steps in 47.94 secs\n",
            "Step  165800: train CrossEntropyLoss |  3.26697898\n",
            "Step  165800: eval  CrossEntropyLoss |  3.75327611\n",
            "Step  165800: eval          Accuracy |  0.34259260\n",
            "\n",
            "Step  165900: Ran 100 train steps in 47.87 secs\n",
            "Step  165900: train CrossEntropyLoss |  3.17990422\n",
            "Step  165900: eval  CrossEntropyLoss |  3.63653326\n",
            "Step  165900: eval          Accuracy |  0.41121495\n",
            "\n",
            "Step  166000: Ran 100 train steps in 47.78 secs\n",
            "Step  166000: train CrossEntropyLoss |  3.25319791\n",
            "Step  166000: eval  CrossEntropyLoss |  3.00963950\n",
            "Step  166000: eval          Accuracy |  0.49758452\n",
            "\n",
            "Step  166100: Ran 100 train steps in 47.65 secs\n",
            "Step  166100: train CrossEntropyLoss |  3.23310947\n",
            "Step  166100: eval  CrossEntropyLoss |  3.00089049\n",
            "Step  166100: eval          Accuracy |  0.52173913\n",
            "\n",
            "Step  166200: Ran 100 train steps in 47.59 secs\n",
            "Step  166200: train CrossEntropyLoss |  3.21661758\n",
            "Step  166200: eval  CrossEntropyLoss |  3.82653570\n",
            "Step  166200: eval          Accuracy |  0.40869564\n",
            "\n",
            "Step  166300: Ran 100 train steps in 47.74 secs\n",
            "Step  166300: train CrossEntropyLoss |  3.27664781\n",
            "Step  166300: eval  CrossEntropyLoss |  3.41513801\n",
            "Step  166300: eval          Accuracy |  0.40845072\n",
            "\n",
            "Step  166400: Ran 100 train steps in 47.83 secs\n",
            "Step  166400: train CrossEntropyLoss |  3.25783253\n",
            "Step  166400: eval  CrossEntropyLoss |  3.50345230\n",
            "Step  166400: eval          Accuracy |  0.48039219\n",
            "\n",
            "Step  166500: Ran 100 train steps in 47.76 secs\n",
            "Step  166500: train CrossEntropyLoss |  3.19893217\n",
            "Step  166500: eval  CrossEntropyLoss |  3.57401609\n",
            "Step  166500: eval          Accuracy |  0.48648649\n",
            "\n",
            "Step  166600: Ran 100 train steps in 47.63 secs\n",
            "Step  166600: train CrossEntropyLoss |  3.23414516\n",
            "Step  166600: eval  CrossEntropyLoss |  3.24982548\n",
            "Step  166600: eval          Accuracy |  0.48484850\n",
            "\n",
            "Step  166700: Ran 100 train steps in 47.63 secs\n",
            "Step  166700: train CrossEntropyLoss |  3.21368289\n",
            "Step  166700: eval  CrossEntropyLoss |  3.05629897\n",
            "Step  166700: eval          Accuracy |  0.49523813\n",
            "\n",
            "Step  166800: Ran 100 train steps in 47.84 secs\n",
            "Step  166800: train CrossEntropyLoss |  3.24116111\n",
            "Step  166800: eval  CrossEntropyLoss |  3.43064141\n",
            "Step  166800: eval          Accuracy |  0.43718591\n",
            "\n",
            "Step  166900: Ran 100 train steps in 47.77 secs\n",
            "Step  166900: train CrossEntropyLoss |  3.16206622\n",
            "Step  166900: eval  CrossEntropyLoss |  3.11924529\n",
            "Step  166900: eval          Accuracy |  0.46363634\n",
            "\n",
            "Step  167000: Ran 100 train steps in 47.87 secs\n",
            "Step  167000: train CrossEntropyLoss |  3.21175599\n",
            "Step  167000: eval  CrossEntropyLoss |  2.77314115\n",
            "Step  167000: eval          Accuracy |  0.54545450\n",
            "\n",
            "Step  167100: Ran 100 train steps in 47.77 secs\n",
            "Step  167100: train CrossEntropyLoss |  3.25567746\n",
            "Step  167100: eval  CrossEntropyLoss |  3.05303121\n",
            "Step  167100: eval          Accuracy |  0.44545454\n",
            "\n",
            "Step  167200: Ran 100 train steps in 47.80 secs\n",
            "Step  167200: train CrossEntropyLoss |  3.20260358\n",
            "Step  167200: eval  CrossEntropyLoss |  3.11741471\n",
            "Step  167200: eval          Accuracy |  0.50230414\n",
            "\n",
            "Step  167300: Ran 100 train steps in 47.94 secs\n",
            "Step  167300: train CrossEntropyLoss |  3.23777890\n",
            "Step  167300: eval  CrossEntropyLoss |  3.28705335\n",
            "Step  167300: eval          Accuracy |  0.47524750\n",
            "\n",
            "Step  167400: Ran 100 train steps in 47.81 secs\n",
            "Step  167400: train CrossEntropyLoss |  3.21931362\n",
            "Step  167400: eval  CrossEntropyLoss |  3.59817195\n",
            "Step  167400: eval          Accuracy |  0.43362832\n",
            "\n",
            "Step  167500: Ran 100 train steps in 47.96 secs\n",
            "Step  167500: train CrossEntropyLoss |  3.26867819\n",
            "Step  167500: eval  CrossEntropyLoss |  3.17292428\n",
            "Step  167500: eval          Accuracy |  0.53061223\n",
            "\n",
            "Step  167600: Ran 100 train steps in 47.88 secs\n",
            "Step  167600: train CrossEntropyLoss |  3.18983126\n",
            "Step  167600: eval  CrossEntropyLoss |  3.40464735\n",
            "Step  167600: eval          Accuracy |  0.44000000\n",
            "\n",
            "Step  167700: Ran 100 train steps in 47.85 secs\n",
            "Step  167700: train CrossEntropyLoss |  3.15542078\n",
            "Step  167700: eval  CrossEntropyLoss |  3.32997513\n",
            "Step  167700: eval          Accuracy |  0.41262135\n",
            "\n",
            "Step  167800: Ran 100 train steps in 47.86 secs\n",
            "Step  167800: train CrossEntropyLoss |  3.21245146\n",
            "Step  167800: eval  CrossEntropyLoss |  2.77861285\n",
            "Step  167800: eval          Accuracy |  0.47872338\n",
            "\n",
            "Step  167900: Ran 100 train steps in 48.11 secs\n",
            "Step  167900: train CrossEntropyLoss |  3.15502167\n",
            "Step  167900: eval  CrossEntropyLoss |  3.36025000\n",
            "Step  167900: eval          Accuracy |  0.42995170\n",
            "\n",
            "Step  168000: Ran 100 train steps in 47.93 secs\n",
            "Step  168000: train CrossEntropyLoss |  3.16290402\n",
            "Step  168000: eval  CrossEntropyLoss |  4.08198786\n",
            "Step  168000: eval          Accuracy |  0.36734694\n",
            "\n",
            "Step  168100: Ran 100 train steps in 47.80 secs\n",
            "Step  168100: train CrossEntropyLoss |  3.14956450\n",
            "Step  168100: eval  CrossEntropyLoss |  2.89968276\n",
            "Step  168100: eval          Accuracy |  0.50420171\n",
            "\n",
            "Step  168200: Ran 100 train steps in 47.83 secs\n",
            "Step  168200: train CrossEntropyLoss |  3.20924306\n",
            "Step  168200: eval  CrossEntropyLoss |  3.23297620\n",
            "Step  168200: eval          Accuracy |  0.43076923\n",
            "\n",
            "Step  168300: Ran 100 train steps in 48.04 secs\n",
            "Step  168300: train CrossEntropyLoss |  3.21013665\n",
            "Step  168300: eval  CrossEntropyLoss |  3.09029722\n",
            "Step  168300: eval          Accuracy |  0.45833334\n",
            "\n",
            "Step  168400: Ran 100 train steps in 47.93 secs\n",
            "Step  168400: train CrossEntropyLoss |  3.21206498\n",
            "Step  168400: eval  CrossEntropyLoss |  3.23521948\n",
            "Step  168400: eval          Accuracy |  0.45045045\n",
            "\n",
            "Step  168500: Ran 100 train steps in 47.94 secs\n",
            "Step  168500: train CrossEntropyLoss |  3.19007444\n",
            "Step  168500: eval  CrossEntropyLoss |  3.12081385\n",
            "Step  168500: eval          Accuracy |  0.47524750\n",
            "\n",
            "Step  168600: Ran 100 train steps in 47.85 secs\n",
            "Step  168600: train CrossEntropyLoss |  3.20166183\n",
            "Step  168600: eval  CrossEntropyLoss |  3.88318062\n",
            "Step  168600: eval          Accuracy |  0.40206188\n",
            "\n",
            "Step  168700: Ran 100 train steps in 47.89 secs\n",
            "Step  168700: train CrossEntropyLoss |  3.10673785\n",
            "Step  168700: eval  CrossEntropyLoss |  3.36112475\n",
            "Step  168700: eval          Accuracy |  0.41666669\n",
            "\n",
            "Step  168800: Ran 100 train steps in 47.89 secs\n",
            "Step  168800: train CrossEntropyLoss |  3.25711775\n",
            "Step  168800: eval  CrossEntropyLoss |  3.32177114\n",
            "Step  168800: eval          Accuracy |  0.45384616\n",
            "\n",
            "Step  168900: Ran 100 train steps in 48.06 secs\n",
            "Step  168900: train CrossEntropyLoss |  3.20955491\n",
            "Step  168900: eval  CrossEntropyLoss |  3.03981471\n",
            "Step  168900: eval          Accuracy |  0.47979799\n",
            "\n",
            "Step  169000: Ran 100 train steps in 48.13 secs\n",
            "Step  169000: train CrossEntropyLoss |  3.20619750\n",
            "Step  169000: eval  CrossEntropyLoss |  3.66142797\n",
            "Step  169000: eval          Accuracy |  0.43103448\n",
            "\n",
            "Step  169100: Ran 100 train steps in 47.77 secs\n",
            "Step  169100: train CrossEntropyLoss |  3.13178515\n",
            "Step  169100: eval  CrossEntropyLoss |  3.10107732\n",
            "Step  169100: eval          Accuracy |  0.52678573\n",
            "\n",
            "Step  169200: Ran 100 train steps in 48.06 secs\n",
            "Step  169200: train CrossEntropyLoss |  3.13675475\n",
            "Step  169200: eval  CrossEntropyLoss |  3.01638699\n",
            "Step  169200: eval          Accuracy |  0.51376146\n",
            "\n",
            "Step  169300: Ran 100 train steps in 47.92 secs\n",
            "Step  169300: train CrossEntropyLoss |  3.06253529\n",
            "Step  169300: eval  CrossEntropyLoss |  3.21021175\n",
            "Step  169300: eval          Accuracy |  0.49999997\n",
            "\n",
            "Step  169400: Ran 100 train steps in 47.87 secs\n",
            "Step  169400: train CrossEntropyLoss |  3.16343427\n",
            "Step  169400: eval  CrossEntropyLoss |  2.48377085\n",
            "Step  169400: eval          Accuracy |  0.51123595\n",
            "\n",
            "Step  169500: Ran 100 train steps in 47.89 secs\n",
            "Step  169500: train CrossEntropyLoss |  3.22076678\n",
            "Step  169500: eval  CrossEntropyLoss |  3.32310605\n",
            "Step  169500: eval          Accuracy |  0.45370370\n",
            "\n",
            "Step  169600: Ran 100 train steps in 47.77 secs\n",
            "Step  169600: train CrossEntropyLoss |  3.14469099\n",
            "Step  169600: eval  CrossEntropyLoss |  3.33704662\n",
            "Step  169600: eval          Accuracy |  0.43157896\n",
            "\n",
            "Step  169700: Ran 100 train steps in 47.99 secs\n",
            "Step  169700: train CrossEntropyLoss |  3.19767928\n",
            "Step  169700: eval  CrossEntropyLoss |  2.83168721\n",
            "Step  169700: eval          Accuracy |  0.51269037\n",
            "\n",
            "Step  169800: Ran 100 train steps in 47.73 secs\n",
            "Step  169800: train CrossEntropyLoss |  3.09449100\n",
            "Step  169800: eval  CrossEntropyLoss |  2.39236498\n",
            "Step  169800: eval          Accuracy |  0.50505048\n",
            "\n",
            "Step  169900: Ran 100 train steps in 47.91 secs\n",
            "Step  169900: train CrossEntropyLoss |  3.15869498\n",
            "Step  169900: eval  CrossEntropyLoss |  3.40751410\n",
            "Step  169900: eval          Accuracy |  0.44329900\n",
            "\n",
            "Step  170000: Ran 100 train steps in 47.84 secs\n",
            "Step  170000: train CrossEntropyLoss |  3.06375360\n",
            "Step  170000: eval  CrossEntropyLoss |  3.11791229\n",
            "Step  170000: eval          Accuracy |  0.51401865\n",
            "\n",
            "Step  170100: Ran 100 train steps in 47.84 secs\n",
            "Step  170100: train CrossEntropyLoss |  3.22333479\n",
            "Step  170100: eval  CrossEntropyLoss |  3.31661844\n",
            "Step  170100: eval          Accuracy |  0.46632126\n",
            "\n",
            "Step  170200: Ran 100 train steps in 47.98 secs\n",
            "Step  170200: train CrossEntropyLoss |  3.12563419\n",
            "Step  170200: eval  CrossEntropyLoss |  3.69026899\n",
            "Step  170200: eval          Accuracy |  0.41228071\n",
            "\n",
            "Step  170300: Ran 100 train steps in 47.87 secs\n",
            "Step  170300: train CrossEntropyLoss |  3.13786483\n",
            "Step  170300: eval  CrossEntropyLoss |  3.13013935\n",
            "Step  170300: eval          Accuracy |  0.44651163\n",
            "\n",
            "Step  170400: Ran 100 train steps in 48.06 secs\n",
            "Step  170400: train CrossEntropyLoss |  3.17768359\n",
            "Step  170400: eval  CrossEntropyLoss |  3.69805264\n",
            "Step  170400: eval          Accuracy |  0.41666669\n",
            "\n",
            "Step  170500: Ran 100 train steps in 48.12 secs\n",
            "Step  170500: train CrossEntropyLoss |  3.08538723\n",
            "Step  170500: eval  CrossEntropyLoss |  3.64297533\n",
            "Step  170500: eval          Accuracy |  0.44736841\n",
            "\n",
            "Step  170600: Ran 100 train steps in 47.92 secs\n",
            "Step  170600: train CrossEntropyLoss |  3.17525148\n",
            "Step  170600: eval  CrossEntropyLoss |  3.11842346\n",
            "Step  170600: eval          Accuracy |  0.56603777\n",
            "\n",
            "Step  170700: Ran 100 train steps in 47.75 secs\n",
            "Step  170700: train CrossEntropyLoss |  3.08635712\n",
            "Step  170700: eval  CrossEntropyLoss |  2.99217129\n",
            "Step  170700: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  170800: Ran 100 train steps in 47.87 secs\n",
            "Step  170800: train CrossEntropyLoss |  3.06328917\n",
            "Step  170800: eval  CrossEntropyLoss |  3.13900447\n",
            "Step  170800: eval          Accuracy |  0.51041669\n",
            "\n",
            "Step  170900: Ran 100 train steps in 47.97 secs\n",
            "Step  170900: train CrossEntropyLoss |  3.19694614\n",
            "Step  170900: eval  CrossEntropyLoss |  2.91516137\n",
            "Step  170900: eval          Accuracy |  0.49593499\n",
            "\n",
            "Step  171000: Ran 100 train steps in 47.94 secs\n",
            "Step  171000: train CrossEntropyLoss |  3.13653135\n",
            "Step  171000: eval  CrossEntropyLoss |  2.57890987\n",
            "Step  171000: eval          Accuracy |  0.54782605\n",
            "\n",
            "Step  171100: Ran 100 train steps in 47.91 secs\n",
            "Step  171100: train CrossEntropyLoss |  3.11998773\n",
            "Step  171100: eval  CrossEntropyLoss |  2.73099828\n",
            "Step  171100: eval          Accuracy |  0.56315792\n",
            "\n",
            "Step  171200: Ran 100 train steps in 47.98 secs\n",
            "Step  171200: train CrossEntropyLoss |  3.13679957\n",
            "Step  171200: eval  CrossEntropyLoss |  3.85741544\n",
            "Step  171200: eval          Accuracy |  0.38999999\n",
            "\n",
            "Step  171300: Ran 100 train steps in 47.81 secs\n",
            "Step  171300: train CrossEntropyLoss |  3.08733726\n",
            "Step  171300: eval  CrossEntropyLoss |  2.58891463\n",
            "Step  171300: eval          Accuracy |  0.56000000\n",
            "\n",
            "Step  171400: Ran 100 train steps in 47.86 secs\n",
            "Step  171400: train CrossEntropyLoss |  3.17420650\n",
            "Step  171400: eval  CrossEntropyLoss |  2.89951062\n",
            "Step  171400: eval          Accuracy |  0.50961542\n",
            "\n",
            "Step  171500: Ran 100 train steps in 47.94 secs\n",
            "Step  171500: train CrossEntropyLoss |  3.10340238\n",
            "Step  171500: eval  CrossEntropyLoss |  3.07312679\n",
            "Step  171500: eval          Accuracy |  0.43877551\n",
            "\n",
            "Step  171600: Ran 100 train steps in 47.76 secs\n",
            "Step  171600: train CrossEntropyLoss |  3.19812441\n",
            "Step  171600: eval  CrossEntropyLoss |  2.67824960\n",
            "Step  171600: eval          Accuracy |  0.54285717\n",
            "\n",
            "Step  171700: Ran 100 train steps in 47.94 secs\n",
            "Step  171700: train CrossEntropyLoss |  3.12864733\n",
            "Step  171700: eval  CrossEntropyLoss |  3.31329823\n",
            "Step  171700: eval          Accuracy |  0.47572815\n",
            "\n",
            "Step  171800: Ran 100 train steps in 48.09 secs\n",
            "Step  171800: train CrossEntropyLoss |  3.10918784\n",
            "Step  171800: eval  CrossEntropyLoss |  3.17761946\n",
            "Step  171800: eval          Accuracy |  0.51063830\n",
            "\n",
            "Step  171900: Ran 100 train steps in 47.98 secs\n",
            "Step  171900: train CrossEntropyLoss |  3.10010028\n",
            "Step  171900: eval  CrossEntropyLoss |  2.91751981\n",
            "Step  171900: eval          Accuracy |  0.48499998\n",
            "\n",
            "Step  172000: Ran 100 train steps in 47.77 secs\n",
            "Step  172000: train CrossEntropyLoss |  3.07971334\n",
            "Step  172000: eval  CrossEntropyLoss |  3.84029961\n",
            "Step  172000: eval          Accuracy |  0.32710278\n",
            "\n",
            "Step  172100: Ran 100 train steps in 47.90 secs\n",
            "Step  172100: train CrossEntropyLoss |  3.20603728\n",
            "Step  172100: eval  CrossEntropyLoss |  3.91394663\n",
            "Step  172100: eval          Accuracy |  0.40540540\n",
            "\n",
            "Step  172200: Ran 100 train steps in 47.95 secs\n",
            "Step  172200: train CrossEntropyLoss |  3.10347867\n",
            "Step  172200: eval  CrossEntropyLoss |  3.04909873\n",
            "Step  172200: eval          Accuracy |  0.50228316\n",
            "\n",
            "Step  172300: Ran 100 train steps in 47.79 secs\n",
            "Step  172300: train CrossEntropyLoss |  3.15649223\n",
            "Step  172300: eval  CrossEntropyLoss |  3.52258873\n",
            "Step  172300: eval          Accuracy |  0.42718446\n",
            "\n",
            "Step  172400: Ran 100 train steps in 47.92 secs\n",
            "Step  172400: train CrossEntropyLoss |  3.14233875\n",
            "Step  172400: eval  CrossEntropyLoss |  2.51875138\n",
            "Step  172400: eval          Accuracy |  0.53211010\n",
            "\n",
            "Step  172500: Ran 100 train steps in 47.88 secs\n",
            "Step  172500: train CrossEntropyLoss |  3.14551210\n",
            "Step  172500: eval  CrossEntropyLoss |  3.00361180\n",
            "Step  172500: eval          Accuracy |  0.48717949\n",
            "\n",
            "Step  172600: Ran 100 train steps in 47.93 secs\n",
            "Step  172600: train CrossEntropyLoss |  3.04246902\n",
            "Step  172600: eval  CrossEntropyLoss |  3.78329754\n",
            "Step  172600: eval          Accuracy |  0.35294119\n",
            "\n",
            "Step  172700: Ran 100 train steps in 47.86 secs\n",
            "Step  172700: train CrossEntropyLoss |  3.12425590\n",
            "Step  172700: eval  CrossEntropyLoss |  4.28665543\n",
            "Step  172700: eval          Accuracy |  0.34426230\n",
            "\n",
            "Step  172800: Ran 100 train steps in 47.86 secs\n",
            "Step  172800: train CrossEntropyLoss |  3.14859939\n",
            "Step  172800: eval  CrossEntropyLoss |  4.23557520\n",
            "Step  172800: eval          Accuracy |  0.36559141\n",
            "\n",
            "Step  172900: Ran 100 train steps in 47.92 secs\n",
            "Step  172900: train CrossEntropyLoss |  3.09241271\n",
            "Step  172900: eval  CrossEntropyLoss |  2.98250270\n",
            "Step  172900: eval          Accuracy |  0.45049503\n",
            "\n",
            "Step  173000: Ran 100 train steps in 47.92 secs\n",
            "Step  173000: train CrossEntropyLoss |  3.09550858\n",
            "Step  173000: eval  CrossEntropyLoss |  3.13980389\n",
            "Step  173000: eval          Accuracy |  0.54464287\n",
            "\n",
            "Step  173100: Ran 100 train steps in 47.96 secs\n",
            "Step  173100: train CrossEntropyLoss |  3.07458830\n",
            "Step  173100: eval  CrossEntropyLoss |  3.64647198\n",
            "Step  173100: eval          Accuracy |  0.45263159\n",
            "\n",
            "Step  173200: Ran 100 train steps in 47.93 secs\n",
            "Step  173200: train CrossEntropyLoss |  3.09599686\n",
            "Step  173200: eval  CrossEntropyLoss |  3.31831527\n",
            "Step  173200: eval          Accuracy |  0.42975205\n",
            "\n",
            "Step  173300: Ran 100 train steps in 47.84 secs\n",
            "Step  173300: train CrossEntropyLoss |  3.07692432\n",
            "Step  173300: eval  CrossEntropyLoss |  2.91592002\n",
            "Step  173300: eval          Accuracy |  0.43269232\n",
            "\n",
            "Step  173400: Ran 100 train steps in 48.09 secs\n",
            "Step  173400: train CrossEntropyLoss |  3.13949394\n",
            "Step  173400: eval  CrossEntropyLoss |  3.15520644\n",
            "Step  173400: eval          Accuracy |  0.51639348\n",
            "\n",
            "Step  173500: Ran 100 train steps in 47.81 secs\n",
            "Step  173500: train CrossEntropyLoss |  3.16712236\n",
            "Step  173500: eval  CrossEntropyLoss |  3.38048124\n",
            "Step  173500: eval          Accuracy |  0.44927534\n",
            "\n",
            "Step  173600: Ran 100 train steps in 47.83 secs\n",
            "Step  173600: train CrossEntropyLoss |  3.05276585\n",
            "Step  173600: eval  CrossEntropyLoss |  3.58767819\n",
            "Step  173600: eval          Accuracy |  0.41739130\n",
            "\n",
            "Step  173700: Ran 100 train steps in 47.96 secs\n",
            "Step  173700: train CrossEntropyLoss |  3.12545133\n",
            "Step  173700: eval  CrossEntropyLoss |  3.33411694\n",
            "Step  173700: eval          Accuracy |  0.43877551\n",
            "\n",
            "Step  173800: Ran 100 train steps in 47.83 secs\n",
            "Step  173800: train CrossEntropyLoss |  3.06804109\n",
            "Step  173800: eval  CrossEntropyLoss |  3.78798699\n",
            "Step  173800: eval          Accuracy |  0.38659796\n",
            "\n",
            "Step  173900: Ran 100 train steps in 48.19 secs\n",
            "Step  173900: train CrossEntropyLoss |  3.12728715\n",
            "Step  173900: eval  CrossEntropyLoss |  2.89734602\n",
            "Step  173900: eval          Accuracy |  0.49514565\n",
            "\n",
            "Step  174000: Ran 100 train steps in 47.90 secs\n",
            "Step  174000: train CrossEntropyLoss |  3.07823515\n",
            "Step  174000: eval  CrossEntropyLoss |  2.53879166\n",
            "Step  174000: eval          Accuracy |  0.53658539\n",
            "\n",
            "Step  174100: Ran 100 train steps in 48.12 secs\n",
            "Step  174100: train CrossEntropyLoss |  3.11332440\n",
            "Step  174100: eval  CrossEntropyLoss |  3.58576465\n",
            "Step  174100: eval          Accuracy |  0.44444448\n",
            "\n",
            "Step  174200: Ran 100 train steps in 47.84 secs\n",
            "Step  174200: train CrossEntropyLoss |  3.13981342\n",
            "Step  174200: eval  CrossEntropyLoss |  3.07743120\n",
            "Step  174200: eval          Accuracy |  0.48453611\n",
            "\n",
            "Step  174300: Ran 100 train steps in 47.97 secs\n",
            "Step  174300: train CrossEntropyLoss |  3.15863037\n",
            "Step  174300: eval  CrossEntropyLoss |  2.79843044\n",
            "Step  174300: eval          Accuracy |  0.49568966\n",
            "\n",
            "Step  174400: Ran 100 train steps in 48.10 secs\n",
            "Step  174400: train CrossEntropyLoss |  3.06364274\n",
            "Step  174400: eval  CrossEntropyLoss |  3.27580833\n",
            "Step  174400: eval          Accuracy |  0.44000003\n",
            "\n",
            "Step  174500: Ran 100 train steps in 47.83 secs\n",
            "Step  174500: train CrossEntropyLoss |  3.11360884\n",
            "Step  174500: eval  CrossEntropyLoss |  3.49219108\n",
            "Step  174500: eval          Accuracy |  0.43564355\n",
            "\n",
            "Step  174600: Ran 100 train steps in 47.82 secs\n",
            "Step  174600: train CrossEntropyLoss |  3.10276198\n",
            "Step  174600: eval  CrossEntropyLoss |  2.75119901\n",
            "Step  174600: eval          Accuracy |  0.51785719\n",
            "\n",
            "Step  174700: Ran 100 train steps in 47.91 secs\n",
            "Step  174700: train CrossEntropyLoss |  3.07688022\n",
            "Step  174700: eval  CrossEntropyLoss |  2.49314070\n",
            "Step  174700: eval          Accuracy |  0.57943922\n",
            "\n",
            "Step  174800: Ran 100 train steps in 47.90 secs\n",
            "Step  174800: train CrossEntropyLoss |  3.04410100\n",
            "Step  174800: eval  CrossEntropyLoss |  2.83285618\n",
            "Step  174800: eval          Accuracy |  0.58536583\n",
            "\n",
            "Step  174900: Ran 100 train steps in 48.26 secs\n",
            "Step  174900: train CrossEntropyLoss |  3.11031055\n",
            "Step  174900: eval  CrossEntropyLoss |  3.46379447\n",
            "Step  174900: eval          Accuracy |  0.37614676\n",
            "\n",
            "Step  175000: Ran 100 train steps in 47.89 secs\n",
            "Step  175000: train CrossEntropyLoss |  3.04752493\n",
            "Step  175000: eval  CrossEntropyLoss |  2.83052611\n",
            "Step  175000: eval          Accuracy |  0.51162791\n",
            "\n",
            "Step  175100: Ran 100 train steps in 47.76 secs\n",
            "Step  175100: train CrossEntropyLoss |  3.09177780\n",
            "Step  175100: eval  CrossEntropyLoss |  3.08257747\n",
            "Step  175100: eval          Accuracy |  0.46534652\n",
            "\n",
            "Step  175200: Ran 100 train steps in 47.83 secs\n",
            "Step  175200: train CrossEntropyLoss |  3.12404633\n",
            "Step  175200: eval  CrossEntropyLoss |  2.99589515\n",
            "Step  175200: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  175300: Ran 100 train steps in 47.72 secs\n",
            "Step  175300: train CrossEntropyLoss |  3.04802275\n",
            "Step  175300: eval  CrossEntropyLoss |  2.80447960\n",
            "Step  175300: eval          Accuracy |  0.50627613\n",
            "\n",
            "Step  175400: Ran 100 train steps in 48.06 secs\n",
            "Step  175400: train CrossEntropyLoss |  3.13801670\n",
            "Step  175400: eval  CrossEntropyLoss |  4.13711071\n",
            "Step  175400: eval          Accuracy |  0.35135135\n",
            "\n",
            "Step  175500: Ran 100 train steps in 47.79 secs\n",
            "Step  175500: train CrossEntropyLoss |  3.17813706\n",
            "Step  175500: eval  CrossEntropyLoss |  3.15938544\n",
            "Step  175500: eval          Accuracy |  0.49557522\n",
            "\n",
            "Step  175600: Ran 100 train steps in 47.75 secs\n",
            "Step  175600: train CrossEntropyLoss |  3.07816482\n",
            "Step  175600: eval  CrossEntropyLoss |  2.58014774\n",
            "Step  175600: eval          Accuracy |  0.53072631\n",
            "\n",
            "Step  175700: Ran 100 train steps in 47.75 secs\n",
            "Step  175700: train CrossEntropyLoss |  3.08378744\n",
            "Step  175700: eval  CrossEntropyLoss |  3.61067319\n",
            "Step  175700: eval          Accuracy |  0.39361700\n",
            "\n",
            "Step  175800: Ran 100 train steps in 47.71 secs\n",
            "Step  175800: train CrossEntropyLoss |  3.07426596\n",
            "Step  175800: eval  CrossEntropyLoss |  4.00862217\n",
            "Step  175800: eval          Accuracy |  0.36082476\n",
            "\n",
            "Step  175900: Ran 100 train steps in 47.89 secs\n",
            "Step  175900: train CrossEntropyLoss |  3.01803970\n",
            "Step  175900: eval  CrossEntropyLoss |  2.68671107\n",
            "Step  175900: eval          Accuracy |  0.52777779\n",
            "\n",
            "Step  176000: Ran 100 train steps in 47.86 secs\n",
            "Step  176000: train CrossEntropyLoss |  3.10350418\n",
            "Step  176000: eval  CrossEntropyLoss |  3.50989079\n",
            "Step  176000: eval          Accuracy |  0.44954127\n",
            "\n",
            "Step  176100: Ran 100 train steps in 47.77 secs\n",
            "Step  176100: train CrossEntropyLoss |  3.06791687\n",
            "Step  176100: eval  CrossEntropyLoss |  3.41586566\n",
            "Step  176100: eval          Accuracy |  0.47593585\n",
            "\n",
            "Step  176200: Ran 100 train steps in 47.87 secs\n",
            "Step  176200: train CrossEntropyLoss |  3.11980438\n",
            "Step  176200: eval  CrossEntropyLoss |  2.63223004\n",
            "Step  176200: eval          Accuracy |  0.56310678\n",
            "\n",
            "Step  176300: Ran 100 train steps in 47.87 secs\n",
            "Step  176300: train CrossEntropyLoss |  3.07749534\n",
            "Step  176300: eval  CrossEntropyLoss |  3.94147086\n",
            "Step  176300: eval          Accuracy |  0.42718446\n",
            "\n",
            "Step  176400: Ran 100 train steps in 47.88 secs\n",
            "Step  176400: train CrossEntropyLoss |  3.07680416\n",
            "Step  176400: eval  CrossEntropyLoss |  3.28747749\n",
            "Step  176400: eval          Accuracy |  0.37168142\n",
            "\n",
            "Step  176500: Ran 100 train steps in 47.68 secs\n",
            "Step  176500: train CrossEntropyLoss |  3.10283065\n",
            "Step  176500: eval  CrossEntropyLoss |  2.90688133\n",
            "Step  176500: eval          Accuracy |  0.48484850\n",
            "\n",
            "Step  176600: Ran 100 train steps in 47.83 secs\n",
            "Step  176600: train CrossEntropyLoss |  3.05670071\n",
            "Step  176600: eval  CrossEntropyLoss |  3.26148677\n",
            "Step  176600: eval          Accuracy |  0.40500000\n",
            "\n",
            "Step  176700: Ran 100 train steps in 47.86 secs\n",
            "Step  176700: train CrossEntropyLoss |  3.05833435\n",
            "Step  176700: eval  CrossEntropyLoss |  3.06879902\n",
            "Step  176700: eval          Accuracy |  0.40000001\n",
            "\n",
            "Step  176800: Ran 100 train steps in 47.69 secs\n",
            "Step  176800: train CrossEntropyLoss |  3.02715230\n",
            "Step  176800: eval  CrossEntropyLoss |  2.84037852\n",
            "Step  176800: eval          Accuracy |  0.53636360\n",
            "\n",
            "Step  176900: Ran 100 train steps in 47.92 secs\n",
            "Step  176900: train CrossEntropyLoss |  3.08433628\n",
            "Step  176900: eval  CrossEntropyLoss |  3.04639792\n",
            "Step  176900: eval          Accuracy |  0.49572653\n",
            "\n",
            "Step  177000: Ran 100 train steps in 47.67 secs\n",
            "Step  177000: train CrossEntropyLoss |  3.03085470\n",
            "Step  177000: eval  CrossEntropyLoss |  3.09959722\n",
            "Step  177000: eval          Accuracy |  0.45685279\n",
            "\n",
            "Step  177100: Ran 100 train steps in 47.70 secs\n",
            "Step  177100: train CrossEntropyLoss |  2.97672319\n",
            "Step  177100: eval  CrossEntropyLoss |  2.37103438\n",
            "Step  177100: eval          Accuracy |  0.55789477\n",
            "\n",
            "Step  177200: Ran 100 train steps in 47.77 secs\n",
            "Step  177200: train CrossEntropyLoss |  3.09053969\n",
            "Step  177200: eval  CrossEntropyLoss |  3.58919191\n",
            "Step  177200: eval          Accuracy |  0.42982456\n",
            "\n",
            "Step  177300: Ran 100 train steps in 47.68 secs\n",
            "Step  177300: train CrossEntropyLoss |  3.11577415\n",
            "Step  177300: eval  CrossEntropyLoss |  2.65148377\n",
            "Step  177300: eval          Accuracy |  0.56000000\n",
            "\n",
            "Step  177400: Ran 100 train steps in 47.91 secs\n",
            "Step  177400: train CrossEntropyLoss |  3.03494525\n",
            "Step  177400: eval  CrossEntropyLoss |  3.31379747\n",
            "Step  177400: eval          Accuracy |  0.47457626\n",
            "\n",
            "Step  177500: Ran 100 train steps in 47.71 secs\n",
            "Step  177500: train CrossEntropyLoss |  3.06895828\n",
            "Step  177500: eval  CrossEntropyLoss |  3.54140925\n",
            "Step  177500: eval          Accuracy |  0.37500000\n",
            "\n",
            "Step  177600: Ran 100 train steps in 48.02 secs\n",
            "Step  177600: train CrossEntropyLoss |  3.05374312\n",
            "Step  177600: eval  CrossEntropyLoss |  3.27371073\n",
            "Step  177600: eval          Accuracy |  0.41361257\n",
            "\n",
            "Step  177700: Ran 100 train steps in 47.77 secs\n",
            "Step  177700: train CrossEntropyLoss |  3.06490374\n",
            "Step  177700: eval  CrossEntropyLoss |  3.15072250\n",
            "Step  177700: eval          Accuracy |  0.48888889\n",
            "\n",
            "Step  177800: Ran 100 train steps in 47.71 secs\n",
            "Step  177800: train CrossEntropyLoss |  3.02315307\n",
            "Step  177800: eval  CrossEntropyLoss |  2.63191581\n",
            "Step  177800: eval          Accuracy |  0.54545450\n",
            "\n",
            "Step  177900: Ran 100 train steps in 47.87 secs\n",
            "Step  177900: train CrossEntropyLoss |  3.06959891\n",
            "Step  177900: eval  CrossEntropyLoss |  2.56228876\n",
            "Step  177900: eval          Accuracy |  0.51724136\n",
            "\n",
            "Step  178000: Ran 100 train steps in 47.76 secs\n",
            "Step  178000: train CrossEntropyLoss |  3.11460376\n",
            "Step  178000: eval  CrossEntropyLoss |  2.48287773\n",
            "Step  178000: eval          Accuracy |  0.56097561\n",
            "\n",
            "Step  178100: Ran 100 train steps in 47.80 secs\n",
            "Step  178100: train CrossEntropyLoss |  3.00864840\n",
            "Step  178100: eval  CrossEntropyLoss |  2.52284956\n",
            "Step  178100: eval          Accuracy |  0.55555558\n",
            "\n",
            "Step  178200: Ran 100 train steps in 47.75 secs\n",
            "Step  178200: train CrossEntropyLoss |  3.00076318\n",
            "Step  178200: eval  CrossEntropyLoss |  3.22700405\n",
            "Step  178200: eval          Accuracy |  0.49107146\n",
            "\n",
            "Step  178300: Ran 100 train steps in 47.80 secs\n",
            "Step  178300: train CrossEntropyLoss |  2.99897051\n",
            "Step  178300: eval  CrossEntropyLoss |  2.78101444\n",
            "Step  178300: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  178400: Ran 100 train steps in 47.79 secs\n",
            "Step  178400: train CrossEntropyLoss |  3.02653074\n",
            "Step  178400: eval  CrossEntropyLoss |  3.62293267\n",
            "Step  178400: eval          Accuracy |  0.41441444\n",
            "\n",
            "Step  178500: Ran 100 train steps in 47.76 secs\n",
            "Step  178500: train CrossEntropyLoss |  3.00968504\n",
            "Step  178500: eval  CrossEntropyLoss |  3.23759174\n",
            "Step  178500: eval          Accuracy |  0.52173913\n",
            "\n",
            "Step  178600: Ran 100 train steps in 47.87 secs\n",
            "Step  178600: train CrossEntropyLoss |  2.96939516\n",
            "Step  178600: eval  CrossEntropyLoss |  2.82839251\n",
            "Step  178600: eval          Accuracy |  0.51041669\n",
            "\n",
            "Step  178700: Ran 100 train steps in 47.82 secs\n",
            "Step  178700: train CrossEntropyLoss |  3.04841733\n",
            "Step  178700: eval  CrossEntropyLoss |  3.09543347\n",
            "Step  178700: eval          Accuracy |  0.44117647\n",
            "\n",
            "Step  178800: Ran 100 train steps in 47.77 secs\n",
            "Step  178800: train CrossEntropyLoss |  3.01307988\n",
            "Step  178800: eval  CrossEntropyLoss |  2.77777648\n",
            "Step  178800: eval          Accuracy |  0.50909090\n",
            "\n",
            "Step  178900: Ran 100 train steps in 48.09 secs\n",
            "Step  178900: train CrossEntropyLoss |  3.10462642\n",
            "Step  178900: eval  CrossEntropyLoss |  3.17913651\n",
            "Step  178900: eval          Accuracy |  0.43750003\n",
            "\n",
            "Step  179000: Ran 100 train steps in 47.73 secs\n",
            "Step  179000: train CrossEntropyLoss |  3.04594302\n",
            "Step  179000: eval  CrossEntropyLoss |  2.92144799\n",
            "Step  179000: eval          Accuracy |  0.49333334\n",
            "\n",
            "Step  179100: Ran 100 train steps in 47.91 secs\n",
            "Step  179100: train CrossEntropyLoss |  2.99547362\n",
            "Step  179100: eval  CrossEntropyLoss |  2.84299779\n",
            "Step  179100: eval          Accuracy |  0.52000004\n",
            "\n",
            "Step  179200: Ran 100 train steps in 47.93 secs\n",
            "Step  179200: train CrossEntropyLoss |  2.96404099\n",
            "Step  179200: eval  CrossEntropyLoss |  2.83023071\n",
            "Step  179200: eval          Accuracy |  0.48369566\n",
            "\n",
            "Step  179300: Ran 100 train steps in 47.83 secs\n",
            "Step  179300: train CrossEntropyLoss |  3.02355361\n",
            "Step  179300: eval  CrossEntropyLoss |  3.50339842\n",
            "Step  179300: eval          Accuracy |  0.42276427\n",
            "\n",
            "Step  179400: Ran 100 train steps in 47.77 secs\n",
            "Step  179400: train CrossEntropyLoss |  2.99358749\n",
            "Step  179400: eval  CrossEntropyLoss |  2.60974479\n",
            "Step  179400: eval          Accuracy |  0.50450450\n",
            "\n",
            "Step  179500: Ran 100 train steps in 47.94 secs\n",
            "Step  179500: train CrossEntropyLoss |  3.01400781\n",
            "Step  179500: eval  CrossEntropyLoss |  3.15247130\n",
            "Step  179500: eval          Accuracy |  0.47899163\n",
            "\n",
            "Step  179600: Ran 100 train steps in 47.84 secs\n",
            "Step  179600: train CrossEntropyLoss |  3.08492637\n",
            "Step  179600: eval  CrossEntropyLoss |  3.27505779\n",
            "Step  179600: eval          Accuracy |  0.43678162\n",
            "\n",
            "Step  179700: Ran 100 train steps in 47.89 secs\n",
            "Step  179700: train CrossEntropyLoss |  3.00028539\n",
            "Step  179700: eval  CrossEntropyLoss |  2.17480254\n",
            "Step  179700: eval          Accuracy |  0.62244898\n",
            "\n",
            "Step  179800: Ran 100 train steps in 47.96 secs\n",
            "Step  179800: train CrossEntropyLoss |  3.01676321\n",
            "Step  179800: eval  CrossEntropyLoss |  3.16030550\n",
            "Step  179800: eval          Accuracy |  0.49246231\n",
            "\n",
            "Step  179900: Ran 100 train steps in 47.94 secs\n",
            "Step  179900: train CrossEntropyLoss |  3.09089231\n",
            "Step  179900: eval  CrossEntropyLoss |  3.34330058\n",
            "Step  179900: eval          Accuracy |  0.45794392\n",
            "\n",
            "Step  180000: Ran 100 train steps in 47.83 secs\n",
            "Step  180000: train CrossEntropyLoss |  2.99435997\n",
            "Step  180000: eval  CrossEntropyLoss |  2.79150271\n",
            "Step  180000: eval          Accuracy |  0.52830189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "# !rsync -a /content/drive/MyDrive/model2/ ~/\r\n",
        "\r\n",
        "# copy the model to Google Drive\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model/\r\n",
        "\r\n",
        "# sync Google Drive dir with the train dir\r\n",
        "# !rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a2c715-60fc-4122-932e-7ff7ccc6cc0b"
      },
      "source": [
        "train_article = train_text_pairs[5][0]\r\n",
        "train_summary = train_text_pairs[5][1]\r\n",
        "print(wrapper.fill(train_article))\r\n",
        "print('')\r\n",
        "eval_article = eval_text_pairs[1][0]\r\n",
        "eval_summary = eval_text_pairs[1][1]\r\n",
        "print(wrapper.fill(eval_article))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые придумали новый способ взаимодействия с графеном, который\n",
            "позволяет избавиться от \"слипающихся\" листов. статья ученых появилась\n",
            "в журнале acs nano, а ее краткое изложение приводится на сайте северо-\n",
            "западного университета, сотрудники которого принимали участие в\n",
            "работе. известно, что основной трудностью при работе с графеновыми\n",
            "листами является то, что при соприкосновении они слипаются под\n",
            "воздействием сил ван-дер-ваальса между собой при наложении друг на\n",
            "друга. это приводит к потере большинства уникальных свойств материала.\n",
            "для решения подобной проблемы, например, некоторые исследователи\n",
            "кладут между листами прокладки из другого материала, однако такое\n",
            "решение часто не слишком эффективно - атомы прокладки могут\n",
            "образовывать связи с атомами углерода в графене, что снова приводит к\n",
            "появлению дефектов в материале. в рамках нового исследования ученые\n",
            "предложили использовать графен не в виде ровных листов, а в виде\n",
            "смятых в комок листов. по словам исследователей, в подобном виде\n",
            "графен ведет себя как бумажные комки в мусорной корзине - несмотря на\n",
            "достаточно плотное расположение, поверхности листов, из которых они\n",
            "состоят, не соприкасаются. расчеты показывают, что при подобной\n",
            "упаковке листов графен сохраняет около 45 процентов исходной площади\n",
            "поверхности. для сравнения, при других способах организации удается\n",
            "спасти не более 16 процентов площади. графен как теоретическая\n",
            "абстракция рассматривался еще в конце 20-х годов прошлого века.\n",
            "начиная с 1960-х годов, он выступал в качестве удобной математической\n",
            "модели для расчетов в квантовой механике. впервые графен получили на\n",
            "практике константин новоселов и андрей гейм в 2004 году.\n",
            "\n",
            "сша планируют сократить численность военного контингента в южной\n",
            "корее. по информации корейского министерства иностранных дел, к концу\n",
            "2005 года из страны будет выведена треть американского контингента,\n",
            "составляющего в настоящее время 37500 военнослужащих, сообщает\n",
            "reuters. всего к концу 2005 года страну покинут 12500 американских\n",
            "солдат. 3600 из них продолжат службу в ираке. глава корейского мид\n",
            "отметил, что сша подходят к выводу войск очень внимательно, так как\n",
            "ситуация на полуострове остается напряженной. тем не менее, сша пошли\n",
            "навстречу желанию властей южной кореи иметь более независимую армию, и\n",
            "обещают оказать им в этом всяческое содействие. собственные силы южной\n",
            "кореи составляют на сегодняшний день 690 000 человек. армия северной\n",
            "кореи насчитывает 1 100 000 военнослужащих.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea653b9-283b-4d20-d344-b0e8a434fcce"
      },
      "source": [
        "# checking first symbol generation\r\n",
        "print(detokenize([next_symbol(tokenize(train_article)+[0], model)]))\r\n",
        "print(detokenize([next_symbol(tokenize(eval_article)+[0], model)]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые\n",
            "сша\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        \r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751a3e14-48e2-4dfa-b2d4-c8291681b5ab"
      },
      "source": [
        "print(train_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(train_article, model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые предложили использовать графен в мятом виде\n",
            "\n",
            "\n",
            "ученые\n",
            "ученые назвали\n",
            "ученые назвали новый\n",
            "ученые назвали новый способ\n",
            "ученые назвали новый способ предотвратить\n",
            "ученые назвали новый способ предотвратить \"\n",
            "ученые назвали новый способ предотвратить \"с\n",
            "ученые назвали новый способ предотвратить \"срон\n",
            "ученые назвали новый способ предотвратить \"сронность\n",
            "ученые назвали новый способ предотвратить \"сронность\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2cd1375-1b2b-434f-ead3-fed229e1bb1e"
      },
      "source": [
        "print(eval_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article, model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша на треть сократят корейскую группировку\n",
            "\n",
            "\n",
            "сша\n",
            "сша сокра\n",
            "сша сократят\n",
            "сша сократят число\n",
            "сша сократят число военных\n",
            "сша сократят число военных в\n",
            "сша сократят число военных в южной\n",
            "сша сократят число военных в южной корее\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L83lEskk4L7"
      },
      "source": [
        "model = SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='eval',\r\n",
        "                  ff_activation=tl.Relu)\r\n",
        "\r\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSDbAXjlF2f"
      },
      "source": [
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True, input_signature=shape11)\r\n",
        "\r\n",
        "# save the starting state\r\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-yINo6McPK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c163f460-e5ce-4b41-d4f3-dc85f032398d"
      },
      "source": [
        "np.array(tokenize(eval_article))[None, :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  401,  5641,  7162, 10989,  4302, 15374,     5,  3396, 11441,\n",
              "        15949,    17,  1205,  8980,   204,  2919,  2799,  1996, 15945,\n",
              "           64,  6453,  2730,   173,    86,   719,   372,   102,  1982,\n",
              "        15926,  1469,  3148, 15374, 15945,  1191,  1839,     5,  1416,\n",
              "          368,  4897, 15974,   423,  4061, 15945,   258,  1761, 15949,\n",
              "          983,    64,  6453,  2730,   173,  4387,  2208,  1132, 14538,\n",
              "          423,  3019,  4848, 15949,  3525,   423,    86,   994,  1869,\n",
              "        15930,  6533,     5,  3961, 15949,   915,  8980,   204,  2872,\n",
              "          996, 15945,    79,   401,   149,  2692,    64,  6696,  3516,\n",
              "         2176,  2557,   833, 15945,   208,   207,  4369,    25, 12391,\n",
              "         4875,  6328, 11725, 15949,   734,    57,  1977, 15945,   401,\n",
              "         6176,    25, 15933,   443,  1323,  3054,   810,  2964,  3396,\n",
              "         5813,  6940,   565,  3121,  4819, 13988, 15945,    16,  4424,\n",
              "          162,  9965,   680,     5,   226,  6550,  7020,    48,  8164,\n",
              "        15949,  9568,  3864,  3396,  5813,  8089,    25,  7553,   892,\n",
              "         9250, 15958, 15924, 10749,   534, 15949,  7248,  3768,  5813,\n",
              "        15210,   107,  1728, 15924, 10749,  4061, 15949,     1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197d3f96-edf8-4e30-aa29-c3d640ad4787"
      },
      "source": [
        "\r\n",
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "model.state = STARTING_STATE\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(tokenize(eval_article) + [0])[None, :],\r\n",
        "                                        temperature=0.3, max_length=20)\r\n",
        "print(wrapper.fill(detokenize(output[0])))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша евро euro евро евро евро евро евро евро евро евро евро евро евро\n",
            "евротяже евро евро рублей евро\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}