{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMbL1yQADGux98iFvJaW5xy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "outputId": "369789be-2bfb-45b5-d7cb-c9c709106dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▋                               | 10kB 27.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 33.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 24.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 40kB 22.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51kB 22.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 61kB 16.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 71kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 81kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 92kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 102kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 112kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 122kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 133kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 143kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 153kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 163kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 174kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 184kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 194kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 204kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 215kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 225kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 235kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 245kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 256kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 266kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 276kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 286kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 296kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 307kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 317kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 327kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 337kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 348kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 358kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 368kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 378kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 389kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 399kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 409kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 419kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 430kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 440kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 450kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 460kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 471kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 481kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 491kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 501kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 512kB 16.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 522kB 16.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 50.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 54.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 57.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 54.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 61.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 49.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 53.6MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "2cdf40aa-445e-442a-889e-6702478bd3f5"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "2b8888be-f76c-490b-d990-be9d8d7449b0"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "a2be42da-8ccc-49fd-9962-7c08edeb86f1"
      },
      "source": [
        "data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7faddbc55048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZ9ElEQVR4nO3df5AcZ53f8fcn0skIG1sSJhOXpNyKQyElrOSQtmylOKg1IvLKcMhJDCWXC605Haor7ItJlAL5qEQU4Co7Vz4fTsBXOqSSRHyWfT4oqc5yhCI8oa4qMv6J17IxWsvirC1ZCpaQb7HBt+SbP/pZ6Fvm2fX07MyOpc+ramq7v/083d/pne3vdvczM4oIzMzMGvlH052AmZl1LxcJMzPLcpEwM7MsFwkzM8tykTAzs6yZ053AVLv44oujp6enUt+f/vSnnH/++VOb0BRwXs1xXs3p1ryge3M7G/N67LHHfhwR7/i1BRFxVj2WL18eVT300EOV+7aT82qO82pOt+YV0b25nY15AY9Gg2OqLzeZmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWdZZ97EcZt1qcPgM1296YFq2ffTWD0/Ldu3Nz2cSZmaW5SJhZmZZLhJmZpblImFmZlmTFglJ2ySdlPR0g2UbJYWki9O8JN0paUjSU5KWldoOSDqcHgOl+HJJg6nPnZKU4vMk7U/t90uaOzVP2czM3qg3ciaxHegfH5S0EFgF/G0pvBpYnB4bgLtS23nAZuBy4DJgc+mgfxfwqVK/sW1tAg5ExGLgQJo3M7MOmrRIRMR3gVMNFt0BfBaIUmwNsDN9h8VBYI6kS4Argf0RcSoiTgP7gf607MKIOJi+9GIncHVpXTvS9I5S3MzMOqTS+yQkrQGGI+L76erQmPnAi6X5Yyk2UfxYgzhALSKOp+mXgNoE+WygOHOhVqtRr9ebfEaFkZGRyn3byXk1p1vzqs2GjUtHp2XbE+2Pbt1f0L25nUt5NV0kJL0V+COKS00dEREhKSZYvgXYAtDb2xt9fX2VtlOv16nat52cV3O6Na//dvdubh+cnvevHr2uL7usW/cXdG9u51JeVUY3/RawCPi+pKPAAuBxSf8EGAYWltouSLGJ4gsaxAFOpMtRpJ8nK+RqZmYtaLpIRMRgRPzjiOiJiB6KS0TLIuIlYA+wLo1yWgGcSZeM9gGrJM1NN6xXAfvSslckrUijmtYBu9Om9gBjo6AGSnEzM+uQNzIE9h7g/wDvlnRM0voJmu8FjgBDwJ8DnwaIiFPAl4BH0uOLKUZq8/XU53ngwRS/FfjXkg4DH0rzZmbWQZNeII2IaydZ3lOaDuCGTLttwLYG8UeBSxvEXwZWTpafmZm1j99xbWZmWS4SZmaW5SJhZmZZ/tIhs3NAzwRfdrRx6WjbvgzJX3b05uczCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7OsSYuEpG2STkp6uhT7Y0k/kPSUpG9JmlNadrOkIUnPSbqyFO9PsSFJm0rxRZIeTvF7Jc1K8fPS/FBa3jNVT9rMzN6YN3ImsR3oHxfbD1waEf8C+CFwM4CkJcBa4D2pz9ckzZA0A/gqsBpYAlyb2gLcBtwREe8CTgPrU3w9cDrF70jtzMysgyYtEhHxXeDUuNi3I2I0zR4EFqTpNcCuiPh5RLwADAGXpcdQRByJiNeBXcAaSQI+CNyf+u8Ari6ta0eavh9YmdqbmVmHTMV3XP8ecG+ank9RNMYcSzGAF8fFLwfeDvykVHDK7eeP9YmIUUlnUvsfj09A0gZgA0CtVqNer1d6IiMjI5X7tpPzas5keQ0On+lcMiW12cX3SXebdubV6uvjzfoamy7tyKulIiHp88AocPfUpFNNRGwBtgD09vZGX19fpfXU63Wq9m0n59WcyfK6ftMDnUumZOPSUW4fnIr/y6ZWO/M6el1fS/3frK+x6dKOvCq/MiRdD3wEWBkRkcLDwMJSswUpRib+MjBH0sx0NlFuP7auY5JmAhel9mZm1iGVhsBK6gc+C3w0Il4tLdoDrE0jkxYBi4HvAY8Ai9NIplkUN7f3pOLyEHBN6j8A7C6tayBNXwN8p1SMzMysAyY9k5B0D9AHXCzpGLCZYjTTecD+dC/5YET8QUQcknQf8AzFZagbIuIXaT03AvuAGcC2iDiUNvE5YJekLwNPAFtTfCvwDUlDFDfO107B8zUzsyZMWiQi4toG4a0NYmPtbwFuaRDfC+xtED9CMfppfPxnwMcmy8/MzNrH77g2M7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8vqvu9SNLOzRk+LXxW7celo5a+bPXrrh1vathV8JmFmZlkuEmZmluUiYWZmWS4SZmaWNWmRkLRN0klJT5di8yTtl3Q4/Zyb4pJ0p6QhSU9JWlbqM5DaH5Y0UIovlzSY+twpSRNtw8zMOueNnElsB/rHxTYBByJiMXAgzQOsBhanxwbgLigO+MBm4HLgMmBz6aB/F/CpUr/+SbZhZmYdMmmRiIjvAqfGhdcAO9L0DuDqUnxnFA4CcyRdAlwJ7I+IUxFxGtgP9KdlF0bEwYgIYOe4dTXahpmZdUjV90nUIuJ4mn4JqKXp+cCLpXbHUmyi+LEG8Ym28WskbaA4c6FWq1Gv15t8OoWRkZHKfdvJeTVnsrw2Lh3tXDIltdnTt+2JdGte0Fpu7Xxtvllf+1W0/Ga6iAhJMRXJVN1GRGwBtgD09vZGX19fpe3U63Wq9m0n59WcyfKq+uasVm1cOsrtg933/tVuzQtay+3odX1Tm0zJm/W1X0XV0U0n0qUi0s+TKT4MLCy1W5BiE8UXNIhPtA0zM+uQqkViDzA2QmkA2F2Kr0ujnFYAZ9Ilo33AKklz0w3rVcC+tOwVSSvSqKZ149bVaBtmZtYhk57HSboH6AMulnSMYpTSrcB9ktYDPwI+nprvBa4ChoBXgU8CRMQpSV8CHkntvhgRYzfDP00xgmo28GB6MME2zMysQyYtEhFxbWbRygZtA7ghs55twLYG8UeBSxvEX260DTMz6xy/49rMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJaKhKS/oOkQ5KelnSPpLdIWiTpYUlDku6VNCu1PS/ND6XlPaX13Jziz0m6shTvT7EhSZtaydXMzJpXuUhImg/8e6A3Ii4FZgBrgduAOyLiXcBpYH3qsh44neJ3pHZIWpL6vQfoB74maYakGcBXgdXAEuDa1NbMzDqk1ctNM4HZkmYCbwWOAx8E7k/LdwBXp+k1aZ60fKUkpfiuiPh5RLwADAGXpcdQRByJiNeBXamtmZl1iCKiemfpJuAW4DXg28BNwMF0toCkhcCDEXGppKeB/og4lpY9D1wOfCH1+R8pvhV4MG2iPyJ+P8U/AVweETc2yGMDsAGgVqst37VrV6XnMzIywgUXXFCpbzs5r+ZMltfg8JkOZvMrtdlw4rVp2fSEujUvaC23pfMvmtpkSt6sr/2JXHHFFY9FRO/4+MyqyUiaS/Gf/SLgJ8BfUlwu6riI2AJsAejt7Y2+vr5K66nX61Tt207OqzmT5XX9pgc6l0zJxqWj3D5Y+U+ubbo1L2gtt6PX9U1tMiVv1td+Fa1cbvoQ8EJE/N+I+Hvgm8D7gDnp8hPAAmA4TQ8DCwHS8ouAl8vxcX1ycTMz65BWisTfAiskvTXdW1gJPAM8BFyT2gwAu9P0njRPWv6dKK517QHWptFPi4DFwPeAR4DFabTULIqb23tayNfMzJpU+RwzIh6WdD/wODAKPEFxyecBYJekL6fY1tRlK/ANSUPAKYqDPhFxSNJ9FAVmFLghIn4BIOlGYB/FyKltEXGoar5mZta8li5ERsRmYPO48BGKkUnj2/4M+FhmPbdQ3AAfH98L7G0lRzMzq87vuDYzsywXCTMzy+rOcW9mZi3qaeNQ541LR7NDqY/e+uG2bXc6+EzCzMyyXCTMzCzLRcLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLHxVu02K6PsbZzJrjMwkzM8tykTAzs6yWioSkOZLul/QDSc9K+leS5knaL+lw+jk3tZWkOyUNSXpK0rLSegZS+8OSBkrx5ZIGU587JamVfM3MrDmtnkl8BfifEfHPgX8JPAtsAg5ExGLgQJoHWA0sTo8NwF0AkuYBm4HLgcuAzWOFJbX5VKlff4v5mplZEyoXCUkXAR8AtgJExOsR8RNgDbAjNdsBXJ2m1wA7o3AQmCPpEuBKYH9EnIqI08B+oD8tuzAiDkZEADtL6zIzsw5Qcfyt0FH6bWAL8AzFWcRjwE3AcETMSW0EnI6IOZL+Grg1Iv4mLTsAfA7oA94SEV9O8f8MvAbUU/sPpfj7gc9FxEca5LKB4uyEWq22fNeuXZWe08jICBdccEGlvu10NuY1OHxmirP5ldpsOPFa21ZfmfNqXrfmNlFeS+df1NlkSlr5m7ziiisei4je8fFWhsDOBJYBfxgRD0v6Cr+6tARARISkalWoCRGxhaJg0dvbG319fZXWU6/Xqdq3nc7GvNo5RHXj0lFuH+y+0d3Oq3ndmttEeR29rq+zyZS041jRyj2JY8CxiHg4zd9PUTROpEtFpJ8n0/JhYGGp/4IUmyi+oEHczMw6pHKRiIiXgBclvTuFVlJcetoDjI1QGgB2p+k9wLo0ymkFcCYijgP7gFWS5qYb1quAfWnZK5JWpMtW60rrMjOzDmj1PO4PgbslzQKOAJ+kKDz3SVoP/Aj4eGq7F7gKGAJeTW2JiFOSvgQ8ktp9MSJOpelPA9uB2cCD6WFmZh3SUpGIiCeBX7vRQXFWMb5tADdk1rMN2NYg/ihwaSs5mplZdX7HtZmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVlW930Gr5nZm1hPGz8GfzLb+8+f8nX6TMLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLKvlIiFphqQnJP11ml8k6WFJQ5LulTQrxc9L80NpeU9pHTen+HOSrizF+1NsSNKmVnM1M7PmTMWZxE3As6X524A7IuJdwGlgfYqvB06n+B2pHZKWAGuB9wD9wNdS4ZkBfBVYDSwBrk1tzcysQ1oqEpIWAB8Gvp7mBXwQuD812QFcnabXpHnS8pWp/RpgV0T8PCJeAIaAy9JjKCKORMTrwK7U1szMOqTVz276U+CzwNvS/NuBn0TEaJo/BsxP0/OBFwEiYlTSmdR+PnCwtM5ynxfHxS9vlISkDcAGgFqtRr1er/RkRkZGKvdtp7Mxr41LRydvVFFtdnvXX5Xzal635tatebXjWFG5SEj6CHAyIh6T1Dd1KTUvIrYAWwB6e3ujr69aOvV6nap92+lszOv6Nn4I2salo9w+2H2fXem8mtetuXVrXtv7z5/yY0Urz/J9wEclXQW8BbgQ+AowR9LMdDaxABhO7YeBhcAxSTOBi4CXS/Ex5T65uJmZdUDlexIRcXNELIiIHoobz9+JiOuAh4BrUrMBYHea3pPmScu/ExGR4mvT6KdFwGLge8AjwOI0WmpW2saeqvmamVnz2nG+9Dlgl6QvA08AW1N8K/ANSUPAKYqDPhFxSNJ9wDPAKHBDRPwCQNKNwD5gBrAtIg61IV8zM8uYkiIREXWgnqaPUIxMGt/mZ8DHMv1vAW5pEN8L7J2KHM3MrHl+x7WZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZVvd9k7d1VM+mByr33bh0lOtb6G9m3c9nEmZmluUiYWZmWZWLhKSFkh6S9IykQ5JuSvF5kvZLOpx+zk1xSbpT0pCkpyQtK61rILU/LGmgFF8uaTD1uVOSWnmyZmbWnFbOJEaBjRGxBFgB3CBpCbAJOBARi4EDaR5gNbA4PTYAd0FRVIDNwOXAZcDmscKS2nyq1K+/hXzNzKxJlYtERByPiMfT9N8BzwLzgTXAjtRsB3B1ml4D7IzCQWCOpEuAK4H9EXEqIk4D+4H+tOzCiDgYEQHsLK3LzMw6YEpGN0nqAd4LPAzUIuJ4WvQSUEvT84EXS92OpdhE8WMN4o22v4Hi7IRarUa9Xq/0PEZGRir3bad25rVx6WjlvrXZrfVvF+fVnG7NC7o3t27Nqx3HipaLhKQLgL8CPhMRr5RvG0RESIpWtzGZiNgCbAHo7e2Nvr6+Suup1+tU7dtO7cyrlSGsG5eOcvtg942idl7N6da8oHtz69a8tvefP+XHipZGN0n6DYoCcXdEfDOFT6RLRaSfJ1N8GFhY6r4gxSaKL2gQNzOzDmlldJOArcCzEfEnpUV7gLERSgPA7lJ8XRrltAI4ky5L7QNWSZqbblivAvalZa9IWpG2ta60LjMz64BWzpfeB3wCGJT0ZIr9EXArcJ+k9cCPgI+nZXuBq4Ah4FXgkwARcUrSl4BHUrsvRsSpNP1pYDswG3gwPczMrEMqF4mI+Bsg976FlQ3aB3BDZl3bgG0N4o8Cl1bN0czMWuN3XJuZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlldd83eZ+jejY9kF22ceko10+w3MysXXwmYWZmWS4SZmaW5SJhZmZZXX9PQlI/8BVgBvD1iLi1XdsaHD7ja/9mZiVdfSYhaQbwVWA1sAS4VtKS6c3KzOzc0dVFArgMGIqIIxHxOrALWDPNOZmZnTMUEdOdQ5aka4D+iPj9NP8J4PKIuHFcuw3AhjT7buC5ipu8GPhxxb7t5Lya47ya0615Qffmdjbm9ZsR8Y7xwa6/J/FGRMQWYEur65H0aET0TkFKU8p5Ncd5Nadb84Luze1cyqvbLzcNAwtL8wtSzMzMOqDbi8QjwGJJiyTNAtYCe6Y5JzOzc0ZXX26KiFFJNwL7KIbAbouIQ23cZMuXrNrEeTXHeTWnW/OC7s3tnMmrq29cm5nZ9Or2y01mZjaNXCTMzCzLRSKR1C/pOUlDkjZ1cLsLJT0k6RlJhyTdlOJfkDQs6cn0uKrU5+aU53OSrmxzfkclDaYcHk2xeZL2Szqcfs5NcUm6M+X2lKRlbcrp3aX98qSkVyR9Zjr2maRtkk5KeroUa3r/SBpI7Q9LGmhTXn8s6Qdp29+SNCfFeyS9Vtpvf1bqszz9/odS7mpDXk3/3qb67zWT172lnI5KejLFO7m/cseHzr3GIuKcf1DcFH8eeCcwC/g+sKRD274EWJam3wb8kOIjSL4A/KcG7Zek/M4DFqW8Z7Qxv6PAxeNi/xXYlKY3Abel6auABwEBK4CHO/S7ewn4zenYZ8AHgGXA01X3DzAPOJJ+zk3Tc9uQ1ypgZpq+rZRXT7nduPV8L+WqlPvqNuTV1O+tHX+vjfIat/x24L9Mw/7KHR869hrzmURh2j7+IyKOR8TjafrvgGeB+RN0WQPsioifR8QLwBBF/p20BtiRpncAV5fiO6NwEJgj6ZI257ISeD4ifjRBm7bts4j4LnCqwfaa2T9XAvsj4lREnAb2A/1TnVdEfDsiRtPsQYr3HWWl3C6MiINRHGl2lp7LlOU1gdzvbcr/XifKK50NfBy4Z6J1tGl/5Y4PHXuNuUgU5gMvluaPMfGBui0k9QDvBR5OoRvTKeO2sdNJOp9rAN+W9JiKjz8BqEXE8TT9ElCbptygeO9M+Y+3G/ZZs/tnOvbb71H8xzlmkaQnJP1vSe9Psfkpl07k1czvrdP76/3AiYg4XIp1fH+NOz507DXmItElJF0A/BXwmYh4BbgL+C3gt4HjFKe70+F3ImIZxSfx3iDpA+WF6T+maRlHreINlh8F/jKFumWf/dJ07p8cSZ8HRoG7U+g48E8j4r3AfwT+QtKFHUyp635v41zLP/xHpOP7q8Hx4Zfa/RpzkShM68d/SPoNihfA3RHxTYCIOBERv4iI/wf8Ob+6PNLRXCNiOP08CXwr5XFi7DJS+nlyOnKjKFyPR8SJlGNX7DOa3z8dy0/S9cBHgOvSwYV0OeflNP0YxfX+f5ZyKF+SakteFX5vndxfM4F/C9xbyrej+6vR8YEOvsZcJArT9vEf6XrnVuDZiPiTUrx8Lf/fAGOjLvYAayWdJ2kRsJjiZlk7cjtf0tvGpilufD6dchgbHTEA7C7lti6NsFgBnCmdErfDP/gPrxv2WWl7zeyffcAqSXPTpZZVKTalVHyB12eBj0bEq6X4O1R8dwuS3kmxf46k3F6RtCK9TteVnstU5tXs762Tf68fAn4QEb+8jNTJ/ZU7PtDJ11grd97PpgfFqIAfUvxX8PkObvd3KE4VnwKeTI+rgG8Agym+B7ik1OfzKc/naHH0xCS5vZNi5Mj3gUNj+wV4O3AAOAz8L2BeioviS6KeT7n3tjG384GXgYtKsY7vM4oidRz4e4rrvOur7B+KewRD6fHJNuU1RHFdeux19mep7b9Lv98ngceB3y2tp5fioP088N9Jn9IwxXk1/Xub6r/XRnml+HbgD8a17eT+yh0fOvYa88dymJlZli83mZlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZ1v8HU/k6fiwkw0wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "8354da6e-5b50-435b-8a10-676fcbb2cfae"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "for i in tqdm(range(data.shape[0])):\r\n",
        "    if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:02<00:00, 12741.74it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c630e70e-7467-4a51-c30a-2a9fcd3cad98"
      },
      "source": [
        "text_pairs[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "2b431b78-e137-4819-92ef-c107f2773c42"
      },
      "source": [
        "s0 = text_pairs[10][0]\r\n",
        "text_list = wrapper.wrap(s0[:300])\r\n",
        "for line in text_list:\r\n",
        "    print(line)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "15c8a849-5dbc-4142-980b-7a2ba001b2d3"
      },
      "source": [
        "# inintial shuffling\r\n",
        "random.shuffle(text_pairs)\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74d1cb7-14e6-44ab-aef6-2c1bab26709f"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cadb5b-d539-42f7-b63f-a13d2e757de8"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f03d146-9920-469c-e1b7-715641fceadf"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5e5b25-83d3-4ef8-bab9-2b487eda17f2"
      },
      "source": [
        "tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "print('tokenized:')\r\n",
        "print(tokenized)\r\n",
        "print('len=', len(tokenized))\r\n",
        "detokenized = detokenize(tokenized)\r\n",
        "print('detokenized:')\r\n",
        "print(detokenized)\r\n",
        "print('len=', len(detokenized.split()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized:\n",
            "[3044, 11, 1627, 1080, 25, 1445, 288, 4205, 5442, 15945, 939, 11463, 1410, 164, 13393, 164, 8476, 1]\n",
            "len= 18\n",
            "detokenized:\n",
            "сведения о пассажирах на всех видах транспорта, где используются\n",
            "именные проездные билеты\n",
            "len= 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA-D8rF6JANP",
        "outputId": "f82a7d47-785d-4f39-a864-655223b49158"
      },
      "source": [
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2061 14474  3545  5038 15949     1     0     5  6959   150   462 12501\n",
            "    81  1899   150   761   131   275 15957     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "c93ecae3-128f-4ac0-9a80-6602800b05e3"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "a36094c1-736c-458c-e83f-3073399b5e98"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9272,  1924,  5077,   364,  3621,  3350,   167,    10, 15935,\n",
              "       15977,  3124,  6101,     5,  1229,   163, 11773,   167,  8755,\n",
              "       15977,   868, 14420, 11054, 15945,  1370,  1170,   372,  3773,\n",
              "         171,  2743,  1249, 10268,    25,  2313,    70,  3171,   544,\n",
              "       15945,    41,  5192, 15945,    48,   823, 12045,  1249,   246,\n",
              "         438, 15945,    25,  2313,    70,  4502,   544, 15949,  1081,\n",
              "        6046,  1456,  5898,     5,  2294,  1249,  5324, 15945,  1217,\n",
              "         744,  2485,  5957,   898,   544,  4364,  3080, 15949,    63,\n",
              "         226,  1203,  1530,    81,  3177,   295,  4669,    25, 15935,\n",
              "         452,  2890,    32,  1628,  1056,   846,  9993,  5144,  3300,\n",
              "         642, 15949,    17,   444,  4244,    25, 15935, 15118,    28,\n",
              "          87,  1152,    58, 15945,  6534,     4,  1249,   246,  1290,\n",
              "       12108,  4678,   128, 15945,    79,     5,  2952,  1477,   237,\n",
              "         520,   930,  2597,   379, 15949,   961,    80,  2215,  3494,\n",
              "        1249, 10268, 13881,  4221,  5439,   207,  4633,  7020, 13167,\n",
              "        1682,    16,   149,    95,  6018,   149,  5387,  1448,   183,\n",
              "        2867,  2952, 15945,   570, 15761,  7035,   158,   162,   528,\n",
              "        2802, 15949,     5,   128,   493,   368,    25, 15935,  1700,\n",
              "       15945,    79,  1574, 11579, 15945,    79,  2423,  7192,   680,\n",
              "        4551,  8873,  2952,   744,  1177,  1157,    10,    81,  8823,\n",
              "        3044, 12478,     4,  2931,  1274,   522, 14843,    16,  1591,\n",
              "       13049,    56,   295,   505,   749,   673,    57,  2454, 15945,\n",
              "          17,  1419,  2320, 15945,  3809,   102, 11387,  8584,  3519,\n",
              "       10892,   141,    17,  7600,  5523,    16, 11394,  1996, 15945,\n",
              "        8197,     4,  1249,   246,  1290, 15949,     5,  1612,  1056,\n",
              "          25, 15935,  1880,   535, 10098,    81,  3707,  6967,    17,\n",
              "        1966,  2659,  7919,   255,  4917,     4,  1249,   246,  1290,\n",
              "         294,   939,  2198,  1678,  5764,   882,    53, 15937,   138,\n",
              "        4691,  1430,  5192,   249,  1249, 10268, 15949,  6086,  3667,\n",
              "        3476, 15945,  3577, 15945,    78,  9135,   106,  7563, 12305,\n",
              "        8646,    34,    50, 15945,  3322,  1659,   521,   468, 15945,\n",
              "        4861,   146,  8442,     5,  3211,  1422,   164,  4010,   245,\n",
              "          75,  3551,   521, 15949,   498,  1310, 15945,    79,     5,\n",
              "         278, 11726,  4221,   620,  2018,   521,  1249,   246,   265,\n",
              "       13196,  6307, 15949,     1,     0,  1924,  5077,   364,  3621,\n",
              "        3350,  3124,   171,  2743,    46, 14963, 13787,  1249,    73,\n",
              "        2044,     1,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "b90aaecf-4eae-4cd1-d66e-5c60063ed158"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"/content/drive/MyDrive/model/\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "    # for initial train\r\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.0002)\r\n",
        "    # for re-train\r\n",
        "    # lr_schedule = trax.supervised.lr_schedules.constant(0.0001)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.0001), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "!rsync -a /content/drive/MyDrive/model2/ ~/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "00ff6446-c46f-45e2-d105-1721a05f763b"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(40000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 37412480\n",
            "Step      1: Ran 1 train steps in 41.04 secs\n",
            "Step      1: train CrossEntropyLoss |  9.73515797\n",
            "Step      1: eval  CrossEntropyLoss |  9.72444630\n",
            "Step      1: eval          Accuracy |  0.00000000\n",
            "\n",
            "Step    100: Ran 99 train steps in 63.67 secs\n",
            "Step    100: train CrossEntropyLoss |  9.66046906\n",
            "Step    100: eval  CrossEntropyLoss |  9.49535656\n",
            "Step    100: eval          Accuracy |  0.08290156\n",
            "\n",
            "Step    200: Ran 100 train steps in 65.03 secs\n",
            "Step    200: train CrossEntropyLoss |  9.34736061\n",
            "Step    200: eval  CrossEntropyLoss |  9.30120850\n",
            "Step    200: eval          Accuracy |  0.06504066\n",
            "\n",
            "Step    300: Ran 100 train steps in 47.52 secs\n",
            "Step    300: train CrossEntropyLoss |  9.10520458\n",
            "Step    300: eval  CrossEntropyLoss |  9.03203964\n",
            "Step    300: eval          Accuracy |  0.08421053\n",
            "\n",
            "Step    400: Ran 100 train steps in 47.88 secs\n",
            "Step    400: train CrossEntropyLoss |  8.91625214\n",
            "Step    400: eval  CrossEntropyLoss |  8.88557529\n",
            "Step    400: eval          Accuracy |  0.08510638\n",
            "\n",
            "Step    500: Ran 100 train steps in 48.24 secs\n",
            "Step    500: train CrossEntropyLoss |  8.72412682\n",
            "Step    500: eval  CrossEntropyLoss |  8.60573673\n",
            "Step    500: eval          Accuracy |  0.07843138\n",
            "\n",
            "Step    600: Ran 100 train steps in 47.85 secs\n",
            "Step    600: train CrossEntropyLoss |  8.54007149\n",
            "Step    600: eval  CrossEntropyLoss |  8.52909756\n",
            "Step    600: eval          Accuracy |  0.07766990\n",
            "\n",
            "Step    700: Ran 100 train steps in 48.08 secs\n",
            "Step    700: train CrossEntropyLoss |  8.35231400\n",
            "Step    700: eval  CrossEntropyLoss |  8.26899338\n",
            "Step    700: eval          Accuracy |  0.08080808\n",
            "\n",
            "Step    800: Ran 100 train steps in 48.11 secs\n",
            "Step    800: train CrossEntropyLoss |  8.20597172\n",
            "Step    800: eval  CrossEntropyLoss |  8.36836720\n",
            "Step    800: eval          Accuracy |  0.07476635\n",
            "\n",
            "Step    900: Ran 100 train steps in 47.90 secs\n",
            "Step    900: train CrossEntropyLoss |  8.11060619\n",
            "Step    900: eval  CrossEntropyLoss |  8.09655094\n",
            "Step    900: eval          Accuracy |  0.08988764\n",
            "\n",
            "Step   1000: Ran 100 train steps in 48.15 secs\n",
            "Step   1000: train CrossEntropyLoss |  8.02518654\n",
            "Step   1000: eval  CrossEntropyLoss |  8.10007763\n",
            "Step   1000: eval          Accuracy |  0.07142857\n",
            "\n",
            "Step   1100: Ran 100 train steps in 48.07 secs\n",
            "Step   1100: train CrossEntropyLoss |  7.97716427\n",
            "Step   1100: eval  CrossEntropyLoss |  8.27538013\n",
            "Step   1100: eval          Accuracy |  0.07547170\n",
            "\n",
            "Step   1200: Ran 100 train steps in 48.01 secs\n",
            "Step   1200: train CrossEntropyLoss |  7.93328524\n",
            "Step   1200: eval  CrossEntropyLoss |  8.11181450\n",
            "Step   1200: eval          Accuracy |  0.07017544\n",
            "\n",
            "Step   1300: Ran 100 train steps in 48.07 secs\n",
            "Step   1300: train CrossEntropyLoss |  7.90933752\n",
            "Step   1300: eval  CrossEntropyLoss |  8.13904858\n",
            "Step   1300: eval          Accuracy |  0.07766990\n",
            "\n",
            "Step   1400: Ran 100 train steps in 48.15 secs\n",
            "Step   1400: train CrossEntropyLoss |  7.91934729\n",
            "Step   1400: eval  CrossEntropyLoss |  7.86064386\n",
            "Step   1400: eval          Accuracy |  0.06956521\n",
            "\n",
            "Step   1500: Ran 100 train steps in 48.09 secs\n",
            "Step   1500: train CrossEntropyLoss |  7.91749382\n",
            "Step   1500: eval  CrossEntropyLoss |  8.22591400\n",
            "Step   1500: eval          Accuracy |  0.08333334\n",
            "\n",
            "Step   1600: Ran 100 train steps in 48.26 secs\n",
            "Step   1600: train CrossEntropyLoss |  7.89171267\n",
            "Step   1600: eval  CrossEntropyLoss |  7.69016743\n",
            "Step   1600: eval          Accuracy |  0.06504066\n",
            "\n",
            "Step   1700: Ran 100 train steps in 47.90 secs\n",
            "Step   1700: train CrossEntropyLoss |  7.89057589\n",
            "Step   1700: eval  CrossEntropyLoss |  8.06675339\n",
            "Step   1700: eval          Accuracy |  0.08333334\n",
            "\n",
            "Step   1800: Ran 100 train steps in 47.95 secs\n",
            "Step   1800: train CrossEntropyLoss |  7.88926363\n",
            "Step   1800: eval  CrossEntropyLoss |  8.11610699\n",
            "Step   1800: eval          Accuracy |  0.08163265\n",
            "\n",
            "Step   1900: Ran 100 train steps in 48.20 secs\n",
            "Step   1900: train CrossEntropyLoss |  7.92666006\n",
            "Step   1900: eval  CrossEntropyLoss |  7.87467718\n",
            "Step   1900: eval          Accuracy |  0.07373272\n",
            "\n",
            "Step   2000: Ran 100 train steps in 48.00 secs\n",
            "Step   2000: train CrossEntropyLoss |  7.89601183\n",
            "Step   2000: eval  CrossEntropyLoss |  7.79682827\n",
            "Step   2000: eval          Accuracy |  0.07142857\n",
            "\n",
            "Step   2100: Ran 100 train steps in 48.16 secs\n",
            "Step   2100: train CrossEntropyLoss |  7.91377926\n",
            "Step   2100: eval  CrossEntropyLoss |  7.77911139\n",
            "Step   2100: eval          Accuracy |  0.07339449\n",
            "\n",
            "Step   2200: Ran 100 train steps in 48.18 secs\n",
            "Step   2200: train CrossEntropyLoss |  7.88450909\n",
            "Step   2200: eval  CrossEntropyLoss |  7.91094875\n",
            "Step   2200: eval          Accuracy |  0.06451613\n",
            "\n",
            "Step   2300: Ran 100 train steps in 47.82 secs\n",
            "Step   2300: train CrossEntropyLoss |  7.90816212\n",
            "Step   2300: eval  CrossEntropyLoss |  7.81778812\n",
            "Step   2300: eval          Accuracy |  0.07547170\n",
            "\n",
            "Step   2400: Ran 100 train steps in 47.84 secs\n",
            "Step   2400: train CrossEntropyLoss |  7.90964413\n",
            "Step   2400: eval  CrossEntropyLoss |  7.58306456\n",
            "Step   2400: eval          Accuracy |  0.08080808\n",
            "\n",
            "Step   2500: Ran 100 train steps in 48.12 secs\n",
            "Step   2500: train CrossEntropyLoss |  7.90656042\n",
            "Step   2500: eval  CrossEntropyLoss |  7.80150509\n",
            "Step   2500: eval          Accuracy |  0.07142857\n",
            "\n",
            "Step   2600: Ran 100 train steps in 47.90 secs\n",
            "Step   2600: train CrossEntropyLoss |  7.90187073\n",
            "Step   2600: eval  CrossEntropyLoss |  7.72962332\n",
            "Step   2600: eval          Accuracy |  0.07804878\n",
            "\n",
            "Step   2700: Ran 100 train steps in 48.15 secs\n",
            "Step   2700: train CrossEntropyLoss |  7.90977097\n",
            "Step   2700: eval  CrossEntropyLoss |  7.92332888\n",
            "Step   2700: eval          Accuracy |  0.07017544\n",
            "\n",
            "Step   2800: Ran 100 train steps in 47.93 secs\n",
            "Step   2800: train CrossEntropyLoss |  7.92147017\n",
            "Step   2800: eval  CrossEntropyLoss |  8.03506279\n",
            "Step   2800: eval          Accuracy |  0.08163265\n",
            "\n",
            "Step   2900: Ran 100 train steps in 48.15 secs\n",
            "Step   2900: train CrossEntropyLoss |  7.88001823\n",
            "Step   2900: eval  CrossEntropyLoss |  8.11469650\n",
            "Step   2900: eval          Accuracy |  0.06837607\n",
            "\n",
            "Step   3000: Ran 100 train steps in 48.06 secs\n",
            "Step   3000: train CrossEntropyLoss |  7.89059496\n",
            "Step   3000: eval  CrossEntropyLoss |  7.98425388\n",
            "Step   3000: eval          Accuracy |  0.08163265\n",
            "\n",
            "Step   3100: Ran 100 train steps in 47.99 secs\n",
            "Step   3100: train CrossEntropyLoss |  7.88748693\n",
            "Step   3100: eval  CrossEntropyLoss |  7.82700205\n",
            "Step   3100: eval          Accuracy |  0.08205128\n",
            "\n",
            "Step   3200: Ran 100 train steps in 48.11 secs\n",
            "Step   3200: train CrossEntropyLoss |  7.93914223\n",
            "Step   3200: eval  CrossEntropyLoss |  8.33888817\n",
            "Step   3200: eval          Accuracy |  0.06666667\n",
            "\n",
            "Step   3300: Ran 100 train steps in 48.03 secs\n",
            "Step   3300: train CrossEntropyLoss |  7.89217138\n",
            "Step   3300: eval  CrossEntropyLoss |  7.59193230\n",
            "Step   3300: eval          Accuracy |  0.08421053\n",
            "\n",
            "Step   3400: Ran 100 train steps in 48.00 secs\n",
            "Step   3400: train CrossEntropyLoss |  7.88664818\n",
            "Step   3400: eval  CrossEntropyLoss |  7.70415592\n",
            "Step   3400: eval          Accuracy |  0.06504066\n",
            "\n",
            "Step   3500: Ran 100 train steps in 48.13 secs\n",
            "Step   3500: train CrossEntropyLoss |  7.84578466\n",
            "Step   3500: eval  CrossEntropyLoss |  7.78129053\n",
            "Step   3500: eval          Accuracy |  0.07731959\n",
            "\n",
            "Step   3600: Ran 100 train steps in 48.20 secs\n",
            "Step   3600: train CrossEntropyLoss |  7.86476135\n",
            "Step   3600: eval  CrossEntropyLoss |  7.78035784\n",
            "Step   3600: eval          Accuracy |  0.09345794\n",
            "\n",
            "Step   3700: Ran 100 train steps in 48.15 secs\n",
            "Step   3700: train CrossEntropyLoss |  7.84291553\n",
            "Step   3700: eval  CrossEntropyLoss |  7.89098930\n",
            "Step   3700: eval          Accuracy |  0.06611570\n",
            "\n",
            "Step   3800: Ran 100 train steps in 48.29 secs\n",
            "Step   3800: train CrossEntropyLoss |  7.79532337\n",
            "Step   3800: eval  CrossEntropyLoss |  8.10487270\n",
            "Step   3800: eval          Accuracy |  0.07476635\n",
            "\n",
            "Step   3900: Ran 100 train steps in 48.18 secs\n",
            "Step   3900: train CrossEntropyLoss |  7.77585316\n",
            "Step   3900: eval  CrossEntropyLoss |  7.90289879\n",
            "Step   3900: eval          Accuracy |  0.07438016\n",
            "\n",
            "Step   4000: Ran 100 train steps in 48.23 secs\n",
            "Step   4000: train CrossEntropyLoss |  7.78727102\n",
            "Step   4000: eval  CrossEntropyLoss |  7.75290918\n",
            "Step   4000: eval          Accuracy |  0.07964602\n",
            "\n",
            "Step   4100: Ran 100 train steps in 48.41 secs\n",
            "Step   4100: train CrossEntropyLoss |  7.80632496\n",
            "Step   4100: eval  CrossEntropyLoss |  7.89361286\n",
            "Step   4100: eval          Accuracy |  0.08421053\n",
            "\n",
            "Step   4200: Ran 100 train steps in 48.14 secs\n",
            "Step   4200: train CrossEntropyLoss |  7.75277281\n",
            "Step   4200: eval  CrossEntropyLoss |  7.94592190\n",
            "Step   4200: eval          Accuracy |  0.09183674\n",
            "\n",
            "Step   4300: Ran 100 train steps in 48.12 secs\n",
            "Step   4300: train CrossEntropyLoss |  7.75687647\n",
            "Step   4300: eval  CrossEntropyLoss |  7.39932442\n",
            "Step   4300: eval          Accuracy |  0.09473684\n",
            "\n",
            "Step   4400: Ran 100 train steps in 48.11 secs\n",
            "Step   4400: train CrossEntropyLoss |  7.75558472\n",
            "Step   4400: eval  CrossEntropyLoss |  7.85988903\n",
            "Step   4400: eval          Accuracy |  0.07627118\n",
            "\n",
            "Step   4500: Ran 100 train steps in 48.18 secs\n",
            "Step   4500: train CrossEntropyLoss |  7.72593737\n",
            "Step   4500: eval  CrossEntropyLoss |  7.71237564\n",
            "Step   4500: eval          Accuracy |  0.09289617\n",
            "\n",
            "Step   4600: Ran 100 train steps in 48.17 secs\n",
            "Step   4600: train CrossEntropyLoss |  7.71447563\n",
            "Step   4600: eval  CrossEntropyLoss |  7.86350298\n",
            "Step   4600: eval          Accuracy |  0.07142857\n",
            "\n",
            "Step   4700: Ran 100 train steps in 48.19 secs\n",
            "Step   4700: train CrossEntropyLoss |  7.71199799\n",
            "Step   4700: eval  CrossEntropyLoss |  8.11520100\n",
            "Step   4700: eval          Accuracy |  0.06451613\n",
            "\n",
            "Step   4800: Ran 100 train steps in 48.31 secs\n",
            "Step   4800: train CrossEntropyLoss |  7.69379997\n",
            "Step   4800: eval  CrossEntropyLoss |  7.45850706\n",
            "Step   4800: eval          Accuracy |  0.10650888\n",
            "\n",
            "Step   4900: Ran 100 train steps in 47.89 secs\n",
            "Step   4900: train CrossEntropyLoss |  7.68532372\n",
            "Step   4900: eval  CrossEntropyLoss |  7.47095680\n",
            "Step   4900: eval          Accuracy |  0.06153846\n",
            "\n",
            "Step   5000: Ran 100 train steps in 48.35 secs\n",
            "Step   5000: train CrossEntropyLoss |  7.67327309\n",
            "Step   5000: eval  CrossEntropyLoss |  7.75581884\n",
            "Step   5000: eval          Accuracy |  0.12222222\n",
            "\n",
            "Step   5100: Ran 100 train steps in 47.92 secs\n",
            "Step   5100: train CrossEntropyLoss |  7.65768003\n",
            "Step   5100: eval  CrossEntropyLoss |  7.48474407\n",
            "Step   5100: eval          Accuracy |  0.10769231\n",
            "\n",
            "Step   5200: Ran 100 train steps in 48.42 secs\n",
            "Step   5200: train CrossEntropyLoss |  7.62097168\n",
            "Step   5200: eval  CrossEntropyLoss |  7.84449339\n",
            "Step   5200: eval          Accuracy |  0.08163265\n",
            "\n",
            "Step   5300: Ran 100 train steps in 48.02 secs\n",
            "Step   5300: train CrossEntropyLoss |  7.61313820\n",
            "Step   5300: eval  CrossEntropyLoss |  7.54515362\n",
            "Step   5300: eval          Accuracy |  0.06666667\n",
            "\n",
            "Step   5400: Ran 100 train steps in 48.04 secs\n",
            "Step   5400: train CrossEntropyLoss |  7.59900427\n",
            "Step   5400: eval  CrossEntropyLoss |  7.44412565\n",
            "Step   5400: eval          Accuracy |  0.10555556\n",
            "\n",
            "Step   5500: Ran 100 train steps in 48.11 secs\n",
            "Step   5500: train CrossEntropyLoss |  7.58899212\n",
            "Step   5500: eval  CrossEntropyLoss |  7.29905367\n",
            "Step   5500: eval          Accuracy |  0.07086614\n",
            "\n",
            "Step   5600: Ran 100 train steps in 48.13 secs\n",
            "Step   5600: train CrossEntropyLoss |  7.57281256\n",
            "Step   5600: eval  CrossEntropyLoss |  7.73824883\n",
            "Step   5600: eval          Accuracy |  0.08035715\n",
            "\n",
            "Step   5700: Ran 100 train steps in 48.16 secs\n",
            "Step   5700: train CrossEntropyLoss |  7.58676291\n",
            "Step   5700: eval  CrossEntropyLoss |  7.52802277\n",
            "Step   5700: eval          Accuracy |  0.07758620\n",
            "\n",
            "Step   5800: Ran 100 train steps in 48.06 secs\n",
            "Step   5800: train CrossEntropyLoss |  7.55474997\n",
            "Step   5800: eval  CrossEntropyLoss |  7.66077280\n",
            "Step   5800: eval          Accuracy |  0.08695652\n",
            "\n",
            "Step   5900: Ran 100 train steps in 48.19 secs\n",
            "Step   5900: train CrossEntropyLoss |  7.55014563\n",
            "Step   5900: eval  CrossEntropyLoss |  7.63214540\n",
            "Step   5900: eval          Accuracy |  0.07070707\n",
            "\n",
            "Step   6000: Ran 100 train steps in 48.15 secs\n",
            "Step   6000: train CrossEntropyLoss |  7.51122189\n",
            "Step   6000: eval  CrossEntropyLoss |  7.55906868\n",
            "Step   6000: eval          Accuracy |  0.07920792\n",
            "\n",
            "Step   6100: Ran 100 train steps in 48.33 secs\n",
            "Step   6100: train CrossEntropyLoss |  7.47343588\n",
            "Step   6100: eval  CrossEntropyLoss |  7.69141722\n",
            "Step   6100: eval          Accuracy |  0.09139785\n",
            "\n",
            "Step   6200: Ran 100 train steps in 48.08 secs\n",
            "Step   6200: train CrossEntropyLoss |  7.47820616\n",
            "Step   6200: eval  CrossEntropyLoss |  7.58182096\n",
            "Step   6200: eval          Accuracy |  0.08000000\n",
            "\n",
            "Step   6300: Ran 100 train steps in 48.02 secs\n",
            "Step   6300: train CrossEntropyLoss |  7.46924686\n",
            "Step   6300: eval  CrossEntropyLoss |  7.49591064\n",
            "Step   6300: eval          Accuracy |  0.10869566\n",
            "\n",
            "Step   6400: Ran 100 train steps in 47.96 secs\n",
            "Step   6400: train CrossEntropyLoss |  7.49057770\n",
            "Step   6400: eval  CrossEntropyLoss |  7.41735363\n",
            "Step   6400: eval          Accuracy |  0.07476635\n",
            "\n",
            "Step   6500: Ran 100 train steps in 48.10 secs\n",
            "Step   6500: train CrossEntropyLoss |  7.42391825\n",
            "Step   6500: eval  CrossEntropyLoss |  7.12654829\n",
            "Step   6500: eval          Accuracy |  0.09615385\n",
            "\n",
            "Step   6600: Ran 100 train steps in 48.12 secs\n",
            "Step   6600: train CrossEntropyLoss |  7.47157812\n",
            "Step   6600: eval  CrossEntropyLoss |  7.30762339\n",
            "Step   6600: eval          Accuracy |  0.09259260\n",
            "\n",
            "Step   6700: Ran 100 train steps in 48.05 secs\n",
            "Step   6700: train CrossEntropyLoss |  7.39938402\n",
            "Step   6700: eval  CrossEntropyLoss |  7.32151794\n",
            "Step   6700: eval          Accuracy |  0.09239130\n",
            "\n",
            "Step   6800: Ran 100 train steps in 48.10 secs\n",
            "Step   6800: train CrossEntropyLoss |  7.39426947\n",
            "Step   6800: eval  CrossEntropyLoss |  7.35140944\n",
            "Step   6800: eval          Accuracy |  0.10891089\n",
            "\n",
            "Step   6900: Ran 100 train steps in 47.89 secs\n",
            "Step   6900: train CrossEntropyLoss |  7.40188026\n",
            "Step   6900: eval  CrossEntropyLoss |  7.41971207\n",
            "Step   6900: eval          Accuracy |  0.09278351\n",
            "\n",
            "Step   7000: Ran 100 train steps in 48.22 secs\n",
            "Step   7000: train CrossEntropyLoss |  7.41067410\n",
            "Step   7000: eval  CrossEntropyLoss |  7.63115454\n",
            "Step   7000: eval          Accuracy |  0.11224490\n",
            "\n",
            "Step   7100: Ran 100 train steps in 48.07 secs\n",
            "Step   7100: train CrossEntropyLoss |  7.37258720\n",
            "Step   7100: eval  CrossEntropyLoss |  8.14089298\n",
            "Step   7100: eval          Accuracy |  0.07608696\n",
            "\n",
            "Step   7200: Ran 100 train steps in 48.09 secs\n",
            "Step   7200: train CrossEntropyLoss |  7.37691832\n",
            "Step   7200: eval  CrossEntropyLoss |  7.67458582\n",
            "Step   7200: eval          Accuracy |  0.07582939\n",
            "\n",
            "Step   7300: Ran 100 train steps in 48.11 secs\n",
            "Step   7300: train CrossEntropyLoss |  7.35146427\n",
            "Step   7300: eval  CrossEntropyLoss |  7.47932911\n",
            "Step   7300: eval          Accuracy |  0.08910891\n",
            "\n",
            "Step   7400: Ran 100 train steps in 48.21 secs\n",
            "Step   7400: train CrossEntropyLoss |  7.34894323\n",
            "Step   7400: eval  CrossEntropyLoss |  7.41860437\n",
            "Step   7400: eval          Accuracy |  0.09999999\n",
            "\n",
            "Step   7500: Ran 100 train steps in 48.04 secs\n",
            "Step   7500: train CrossEntropyLoss |  7.36089563\n",
            "Step   7500: eval  CrossEntropyLoss |  7.64543772\n",
            "Step   7500: eval          Accuracy |  0.08256880\n",
            "\n",
            "Step   7600: Ran 100 train steps in 48.10 secs\n",
            "Step   7600: train CrossEntropyLoss |  7.33634377\n",
            "Step   7600: eval  CrossEntropyLoss |  7.39528847\n",
            "Step   7600: eval          Accuracy |  0.07031250\n",
            "\n",
            "Step   7700: Ran 100 train steps in 48.11 secs\n",
            "Step   7700: train CrossEntropyLoss |  7.27292585\n",
            "Step   7700: eval  CrossEntropyLoss |  7.33800077\n",
            "Step   7700: eval          Accuracy |  0.11004785\n",
            "\n",
            "Step   7800: Ran 100 train steps in 47.99 secs\n",
            "Step   7800: train CrossEntropyLoss |  7.30157137\n",
            "Step   7800: eval  CrossEntropyLoss |  7.39141321\n",
            "Step   7800: eval          Accuracy |  0.07142857\n",
            "\n",
            "Step   7900: Ran 100 train steps in 48.17 secs\n",
            "Step   7900: train CrossEntropyLoss |  7.35271025\n",
            "Step   7900: eval  CrossEntropyLoss |  7.20925331\n",
            "Step   7900: eval          Accuracy |  0.07339449\n",
            "\n",
            "Step   8000: Ran 100 train steps in 48.07 secs\n",
            "Step   8000: train CrossEntropyLoss |  7.32103300\n",
            "Step   8000: eval  CrossEntropyLoss |  7.23018360\n",
            "Step   8000: eval          Accuracy |  0.08130082\n",
            "\n",
            "Step   8100: Ran 100 train steps in 48.06 secs\n",
            "Step   8100: train CrossEntropyLoss |  7.27422428\n",
            "Step   8100: eval  CrossEntropyLoss |  7.32838249\n",
            "Step   8100: eval          Accuracy |  0.08035715\n",
            "\n",
            "Step   8200: Ran 100 train steps in 48.19 secs\n",
            "Step   8200: train CrossEntropyLoss |  7.31786299\n",
            "Step   8200: eval  CrossEntropyLoss |  6.99859571\n",
            "Step   8200: eval          Accuracy |  0.09009010\n",
            "\n",
            "Step   8300: Ran 100 train steps in 47.97 secs\n",
            "Step   8300: train CrossEntropyLoss |  7.28342962\n",
            "Step   8300: eval  CrossEntropyLoss |  7.24697304\n",
            "Step   8300: eval          Accuracy |  0.07476635\n",
            "\n",
            "Step   8400: Ran 100 train steps in 48.18 secs\n",
            "Step   8400: train CrossEntropyLoss |  7.23918438\n",
            "Step   8400: eval  CrossEntropyLoss |  7.30911970\n",
            "Step   8400: eval          Accuracy |  0.06779661\n",
            "\n",
            "Step   8500: Ran 100 train steps in 48.36 secs\n",
            "Step   8500: train CrossEntropyLoss |  7.23909760\n",
            "Step   8500: eval  CrossEntropyLoss |  7.08328819\n",
            "Step   8500: eval          Accuracy |  0.08108108\n",
            "\n",
            "Step   8600: Ran 100 train steps in 48.44 secs\n",
            "Step   8600: train CrossEntropyLoss |  7.23157692\n",
            "Step   8600: eval  CrossEntropyLoss |  6.95773411\n",
            "Step   8600: eval          Accuracy |  0.08482143\n",
            "\n",
            "Step   8700: Ran 100 train steps in 48.50 secs\n",
            "Step   8700: train CrossEntropyLoss |  7.21606493\n",
            "Step   8700: eval  CrossEntropyLoss |  7.17399120\n",
            "Step   8700: eval          Accuracy |  0.10810811\n",
            "\n",
            "Step   8800: Ran 100 train steps in 48.27 secs\n",
            "Step   8800: train CrossEntropyLoss |  7.22502470\n",
            "Step   8800: eval  CrossEntropyLoss |  7.26734161\n",
            "Step   8800: eval          Accuracy |  0.07446808\n",
            "\n",
            "Step   8900: Ran 100 train steps in 48.34 secs\n",
            "Step   8900: train CrossEntropyLoss |  7.24780798\n",
            "Step   8900: eval  CrossEntropyLoss |  6.93429327\n",
            "Step   8900: eval          Accuracy |  0.12222222\n",
            "\n",
            "Step   9000: Ran 100 train steps in 48.29 secs\n",
            "Step   9000: train CrossEntropyLoss |  7.20970392\n",
            "Step   9000: eval  CrossEntropyLoss |  7.37612629\n",
            "Step   9000: eval          Accuracy |  0.08411215\n",
            "\n",
            "Step   9100: Ran 100 train steps in 48.41 secs\n",
            "Step   9100: train CrossEntropyLoss |  7.22938633\n",
            "Step   9100: eval  CrossEntropyLoss |  7.33526516\n",
            "Step   9100: eval          Accuracy |  0.06666667\n",
            "\n",
            "Step   9200: Ran 100 train steps in 48.48 secs\n",
            "Step   9200: train CrossEntropyLoss |  7.21689558\n",
            "Step   9200: eval  CrossEntropyLoss |  7.28539658\n",
            "Step   9200: eval          Accuracy |  0.07200000\n",
            "\n",
            "Step   9300: Ran 100 train steps in 48.47 secs\n",
            "Step   9300: train CrossEntropyLoss |  7.18606234\n",
            "Step   9300: eval  CrossEntropyLoss |  7.10919380\n",
            "Step   9300: eval          Accuracy |  0.08571429\n",
            "\n",
            "Step   9400: Ran 100 train steps in 48.32 secs\n",
            "Step   9400: train CrossEntropyLoss |  7.18778372\n",
            "Step   9400: eval  CrossEntropyLoss |  6.81569338\n",
            "Step   9400: eval          Accuracy |  0.10256410\n",
            "\n",
            "Step   9500: Ran 100 train steps in 48.25 secs\n",
            "Step   9500: train CrossEntropyLoss |  7.20126534\n",
            "Step   9500: eval  CrossEntropyLoss |  7.01848745\n",
            "Step   9500: eval          Accuracy |  0.07692308\n",
            "\n",
            "Step   9600: Ran 100 train steps in 48.41 secs\n",
            "Step   9600: train CrossEntropyLoss |  7.19024658\n",
            "Step   9600: eval  CrossEntropyLoss |  7.11784029\n",
            "Step   9600: eval          Accuracy |  0.11219512\n",
            "\n",
            "Step   9700: Ran 100 train steps in 48.43 secs\n",
            "Step   9700: train CrossEntropyLoss |  7.14180660\n",
            "Step   9700: eval  CrossEntropyLoss |  7.35527039\n",
            "Step   9700: eval          Accuracy |  0.08910891\n",
            "\n",
            "Step   9800: Ran 100 train steps in 48.42 secs\n",
            "Step   9800: train CrossEntropyLoss |  7.15340948\n",
            "Step   9800: eval  CrossEntropyLoss |  7.20091200\n",
            "Step   9800: eval          Accuracy |  0.09615385\n",
            "\n",
            "Step   9900: Ran 100 train steps in 48.14 secs\n",
            "Step   9900: train CrossEntropyLoss |  7.13233566\n",
            "Step   9900: eval  CrossEntropyLoss |  7.08918858\n",
            "Step   9900: eval          Accuracy |  0.11214953\n",
            "\n",
            "Step  10000: Ran 100 train steps in 48.34 secs\n",
            "Step  10000: train CrossEntropyLoss |  7.14146519\n",
            "Step  10000: eval  CrossEntropyLoss |  7.26629782\n",
            "Step  10000: eval          Accuracy |  0.08715596\n",
            "\n",
            "Step  10100: Ran 100 train steps in 48.23 secs\n",
            "Step  10100: train CrossEntropyLoss |  7.14417601\n",
            "Step  10100: eval  CrossEntropyLoss |  7.37555695\n",
            "Step  10100: eval          Accuracy |  0.08181818\n",
            "\n",
            "Step  10200: Ran 100 train steps in 48.45 secs\n",
            "Step  10200: train CrossEntropyLoss |  7.11492395\n",
            "Step  10200: eval  CrossEntropyLoss |  7.19954205\n",
            "Step  10200: eval          Accuracy |  0.06956521\n",
            "\n",
            "Step  10300: Ran 100 train steps in 48.21 secs\n",
            "Step  10300: train CrossEntropyLoss |  7.09368229\n",
            "Step  10300: eval  CrossEntropyLoss |  7.05363703\n",
            "Step  10300: eval          Accuracy |  0.09278351\n",
            "\n",
            "Step  10400: Ran 100 train steps in 48.44 secs\n",
            "Step  10400: train CrossEntropyLoss |  7.12997246\n",
            "Step  10400: eval  CrossEntropyLoss |  6.93652105\n",
            "Step  10400: eval          Accuracy |  0.09259260\n",
            "\n",
            "Step  10500: Ran 100 train steps in 48.22 secs\n",
            "Step  10500: train CrossEntropyLoss |  7.06170082\n",
            "Step  10500: eval  CrossEntropyLoss |  7.03444004\n",
            "Step  10500: eval          Accuracy |  0.08823530\n",
            "\n",
            "Step  10600: Ran 100 train steps in 48.59 secs\n",
            "Step  10600: train CrossEntropyLoss |  7.11829185\n",
            "Step  10600: eval  CrossEntropyLoss |  7.12449932\n",
            "Step  10600: eval          Accuracy |  0.09389672\n",
            "\n",
            "Step  10700: Ran 100 train steps in 48.37 secs\n",
            "Step  10700: train CrossEntropyLoss |  7.05739546\n",
            "Step  10700: eval  CrossEntropyLoss |  7.10576820\n",
            "Step  10700: eval          Accuracy |  0.09523810\n",
            "\n",
            "Step  10800: Ran 100 train steps in 48.38 secs\n",
            "Step  10800: train CrossEntropyLoss |  7.06359529\n",
            "Step  10800: eval  CrossEntropyLoss |  7.16312599\n",
            "Step  10800: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step  10900: Ran 100 train steps in 48.25 secs\n",
            "Step  10900: train CrossEntropyLoss |  7.05905914\n",
            "Step  10900: eval  CrossEntropyLoss |  6.82963848\n",
            "Step  10900: eval          Accuracy |  0.13725491\n",
            "\n",
            "Step  11000: Ran 100 train steps in 48.40 secs\n",
            "Step  11000: train CrossEntropyLoss |  7.05052519\n",
            "Step  11000: eval  CrossEntropyLoss |  6.56698513\n",
            "Step  11000: eval          Accuracy |  0.11818182\n",
            "\n",
            "Step  11100: Ran 100 train steps in 48.19 secs\n",
            "Step  11100: train CrossEntropyLoss |  7.05446625\n",
            "Step  11100: eval  CrossEntropyLoss |  7.12236643\n",
            "Step  11100: eval          Accuracy |  0.10769231\n",
            "\n",
            "Step  11200: Ran 100 train steps in 48.09 secs\n",
            "Step  11200: train CrossEntropyLoss |  7.03873777\n",
            "Step  11200: eval  CrossEntropyLoss |  7.33618402\n",
            "Step  11200: eval          Accuracy |  0.11304347\n",
            "\n",
            "Step  11300: Ran 100 train steps in 48.00 secs\n",
            "Step  11300: train CrossEntropyLoss |  6.99568415\n",
            "Step  11300: eval  CrossEntropyLoss |  7.16380978\n",
            "Step  11300: eval          Accuracy |  0.08571429\n",
            "\n",
            "Step  11400: Ran 100 train steps in 48.50 secs\n",
            "Step  11400: train CrossEntropyLoss |  6.97317123\n",
            "Step  11400: eval  CrossEntropyLoss |  7.03589296\n",
            "Step  11400: eval          Accuracy |  0.11904762\n",
            "\n",
            "Step  11500: Ran 100 train steps in 48.05 secs\n",
            "Step  11500: train CrossEntropyLoss |  6.96688843\n",
            "Step  11500: eval  CrossEntropyLoss |  6.96990919\n",
            "Step  11500: eval          Accuracy |  0.07207207\n",
            "\n",
            "Step  11600: Ran 100 train steps in 48.40 secs\n",
            "Step  11600: train CrossEntropyLoss |  6.96376038\n",
            "Step  11600: eval  CrossEntropyLoss |  6.96914387\n",
            "Step  11600: eval          Accuracy |  0.06194690\n",
            "\n",
            "Step  11700: Ran 100 train steps in 48.05 secs\n",
            "Step  11700: train CrossEntropyLoss |  6.95783043\n",
            "Step  11700: eval  CrossEntropyLoss |  7.19611979\n",
            "Step  11700: eval          Accuracy |  0.11009174\n",
            "\n",
            "Step  11800: Ran 100 train steps in 48.23 secs\n",
            "Step  11800: train CrossEntropyLoss |  6.95992851\n",
            "Step  11800: eval  CrossEntropyLoss |  6.49746180\n",
            "Step  11800: eval          Accuracy |  0.15639812\n",
            "\n",
            "Step  11900: Ran 100 train steps in 47.90 secs\n",
            "Step  11900: train CrossEntropyLoss |  6.95672274\n",
            "Step  11900: eval  CrossEntropyLoss |  6.85088539\n",
            "Step  11900: eval          Accuracy |  0.10576923\n",
            "\n",
            "Step  12000: Ran 100 train steps in 48.09 secs\n",
            "Step  12000: train CrossEntropyLoss |  6.89446354\n",
            "Step  12000: eval  CrossEntropyLoss |  6.75171757\n",
            "Step  12000: eval          Accuracy |  0.08870967\n",
            "\n",
            "Step  12100: Ran 100 train steps in 47.97 secs\n",
            "Step  12100: train CrossEntropyLoss |  6.97557831\n",
            "Step  12100: eval  CrossEntropyLoss |  6.81487846\n",
            "Step  12100: eval          Accuracy |  0.10810811\n",
            "\n",
            "Step  12200: Ran 100 train steps in 48.13 secs\n",
            "Step  12200: train CrossEntropyLoss |  6.91386414\n",
            "Step  12200: eval  CrossEntropyLoss |  7.09979486\n",
            "Step  12200: eval          Accuracy |  0.07853403\n",
            "\n",
            "Step  12300: Ran 100 train steps in 48.07 secs\n",
            "Step  12300: train CrossEntropyLoss |  6.96788502\n",
            "Step  12300: eval  CrossEntropyLoss |  6.64271545\n",
            "Step  12300: eval          Accuracy |  0.13761467\n",
            "\n",
            "Step  12400: Ran 100 train steps in 47.93 secs\n",
            "Step  12400: train CrossEntropyLoss |  6.91598511\n",
            "Step  12400: eval  CrossEntropyLoss |  6.47641230\n",
            "Step  12400: eval          Accuracy |  0.11818182\n",
            "\n",
            "Step  12500: Ran 100 train steps in 48.08 secs\n",
            "Step  12500: train CrossEntropyLoss |  6.91654825\n",
            "Step  12500: eval  CrossEntropyLoss |  7.16570091\n",
            "Step  12500: eval          Accuracy |  0.07031250\n",
            "\n",
            "Step  12600: Ran 100 train steps in 47.99 secs\n",
            "Step  12600: train CrossEntropyLoss |  6.92122746\n",
            "Step  12600: eval  CrossEntropyLoss |  6.80951977\n",
            "Step  12600: eval          Accuracy |  0.11016949\n",
            "\n",
            "Step  12700: Ran 100 train steps in 47.98 secs\n",
            "Step  12700: train CrossEntropyLoss |  6.90037870\n",
            "Step  12700: eval  CrossEntropyLoss |  6.81076288\n",
            "Step  12700: eval          Accuracy |  0.11872147\n",
            "\n",
            "Step  12800: Ran 100 train steps in 47.88 secs\n",
            "Step  12800: train CrossEntropyLoss |  6.85135221\n",
            "Step  12800: eval  CrossEntropyLoss |  7.01737547\n",
            "Step  12800: eval          Accuracy |  0.08474576\n",
            "\n",
            "Step  12900: Ran 100 train steps in 47.94 secs\n",
            "Step  12900: train CrossEntropyLoss |  6.86006355\n",
            "Step  12900: eval  CrossEntropyLoss |  6.93504620\n",
            "Step  12900: eval          Accuracy |  0.09278351\n",
            "\n",
            "Step  13000: Ran 100 train steps in 47.91 secs\n",
            "Step  13000: train CrossEntropyLoss |  6.84146881\n",
            "Step  13000: eval  CrossEntropyLoss |  7.13016319\n",
            "Step  13000: eval          Accuracy |  0.09259260\n",
            "\n",
            "Step  13100: Ran 100 train steps in 48.27 secs\n",
            "Step  13100: train CrossEntropyLoss |  6.87793016\n",
            "Step  13100: eval  CrossEntropyLoss |  6.86507082\n",
            "Step  13100: eval          Accuracy |  0.12195122\n",
            "\n",
            "Step  13200: Ran 100 train steps in 48.07 secs\n",
            "Step  13200: train CrossEntropyLoss |  6.86242151\n",
            "Step  13200: eval  CrossEntropyLoss |  7.43964911\n",
            "Step  13200: eval          Accuracy |  0.05217391\n",
            "\n",
            "Step  13300: Ran 100 train steps in 47.97 secs\n",
            "Step  13300: train CrossEntropyLoss |  6.85031462\n",
            "Step  13300: eval  CrossEntropyLoss |  6.94435310\n",
            "Step  13300: eval          Accuracy |  0.09259260\n",
            "\n",
            "Step  13400: Ran 100 train steps in 48.15 secs\n",
            "Step  13400: train CrossEntropyLoss |  6.81944990\n",
            "Step  13400: eval  CrossEntropyLoss |  6.85134935\n",
            "Step  13400: eval          Accuracy |  0.10752688\n",
            "\n",
            "Step  13500: Ran 100 train steps in 48.14 secs\n",
            "Step  13500: train CrossEntropyLoss |  6.82728052\n",
            "Step  13500: eval  CrossEntropyLoss |  7.02127838\n",
            "Step  13500: eval          Accuracy |  0.11428572\n",
            "\n",
            "Step  13600: Ran 100 train steps in 48.27 secs\n",
            "Step  13600: train CrossEntropyLoss |  6.80430460\n",
            "Step  13600: eval  CrossEntropyLoss |  6.57166862\n",
            "Step  13600: eval          Accuracy |  0.12820514\n",
            "\n",
            "Step  13700: Ran 100 train steps in 48.17 secs\n",
            "Step  13700: train CrossEntropyLoss |  6.80619001\n",
            "Step  13700: eval  CrossEntropyLoss |  6.72758627\n",
            "Step  13700: eval          Accuracy |  0.11111112\n",
            "\n",
            "Step  13800: Ran 100 train steps in 48.26 secs\n",
            "Step  13800: train CrossEntropyLoss |  6.78565168\n",
            "Step  13800: eval  CrossEntropyLoss |  6.59647036\n",
            "Step  13800: eval          Accuracy |  0.12371135\n",
            "\n",
            "Step  13900: Ran 100 train steps in 48.11 secs\n",
            "Step  13900: train CrossEntropyLoss |  6.78711605\n",
            "Step  13900: eval  CrossEntropyLoss |  6.98266697\n",
            "Step  13900: eval          Accuracy |  0.12612613\n",
            "\n",
            "Step  14000: Ran 100 train steps in 48.27 secs\n",
            "Step  14000: train CrossEntropyLoss |  6.83475924\n",
            "Step  14000: eval  CrossEntropyLoss |  6.82272053\n",
            "Step  14000: eval          Accuracy |  0.11111111\n",
            "\n",
            "Step  14100: Ran 100 train steps in 47.99 secs\n",
            "Step  14100: train CrossEntropyLoss |  6.78205299\n",
            "Step  14100: eval  CrossEntropyLoss |  6.98650789\n",
            "Step  14100: eval          Accuracy |  0.09259260\n",
            "\n",
            "Step  14200: Ran 100 train steps in 48.23 secs\n",
            "Step  14200: train CrossEntropyLoss |  6.79327202\n",
            "Step  14200: eval  CrossEntropyLoss |  6.44012165\n",
            "Step  14200: eval          Accuracy |  0.12280702\n",
            "\n",
            "Step  14300: Ran 100 train steps in 48.04 secs\n",
            "Step  14300: train CrossEntropyLoss |  6.73242950\n",
            "Step  14300: eval  CrossEntropyLoss |  6.68242073\n",
            "Step  14300: eval          Accuracy |  0.11386138\n",
            "\n",
            "Step  14400: Ran 100 train steps in 48.03 secs\n",
            "Step  14400: train CrossEntropyLoss |  6.72405195\n",
            "Step  14400: eval  CrossEntropyLoss |  6.36968803\n",
            "Step  14400: eval          Accuracy |  0.11206897\n",
            "\n",
            "Step  14500: Ran 100 train steps in 48.39 secs\n",
            "Step  14500: train CrossEntropyLoss |  6.79159880\n",
            "Step  14500: eval  CrossEntropyLoss |  7.00020790\n",
            "Step  14500: eval          Accuracy |  0.10309279\n",
            "\n",
            "Step  14600: Ran 100 train steps in 48.21 secs\n",
            "Step  14600: train CrossEntropyLoss |  6.70987797\n",
            "Step  14600: eval  CrossEntropyLoss |  6.36716127\n",
            "Step  14600: eval          Accuracy |  0.13559322\n",
            "\n",
            "Step  14700: Ran 100 train steps in 48.16 secs\n",
            "Step  14700: train CrossEntropyLoss |  6.71397638\n",
            "Step  14700: eval  CrossEntropyLoss |  6.60436296\n",
            "Step  14700: eval          Accuracy |  0.15979382\n",
            "\n",
            "Step  14800: Ran 100 train steps in 48.18 secs\n",
            "Step  14800: train CrossEntropyLoss |  6.71941042\n",
            "Step  14800: eval  CrossEntropyLoss |  6.84820795\n",
            "Step  14800: eval          Accuracy |  0.09900990\n",
            "\n",
            "Step  14900: Ran 100 train steps in 48.07 secs\n",
            "Step  14900: train CrossEntropyLoss |  6.68614864\n",
            "Step  14900: eval  CrossEntropyLoss |  6.51612186\n",
            "Step  14900: eval          Accuracy |  0.11538462\n",
            "\n",
            "Step  15000: Ran 100 train steps in 48.28 secs\n",
            "Step  15000: train CrossEntropyLoss |  6.72171879\n",
            "Step  15000: eval  CrossEntropyLoss |  6.49738598\n",
            "Step  15000: eval          Accuracy |  0.12886599\n",
            "\n",
            "Step  15100: Ran 100 train steps in 48.36 secs\n",
            "Step  15100: train CrossEntropyLoss |  6.72870111\n",
            "Step  15100: eval  CrossEntropyLoss |  6.54504871\n",
            "Step  15100: eval          Accuracy |  0.14285713\n",
            "\n",
            "Step  15200: Ran 100 train steps in 48.44 secs\n",
            "Step  15200: train CrossEntropyLoss |  6.65963507\n",
            "Step  15200: eval  CrossEntropyLoss |  6.45875120\n",
            "Step  15200: eval          Accuracy |  0.11504425\n",
            "\n",
            "Step  15300: Ran 100 train steps in 48.13 secs\n",
            "Step  15300: train CrossEntropyLoss |  6.67360640\n",
            "Step  15300: eval  CrossEntropyLoss |  6.48127842\n",
            "Step  15300: eval          Accuracy |  0.11386138\n",
            "\n",
            "Step  15400: Ran 100 train steps in 47.98 secs\n",
            "Step  15400: train CrossEntropyLoss |  6.64157867\n",
            "Step  15400: eval  CrossEntropyLoss |  6.57495975\n",
            "Step  15400: eval          Accuracy |  0.12396694\n",
            "\n",
            "Step  15500: Ran 100 train steps in 48.23 secs\n",
            "Step  15500: train CrossEntropyLoss |  6.62703276\n",
            "Step  15500: eval  CrossEntropyLoss |  6.69889450\n",
            "Step  15500: eval          Accuracy |  0.12264151\n",
            "\n",
            "Step  15600: Ran 100 train steps in 48.13 secs\n",
            "Step  15600: train CrossEntropyLoss |  6.65101051\n",
            "Step  15600: eval  CrossEntropyLoss |  6.88191366\n",
            "Step  15600: eval          Accuracy |  0.08547009\n",
            "\n",
            "Step  15700: Ran 100 train steps in 48.17 secs\n",
            "Step  15700: train CrossEntropyLoss |  6.66875362\n",
            "Step  15700: eval  CrossEntropyLoss |  6.50421524\n",
            "Step  15700: eval          Accuracy |  0.11965813\n",
            "\n",
            "Step  15800: Ran 100 train steps in 48.10 secs\n",
            "Step  15800: train CrossEntropyLoss |  6.64642668\n",
            "Step  15800: eval  CrossEntropyLoss |  7.00046349\n",
            "Step  15800: eval          Accuracy |  0.09677419\n",
            "\n",
            "Step  15900: Ran 100 train steps in 48.06 secs\n",
            "Step  15900: train CrossEntropyLoss |  6.60471010\n",
            "Step  15900: eval  CrossEntropyLoss |  6.49494791\n",
            "Step  15900: eval          Accuracy |  0.11538462\n",
            "\n",
            "Step  16000: Ran 100 train steps in 48.24 secs\n",
            "Step  16000: train CrossEntropyLoss |  6.61964512\n",
            "Step  16000: eval  CrossEntropyLoss |  6.12351418\n",
            "Step  16000: eval          Accuracy |  0.12962963\n",
            "\n",
            "Step  16100: Ran 100 train steps in 48.20 secs\n",
            "Step  16100: train CrossEntropyLoss |  6.61772585\n",
            "Step  16100: eval  CrossEntropyLoss |  7.19209194\n",
            "Step  16100: eval          Accuracy |  0.10434783\n",
            "\n",
            "Step  16200: Ran 100 train steps in 48.09 secs\n",
            "Step  16200: train CrossEntropyLoss |  6.62373543\n",
            "Step  16200: eval  CrossEntropyLoss |  6.57103109\n",
            "Step  16200: eval          Accuracy |  0.10091743\n",
            "\n",
            "Step  16300: Ran 100 train steps in 47.98 secs\n",
            "Step  16300: train CrossEntropyLoss |  6.56458855\n",
            "Step  16300: eval  CrossEntropyLoss |  6.94360828\n",
            "Step  16300: eval          Accuracy |  0.07826087\n",
            "\n",
            "Step  16400: Ran 100 train steps in 48.15 secs\n",
            "Step  16400: train CrossEntropyLoss |  6.57768488\n",
            "Step  16400: eval  CrossEntropyLoss |  6.38966513\n",
            "Step  16400: eval          Accuracy |  0.11616161\n",
            "\n",
            "Step  16500: Ran 100 train steps in 48.06 secs\n",
            "Step  16500: train CrossEntropyLoss |  6.59755278\n",
            "Step  16500: eval  CrossEntropyLoss |  7.02803040\n",
            "Step  16500: eval          Accuracy |  0.11818182\n",
            "\n",
            "Step  16600: Ran 100 train steps in 48.04 secs\n",
            "Step  16600: train CrossEntropyLoss |  6.59562492\n",
            "Step  16600: eval  CrossEntropyLoss |  6.77657843\n",
            "Step  16600: eval          Accuracy |  0.09375000\n",
            "\n",
            "Step  16700: Ran 100 train steps in 48.05 secs\n",
            "Step  16700: train CrossEntropyLoss |  6.55807686\n",
            "Step  16700: eval  CrossEntropyLoss |  6.68767118\n",
            "Step  16700: eval          Accuracy |  0.10185185\n",
            "\n",
            "Step  16800: Ran 100 train steps in 48.51 secs\n",
            "Step  16800: train CrossEntropyLoss |  6.49612284\n",
            "Step  16800: eval  CrossEntropyLoss |  6.31133032\n",
            "Step  16800: eval          Accuracy |  0.13736264\n",
            "\n",
            "Step  16900: Ran 100 train steps in 47.94 secs\n",
            "Step  16900: train CrossEntropyLoss |  6.54880667\n",
            "Step  16900: eval  CrossEntropyLoss |  6.74463987\n",
            "Step  16900: eval          Accuracy |  0.10000000\n",
            "\n",
            "Step  17000: Ran 100 train steps in 48.27 secs\n",
            "Step  17000: train CrossEntropyLoss |  6.53655386\n",
            "Step  17000: eval  CrossEntropyLoss |  6.82517004\n",
            "Step  17000: eval          Accuracy |  0.09401710\n",
            "\n",
            "Step  17100: Ran 100 train steps in 48.25 secs\n",
            "Step  17100: train CrossEntropyLoss |  6.56333160\n",
            "Step  17100: eval  CrossEntropyLoss |  6.47793102\n",
            "Step  17100: eval          Accuracy |  0.16161616\n",
            "\n",
            "Step  17200: Ran 100 train steps in 48.51 secs\n",
            "Step  17200: train CrossEntropyLoss |  6.52248621\n",
            "Step  17200: eval  CrossEntropyLoss |  6.70061398\n",
            "Step  17200: eval          Accuracy |  0.10924371\n",
            "\n",
            "Step  17300: Ran 100 train steps in 48.00 secs\n",
            "Step  17300: train CrossEntropyLoss |  6.52626801\n",
            "Step  17300: eval  CrossEntropyLoss |  6.56192064\n",
            "Step  17300: eval          Accuracy |  0.12500000\n",
            "\n",
            "Step  17400: Ran 100 train steps in 48.03 secs\n",
            "Step  17400: train CrossEntropyLoss |  6.51046801\n",
            "Step  17400: eval  CrossEntropyLoss |  6.75693989\n",
            "Step  17400: eval          Accuracy |  0.12173913\n",
            "\n",
            "Step  17500: Ran 100 train steps in 48.34 secs\n",
            "Step  17500: train CrossEntropyLoss |  6.49282312\n",
            "Step  17500: eval  CrossEntropyLoss |  6.21054363\n",
            "Step  17500: eval          Accuracy |  0.15789473\n",
            "\n",
            "Step  17600: Ran 100 train steps in 48.20 secs\n",
            "Step  17600: train CrossEntropyLoss |  6.49263668\n",
            "Step  17600: eval  CrossEntropyLoss |  6.41340208\n",
            "Step  17600: eval          Accuracy |  0.13658537\n",
            "\n",
            "Step  17700: Ran 100 train steps in 48.21 secs\n",
            "Step  17700: train CrossEntropyLoss |  6.42566109\n",
            "Step  17700: eval  CrossEntropyLoss |  6.39327192\n",
            "Step  17700: eval          Accuracy |  0.14516129\n",
            "\n",
            "Step  17800: Ran 100 train steps in 48.27 secs\n",
            "Step  17800: train CrossEntropyLoss |  6.47202969\n",
            "Step  17800: eval  CrossEntropyLoss |  6.46135187\n",
            "Step  17800: eval          Accuracy |  0.10084035\n",
            "\n",
            "Step  17900: Ran 100 train steps in 48.41 secs\n",
            "Step  17900: train CrossEntropyLoss |  6.44361877\n",
            "Step  17900: eval  CrossEntropyLoss |  6.70921707\n",
            "Step  17900: eval          Accuracy |  0.10648149\n",
            "\n",
            "Step  18000: Ran 100 train steps in 48.30 secs\n",
            "Step  18000: train CrossEntropyLoss |  6.47465801\n",
            "Step  18000: eval  CrossEntropyLoss |  6.49445868\n",
            "Step  18000: eval          Accuracy |  0.14457832\n",
            "\n",
            "Step  18100: Ran 100 train steps in 48.12 secs\n",
            "Step  18100: train CrossEntropyLoss |  6.43259954\n",
            "Step  18100: eval  CrossEntropyLoss |  6.44291639\n",
            "Step  18100: eval          Accuracy |  0.12631580\n",
            "\n",
            "Step  18200: Ran 100 train steps in 48.38 secs\n",
            "Step  18200: train CrossEntropyLoss |  6.46438265\n",
            "Step  18200: eval  CrossEntropyLoss |  6.72624063\n",
            "Step  18200: eval          Accuracy |  0.12173913\n",
            "\n",
            "Step  18300: Ran 100 train steps in 48.36 secs\n",
            "Step  18300: train CrossEntropyLoss |  6.44947910\n",
            "Step  18300: eval  CrossEntropyLoss |  6.24885035\n",
            "Step  18300: eval          Accuracy |  0.12953369\n",
            "\n",
            "Step  18400: Ran 100 train steps in 48.04 secs\n",
            "Step  18400: train CrossEntropyLoss |  6.45540524\n",
            "Step  18400: eval  CrossEntropyLoss |  6.01049805\n",
            "Step  18400: eval          Accuracy |  0.16393444\n",
            "\n",
            "Step  18500: Ran 100 train steps in 48.07 secs\n",
            "Step  18500: train CrossEntropyLoss |  6.44547653\n",
            "Step  18500: eval  CrossEntropyLoss |  6.31132746\n",
            "Step  18500: eval          Accuracy |  0.09999999\n",
            "\n",
            "Step  18600: Ran 100 train steps in 47.89 secs\n",
            "Step  18600: train CrossEntropyLoss |  6.42956591\n",
            "Step  18600: eval  CrossEntropyLoss |  6.34671402\n",
            "Step  18600: eval          Accuracy |  0.14563107\n",
            "\n",
            "Step  18700: Ran 100 train steps in 47.99 secs\n",
            "Step  18700: train CrossEntropyLoss |  6.40648127\n",
            "Step  18700: eval  CrossEntropyLoss |  6.08763504\n",
            "Step  18700: eval          Accuracy |  0.13364056\n",
            "\n",
            "Step  18800: Ran 100 train steps in 47.81 secs\n",
            "Step  18800: train CrossEntropyLoss |  6.38536119\n",
            "Step  18800: eval  CrossEntropyLoss |  6.10601091\n",
            "Step  18800: eval          Accuracy |  0.15929204\n",
            "\n",
            "Step  18900: Ran 100 train steps in 47.92 secs\n",
            "Step  18900: train CrossEntropyLoss |  6.39605618\n",
            "Step  18900: eval  CrossEntropyLoss |  6.48069429\n",
            "Step  18900: eval          Accuracy |  0.14606741\n",
            "\n",
            "Step  19000: Ran 100 train steps in 47.97 secs\n",
            "Step  19000: train CrossEntropyLoss |  6.38255215\n",
            "Step  19000: eval  CrossEntropyLoss |  6.41440964\n",
            "Step  19000: eval          Accuracy |  0.11794872\n",
            "\n",
            "Step  19100: Ran 100 train steps in 47.69 secs\n",
            "Step  19100: train CrossEntropyLoss |  6.37795782\n",
            "Step  19100: eval  CrossEntropyLoss |  6.20763206\n",
            "Step  19100: eval          Accuracy |  0.12037037\n",
            "\n",
            "Step  19200: Ran 100 train steps in 47.86 secs\n",
            "Step  19200: train CrossEntropyLoss |  6.36258841\n",
            "Step  19200: eval  CrossEntropyLoss |  6.27145863\n",
            "Step  19200: eval          Accuracy |  0.17441860\n",
            "\n",
            "Step  19300: Ran 100 train steps in 47.57 secs\n",
            "Step  19300: train CrossEntropyLoss |  6.34532833\n",
            "Step  19300: eval  CrossEntropyLoss |  6.35349846\n",
            "Step  19300: eval          Accuracy |  0.17142858\n",
            "\n",
            "Step  19400: Ran 100 train steps in 47.73 secs\n",
            "Step  19400: train CrossEntropyLoss |  6.38681650\n",
            "Step  19400: eval  CrossEntropyLoss |  6.66712189\n",
            "Step  19400: eval          Accuracy |  0.10810811\n",
            "\n",
            "Step  19500: Ran 100 train steps in 47.69 secs\n",
            "Step  19500: train CrossEntropyLoss |  6.34994173\n",
            "Step  19500: eval  CrossEntropyLoss |  6.65193224\n",
            "Step  19500: eval          Accuracy |  0.08571429\n",
            "\n",
            "Step  19600: Ran 100 train steps in 47.71 secs\n",
            "Step  19600: train CrossEntropyLoss |  6.36789370\n",
            "Step  19600: eval  CrossEntropyLoss |  6.19141102\n",
            "Step  19600: eval          Accuracy |  0.14851485\n",
            "\n",
            "Step  19700: Ran 100 train steps in 47.84 secs\n",
            "Step  19700: train CrossEntropyLoss |  6.39040518\n",
            "Step  19700: eval  CrossEntropyLoss |  6.18703747\n",
            "Step  19700: eval          Accuracy |  0.09701493\n",
            "\n",
            "Step  19800: Ran 100 train steps in 47.69 secs\n",
            "Step  19800: train CrossEntropyLoss |  6.34151030\n",
            "Step  19800: eval  CrossEntropyLoss |  6.65505791\n",
            "Step  19800: eval          Accuracy |  0.10256411\n",
            "\n",
            "Step  19900: Ran 100 train steps in 47.81 secs\n",
            "Step  19900: train CrossEntropyLoss |  6.35297585\n",
            "Step  19900: eval  CrossEntropyLoss |  6.37861872\n",
            "Step  19900: eval          Accuracy |  0.12173913\n",
            "\n",
            "Step  20000: Ran 100 train steps in 47.65 secs\n",
            "Step  20000: train CrossEntropyLoss |  6.33222628\n",
            "Step  20000: eval  CrossEntropyLoss |  6.23461294\n",
            "Step  20000: eval          Accuracy |  0.15736040\n",
            "\n",
            "Step  20100: Ran 100 train steps in 47.54 secs\n",
            "Step  20100: train CrossEntropyLoss |  6.31740904\n",
            "Step  20100: eval  CrossEntropyLoss |  6.51259327\n",
            "Step  20100: eval          Accuracy |  0.12621360\n",
            "\n",
            "Step  20200: Ran 100 train steps in 47.82 secs\n",
            "Step  20200: train CrossEntropyLoss |  6.29641724\n",
            "Step  20200: eval  CrossEntropyLoss |  6.13819933\n",
            "Step  20200: eval          Accuracy |  0.19130434\n",
            "\n",
            "Step  20300: Ran 100 train steps in 47.59 secs\n",
            "Step  20300: train CrossEntropyLoss |  6.34405804\n",
            "Step  20300: eval  CrossEntropyLoss |  5.93194723\n",
            "Step  20300: eval          Accuracy |  0.15178572\n",
            "\n",
            "Step  20400: Ran 100 train steps in 47.65 secs\n",
            "Step  20400: train CrossEntropyLoss |  6.31549406\n",
            "Step  20400: eval  CrossEntropyLoss |  6.17706919\n",
            "Step  20400: eval          Accuracy |  0.16580312\n",
            "\n",
            "Step  20500: Ran 100 train steps in 47.80 secs\n",
            "Step  20500: train CrossEntropyLoss |  6.30831909\n",
            "Step  20500: eval  CrossEntropyLoss |  6.25735474\n",
            "Step  20500: eval          Accuracy |  0.10185185\n",
            "\n",
            "Step  20600: Ran 100 train steps in 47.64 secs\n",
            "Step  20600: train CrossEntropyLoss |  6.31354618\n",
            "Step  20600: eval  CrossEntropyLoss |  5.86678314\n",
            "Step  20600: eval          Accuracy |  0.16190477\n",
            "\n",
            "Step  20700: Ran 100 train steps in 47.94 secs\n",
            "Step  20700: train CrossEntropyLoss |  6.31754303\n",
            "Step  20700: eval  CrossEntropyLoss |  6.72140646\n",
            "Step  20700: eval          Accuracy |  0.11570247\n",
            "\n",
            "Step  20800: Ran 100 train steps in 47.73 secs\n",
            "Step  20800: train CrossEntropyLoss |  6.29705048\n",
            "Step  20800: eval  CrossEntropyLoss |  6.53889179\n",
            "Step  20800: eval          Accuracy |  0.12621360\n",
            "\n",
            "Step  20900: Ran 100 train steps in 47.82 secs\n",
            "Step  20900: train CrossEntropyLoss |  6.29349470\n",
            "Step  20900: eval  CrossEntropyLoss |  5.85329580\n",
            "Step  20900: eval          Accuracy |  0.18324608\n",
            "\n",
            "Step  21000: Ran 100 train steps in 47.61 secs\n",
            "Step  21000: train CrossEntropyLoss |  6.26566505\n",
            "Step  21000: eval  CrossEntropyLoss |  6.50160360\n",
            "Step  21000: eval          Accuracy |  0.12173913\n",
            "\n",
            "Step  21100: Ran 100 train steps in 47.74 secs\n",
            "Step  21100: train CrossEntropyLoss |  6.31687689\n",
            "Step  21100: eval  CrossEntropyLoss |  6.24572515\n",
            "Step  21100: eval          Accuracy |  0.16000000\n",
            "\n",
            "Step  21200: Ran 100 train steps in 47.74 secs\n",
            "Step  21200: train CrossEntropyLoss |  6.28614378\n",
            "Step  21200: eval  CrossEntropyLoss |  6.52617788\n",
            "Step  21200: eval          Accuracy |  0.10084035\n",
            "\n",
            "Step  21300: Ran 100 train steps in 47.78 secs\n",
            "Step  21300: train CrossEntropyLoss |  6.24829578\n",
            "Step  21300: eval  CrossEntropyLoss |  6.32999039\n",
            "Step  21300: eval          Accuracy |  0.14606741\n",
            "\n",
            "Step  21400: Ran 100 train steps in 47.81 secs\n",
            "Step  21400: train CrossEntropyLoss |  6.29327250\n",
            "Step  21400: eval  CrossEntropyLoss |  6.36456919\n",
            "Step  21400: eval          Accuracy |  0.16836734\n",
            "\n",
            "Step  21500: Ran 100 train steps in 47.73 secs\n",
            "Step  21500: train CrossEntropyLoss |  6.22897005\n",
            "Step  21500: eval  CrossEntropyLoss |  6.28469896\n",
            "Step  21500: eval          Accuracy |  0.14159292\n",
            "\n",
            "Step  21600: Ran 100 train steps in 47.56 secs\n",
            "Step  21600: train CrossEntropyLoss |  6.24979162\n",
            "Step  21600: eval  CrossEntropyLoss |  6.23898649\n",
            "Step  21600: eval          Accuracy |  0.12820514\n",
            "\n",
            "Step  21700: Ran 100 train steps in 47.85 secs\n",
            "Step  21700: train CrossEntropyLoss |  6.25902748\n",
            "Step  21700: eval  CrossEntropyLoss |  6.15635347\n",
            "Step  21700: eval          Accuracy |  0.16499999\n",
            "\n",
            "Step  21800: Ran 100 train steps in 47.67 secs\n",
            "Step  21800: train CrossEntropyLoss |  6.19536543\n",
            "Step  21800: eval  CrossEntropyLoss |  6.52693796\n",
            "Step  21800: eval          Accuracy |  0.13131313\n",
            "\n",
            "Step  21900: Ran 100 train steps in 47.78 secs\n",
            "Step  21900: train CrossEntropyLoss |  6.24706507\n",
            "Step  21900: eval  CrossEntropyLoss |  6.52456522\n",
            "Step  21900: eval          Accuracy |  0.13207547\n",
            "\n",
            "Step  22000: Ran 100 train steps in 47.73 secs\n",
            "Step  22000: train CrossEntropyLoss |  6.22949648\n",
            "Step  22000: eval  CrossEntropyLoss |  6.33109093\n",
            "Step  22000: eval          Accuracy |  0.12962963\n",
            "\n",
            "Step  22100: Ran 100 train steps in 47.67 secs\n",
            "Step  22100: train CrossEntropyLoss |  6.23446989\n",
            "Step  22100: eval  CrossEntropyLoss |  5.62379551\n",
            "Step  22100: eval          Accuracy |  0.18181819\n",
            "\n",
            "Step  22200: Ran 100 train steps in 47.91 secs\n",
            "Step  22200: train CrossEntropyLoss |  6.23831701\n",
            "Step  22200: eval  CrossEntropyLoss |  5.90671015\n",
            "Step  22200: eval          Accuracy |  0.14814815\n",
            "\n",
            "Step  22300: Ran 100 train steps in 47.74 secs\n",
            "Step  22300: train CrossEntropyLoss |  6.22666550\n",
            "Step  22300: eval  CrossEntropyLoss |  5.58727884\n",
            "Step  22300: eval          Accuracy |  0.19148935\n",
            "\n",
            "Step  22400: Ran 100 train steps in 47.86 secs\n",
            "Step  22400: train CrossEntropyLoss |  6.19637060\n",
            "Step  22400: eval  CrossEntropyLoss |  6.30198717\n",
            "Step  22400: eval          Accuracy |  0.12745099\n",
            "\n",
            "Step  22500: Ran 100 train steps in 47.78 secs\n",
            "Step  22500: train CrossEntropyLoss |  6.19657087\n",
            "Step  22500: eval  CrossEntropyLoss |  6.10873079\n",
            "Step  22500: eval          Accuracy |  0.15346535\n",
            "\n",
            "Step  22600: Ran 100 train steps in 47.71 secs\n",
            "Step  22600: train CrossEntropyLoss |  6.18956089\n",
            "Step  22600: eval  CrossEntropyLoss |  5.99373817\n",
            "Step  22600: eval          Accuracy |  0.14678898\n",
            "\n",
            "Step  22700: Ran 100 train steps in 47.75 secs\n",
            "Step  22700: train CrossEntropyLoss |  6.18729639\n",
            "Step  22700: eval  CrossEntropyLoss |  5.90777779\n",
            "Step  22700: eval          Accuracy |  0.15841584\n",
            "\n",
            "Step  22800: Ran 100 train steps in 47.67 secs\n",
            "Step  22800: train CrossEntropyLoss |  6.17973185\n",
            "Step  22800: eval  CrossEntropyLoss |  6.14146900\n",
            "Step  22800: eval          Accuracy |  0.18681319\n",
            "\n",
            "Step  22900: Ran 100 train steps in 47.89 secs\n",
            "Step  22900: train CrossEntropyLoss |  6.16751289\n",
            "Step  22900: eval  CrossEntropyLoss |  5.83876467\n",
            "Step  22900: eval          Accuracy |  0.17127073\n",
            "\n",
            "Step  23000: Ran 100 train steps in 47.88 secs\n",
            "Step  23000: train CrossEntropyLoss |  6.17881870\n",
            "Step  23000: eval  CrossEntropyLoss |  6.35654545\n",
            "Step  23000: eval          Accuracy |  0.11864407\n",
            "\n",
            "Step  23100: Ran 100 train steps in 47.82 secs\n",
            "Step  23100: train CrossEntropyLoss |  6.10912466\n",
            "Step  23100: eval  CrossEntropyLoss |  5.98606539\n",
            "Step  23100: eval          Accuracy |  0.12765957\n",
            "\n",
            "Step  23200: Ran 100 train steps in 47.78 secs\n",
            "Step  23200: train CrossEntropyLoss |  6.18197632\n",
            "Step  23200: eval  CrossEntropyLoss |  6.05569124\n",
            "Step  23200: eval          Accuracy |  0.18518519\n",
            "\n",
            "Step  23300: Ran 100 train steps in 47.57 secs\n",
            "Step  23300: train CrossEntropyLoss |  6.20278263\n",
            "Step  23300: eval  CrossEntropyLoss |  5.99017000\n",
            "Step  23300: eval          Accuracy |  0.13775510\n",
            "\n",
            "Step  23400: Ran 100 train steps in 47.74 secs\n",
            "Step  23400: train CrossEntropyLoss |  6.15624475\n",
            "Step  23400: eval  CrossEntropyLoss |  6.16306448\n",
            "Step  23400: eval          Accuracy |  0.12631580\n",
            "\n",
            "Step  23500: Ran 100 train steps in 47.79 secs\n",
            "Step  23500: train CrossEntropyLoss |  6.13994503\n",
            "Step  23500: eval  CrossEntropyLoss |  6.32859039\n",
            "Step  23500: eval          Accuracy |  0.17431192\n",
            "\n",
            "Step  23600: Ran 100 train steps in 47.78 secs\n",
            "Step  23600: train CrossEntropyLoss |  6.16836452\n",
            "Step  23600: eval  CrossEntropyLoss |  6.27996635\n",
            "Step  23600: eval          Accuracy |  0.11881188\n",
            "\n",
            "Step  23700: Ran 100 train steps in 47.76 secs\n",
            "Step  23700: train CrossEntropyLoss |  6.13796616\n",
            "Step  23700: eval  CrossEntropyLoss |  6.14703703\n",
            "Step  23700: eval          Accuracy |  0.13145541\n",
            "\n",
            "Step  23800: Ran 100 train steps in 47.86 secs\n",
            "Step  23800: train CrossEntropyLoss |  6.09997129\n",
            "Step  23800: eval  CrossEntropyLoss |  5.31410599\n",
            "Step  23800: eval          Accuracy |  0.16346155\n",
            "\n",
            "Step  23900: Ran 100 train steps in 47.56 secs\n",
            "Step  23900: train CrossEntropyLoss |  6.04000664\n",
            "Step  23900: eval  CrossEntropyLoss |  6.41740179\n",
            "Step  23900: eval          Accuracy |  0.12396694\n",
            "\n",
            "Step  24000: Ran 100 train steps in 47.81 secs\n",
            "Step  24000: train CrossEntropyLoss |  6.12752199\n",
            "Step  24000: eval  CrossEntropyLoss |  6.05414534\n",
            "Step  24000: eval          Accuracy |  0.18691587\n",
            "\n",
            "Step  24100: Ran 100 train steps in 47.77 secs\n",
            "Step  24100: train CrossEntropyLoss |  6.11342144\n",
            "Step  24100: eval  CrossEntropyLoss |  6.18393230\n",
            "Step  24100: eval          Accuracy |  0.15094340\n",
            "\n",
            "Step  24200: Ran 100 train steps in 47.80 secs\n",
            "Step  24200: train CrossEntropyLoss |  6.12662029\n",
            "Step  24200: eval  CrossEntropyLoss |  5.98755836\n",
            "Step  24200: eval          Accuracy |  0.14414415\n",
            "\n",
            "Step  24300: Ran 100 train steps in 47.88 secs\n",
            "Step  24300: train CrossEntropyLoss |  6.10146666\n",
            "Step  24300: eval  CrossEntropyLoss |  6.42751312\n",
            "Step  24300: eval          Accuracy |  0.12389380\n",
            "\n",
            "Step  24400: Ran 100 train steps in 47.77 secs\n",
            "Step  24400: train CrossEntropyLoss |  6.08220863\n",
            "Step  24400: eval  CrossEntropyLoss |  6.42009830\n",
            "Step  24400: eval          Accuracy |  0.13592233\n",
            "\n",
            "Step  24500: Ran 100 train steps in 47.79 secs\n",
            "Step  24500: train CrossEntropyLoss |  6.11114120\n",
            "Step  24500: eval  CrossEntropyLoss |  6.07484150\n",
            "Step  24500: eval          Accuracy |  0.17272727\n",
            "\n",
            "Step  24600: Ran 100 train steps in 47.78 secs\n",
            "Step  24600: train CrossEntropyLoss |  6.10840559\n",
            "Step  24600: eval  CrossEntropyLoss |  5.78094053\n",
            "Step  24600: eval          Accuracy |  0.17010310\n",
            "\n",
            "Step  24700: Ran 100 train steps in 47.79 secs\n",
            "Step  24700: train CrossEntropyLoss |  6.07004881\n",
            "Step  24700: eval  CrossEntropyLoss |  6.18529129\n",
            "Step  24700: eval          Accuracy |  0.14150944\n",
            "\n",
            "Step  24800: Ran 100 train steps in 47.85 secs\n",
            "Step  24800: train CrossEntropyLoss |  6.03771591\n",
            "Step  24800: eval  CrossEntropyLoss |  6.31723785\n",
            "Step  24800: eval          Accuracy |  0.14159292\n",
            "\n",
            "Step  24900: Ran 100 train steps in 47.99 secs\n",
            "Step  24900: train CrossEntropyLoss |  6.07025862\n",
            "Step  24900: eval  CrossEntropyLoss |  6.20113611\n",
            "Step  24900: eval          Accuracy |  0.14659686\n",
            "\n",
            "Step  25000: Ran 100 train steps in 47.90 secs\n",
            "Step  25000: train CrossEntropyLoss |  6.03184938\n",
            "Step  25000: eval  CrossEntropyLoss |  6.40267944\n",
            "Step  25000: eval          Accuracy |  0.13636363\n",
            "\n",
            "Step  25100: Ran 100 train steps in 47.93 secs\n",
            "Step  25100: train CrossEntropyLoss |  6.04283142\n",
            "Step  25100: eval  CrossEntropyLoss |  6.39434195\n",
            "Step  25100: eval          Accuracy |  0.13829787\n",
            "\n",
            "Step  25200: Ran 100 train steps in 47.85 secs\n",
            "Step  25200: train CrossEntropyLoss |  6.12665415\n",
            "Step  25200: eval  CrossEntropyLoss |  5.97903872\n",
            "Step  25200: eval          Accuracy |  0.11000000\n",
            "\n",
            "Step  25300: Ran 100 train steps in 47.96 secs\n",
            "Step  25300: train CrossEntropyLoss |  6.08381748\n",
            "Step  25300: eval  CrossEntropyLoss |  5.86941433\n",
            "Step  25300: eval          Accuracy |  0.20999999\n",
            "\n",
            "Step  25400: Ran 100 train steps in 47.80 secs\n",
            "Step  25400: train CrossEntropyLoss |  6.00889874\n",
            "Step  25400: eval  CrossEntropyLoss |  5.80533266\n",
            "Step  25400: eval          Accuracy |  0.14871795\n",
            "\n",
            "Step  25500: Ran 100 train steps in 47.90 secs\n",
            "Step  25500: train CrossEntropyLoss |  6.01349306\n",
            "Step  25500: eval  CrossEntropyLoss |  6.40139818\n",
            "Step  25500: eval          Accuracy |  0.13333334\n",
            "\n",
            "Step  25600: Ran 100 train steps in 47.91 secs\n",
            "Step  25600: train CrossEntropyLoss |  6.03759432\n",
            "Step  25600: eval  CrossEntropyLoss |  6.07924557\n",
            "Step  25600: eval          Accuracy |  0.16379310\n",
            "\n",
            "Step  25700: Ran 100 train steps in 47.78 secs\n",
            "Step  25700: train CrossEntropyLoss |  5.98950005\n",
            "Step  25700: eval  CrossEntropyLoss |  6.36014032\n",
            "Step  25700: eval          Accuracy |  0.12500000\n",
            "\n",
            "Step  25800: Ran 100 train steps in 47.86 secs\n",
            "Step  25800: train CrossEntropyLoss |  5.96422958\n",
            "Step  25800: eval  CrossEntropyLoss |  6.39183855\n",
            "Step  25800: eval          Accuracy |  0.11607143\n",
            "\n",
            "Step  25900: Ran 100 train steps in 47.59 secs\n",
            "Step  25900: train CrossEntropyLoss |  6.04503536\n",
            "Step  25900: eval  CrossEntropyLoss |  5.80923748\n",
            "Step  25900: eval          Accuracy |  0.15228426\n",
            "\n",
            "Step  26000: Ran 100 train steps in 47.81 secs\n",
            "Step  26000: train CrossEntropyLoss |  6.04421759\n",
            "Step  26000: eval  CrossEntropyLoss |  6.35746098\n",
            "Step  26000: eval          Accuracy |  0.13008131\n",
            "\n",
            "Step  26100: Ran 100 train steps in 47.91 secs\n",
            "Step  26100: train CrossEntropyLoss |  6.00445557\n",
            "Step  26100: eval  CrossEntropyLoss |  6.24587393\n",
            "Step  26100: eval          Accuracy |  0.16037735\n",
            "\n",
            "Step  26200: Ran 100 train steps in 47.59 secs\n",
            "Step  26200: train CrossEntropyLoss |  6.02090263\n",
            "Step  26200: eval  CrossEntropyLoss |  5.78405619\n",
            "Step  26200: eval          Accuracy |  0.16101696\n",
            "\n",
            "Step  26300: Ran 100 train steps in 47.89 secs\n",
            "Step  26300: train CrossEntropyLoss |  6.01889706\n",
            "Step  26300: eval  CrossEntropyLoss |  6.25203657\n",
            "Step  26300: eval          Accuracy |  0.13592233\n",
            "\n",
            "Step  26400: Ran 100 train steps in 47.65 secs\n",
            "Step  26400: train CrossEntropyLoss |  5.98706245\n",
            "Step  26400: eval  CrossEntropyLoss |  5.82421255\n",
            "Step  26400: eval          Accuracy |  0.17801048\n",
            "\n",
            "Step  26500: Ran 100 train steps in 47.80 secs\n",
            "Step  26500: train CrossEntropyLoss |  6.01668549\n",
            "Step  26500: eval  CrossEntropyLoss |  6.40668011\n",
            "Step  26500: eval          Accuracy |  0.14414415\n",
            "\n",
            "Step  26600: Ran 100 train steps in 47.77 secs\n",
            "Step  26600: train CrossEntropyLoss |  5.99574900\n",
            "Step  26600: eval  CrossEntropyLoss |  6.18819571\n",
            "Step  26600: eval          Accuracy |  0.10576923\n",
            "\n",
            "Step  26700: Ran 100 train steps in 47.89 secs\n",
            "Step  26700: train CrossEntropyLoss |  5.97868967\n",
            "Step  26700: eval  CrossEntropyLoss |  6.10027504\n",
            "Step  26700: eval          Accuracy |  0.11214953\n",
            "\n",
            "Step  26800: Ran 100 train steps in 47.96 secs\n",
            "Step  26800: train CrossEntropyLoss |  6.00789881\n",
            "Step  26800: eval  CrossEntropyLoss |  5.77148294\n",
            "Step  26800: eval          Accuracy |  0.16230367\n",
            "\n",
            "Step  26900: Ran 100 train steps in 47.90 secs\n",
            "Step  26900: train CrossEntropyLoss |  5.98045826\n",
            "Step  26900: eval  CrossEntropyLoss |  6.28414917\n",
            "Step  26900: eval          Accuracy |  0.12121212\n",
            "\n",
            "Step  27000: Ran 100 train steps in 47.70 secs\n",
            "Step  27000: train CrossEntropyLoss |  6.02296209\n",
            "Step  27000: eval  CrossEntropyLoss |  6.10550308\n",
            "Step  27000: eval          Accuracy |  0.15384616\n",
            "\n",
            "Step  27100: Ran 100 train steps in 47.85 secs\n",
            "Step  27100: train CrossEntropyLoss |  5.98741865\n",
            "Step  27100: eval  CrossEntropyLoss |  5.90569115\n",
            "Step  27100: eval          Accuracy |  0.17368422\n",
            "\n",
            "Step  27200: Ran 100 train steps in 47.54 secs\n",
            "Step  27200: train CrossEntropyLoss |  5.99669647\n",
            "Step  27200: eval  CrossEntropyLoss |  6.05895901\n",
            "Step  27200: eval          Accuracy |  0.14563107\n",
            "\n",
            "Step  27300: Ran 100 train steps in 47.91 secs\n",
            "Step  27300: train CrossEntropyLoss |  5.95945406\n",
            "Step  27300: eval  CrossEntropyLoss |  6.10978889\n",
            "Step  27300: eval          Accuracy |  0.11764707\n",
            "\n",
            "Step  27400: Ran 100 train steps in 47.74 secs\n",
            "Step  27400: train CrossEntropyLoss |  6.01011753\n",
            "Step  27400: eval  CrossEntropyLoss |  6.36915016\n",
            "Step  27400: eval          Accuracy |  0.08943090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dyYwnk_1i56"
      },
      "source": [
        "# copy the model to Google Drive\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPWbvRXFyAC8"
      },
      "source": [
        "# sync Google Drive dir with the train dir\r\n",
        "!rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7067530c-31d1-4c81-a936-c537c93d93ee"
      },
      "source": [
        "train_text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('подразделения мчс россии завершили ликвидацию последствий пожаров в населенных пунктах хакасии, сообщает в четверг, 23 апреля, риа новости со ссылкой на республиканское управление ведомства. «мчс россии начинает поэтапный вывод группировки сил, привлеченных в хакасию для оказания помощи населению при разборе завалов и сгоревших конструкций», — говорится в сообщении главного управления мчс по республике хакасия. первая группа спасателей из 120 человек улетает ведомственным самолетом. на борт загружен аварийно-спасательный инструмент. до конца недели все подразделения мчс россии из других регионов покинут хакасию и вернутся к местам постоянной дислокации. 12 апреля лесные пожары, распространявшиеся на территории хакасии, перекинулись на населенные пункты. пострадало более 40 населенных пунктов, около 5 тысяч человек остались без жилья. уже 15 апреля в мчс сообщили о ликвидации всех пожаров в населенных пунктах хакасии, красноярского края и забайкалья. по последним данным, от пожаров погибли 34 человека, более 600\\xa0обратились за медицинской помощью. основными причинами распространения огня называют неконтролируемый пал травы и сильные порывы ветра. следственный комитет рф после пожаров в хакасии возбудил пять уголовных дел. предварительно ущерб от огня оценен в 7\\xa0миллиардов рублей.',\n",
              " 'мчс завершило ликвидацию последствий пожаров в\\xa0хакасии')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYS0jHDJkAd5"
      },
      "source": [
        "eval_text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU9OvGMPiVLR"
      },
      "source": [
        "# fixed input for comparison purposes\r\n",
        "train_input = \"\"\r\n",
        "eval_input = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "34600992-051b-4a19-b680-efec7d323b62"
      },
      "source": [
        "detokenize([next_symbol(tokenize(train_input)+[0], model)])\r\n",
        "detokenize([next_symbol(tokenize(eval_input)+[0], model)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "26\n",
            "[ 5317  3090   278  2130    18 14638   408  9828 12479     5  9291 15468\n",
            "   710    26  7054 15945   258     5  1801 15945  1628  1215 15945   785\n",
            "  1006 15949]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'в'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "        if len(generated_output) >= 20:\r\n",
        "            print(detokenize(generated_output))\r\n",
        "            break\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "    \r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37f4d576-752b-4c46-9410-9582e7384309"
      },
      "source": [
        "print(greedy_decode(train_input, model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "подразделения мчс россии завершили ликвидацию последствий пожаров в\n",
            "населенных пунктах хакасии, сообщает в четверг, 23 апреля, риа\n",
            "новости. \n",
            "\n",
            "32\n",
            "26\n",
            "[ 5317  3090   278  2130    18 14638   408  9828 12479     5  9291 15468\n",
            "   710    26  7054 15945   258     5  1801 15945  1628  1215 15945   785\n",
            "  1006 15949]\n",
            "в\n",
            "32\n",
            "27\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5]\n",
            "в результате\n",
            "32\n",
            "28\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818]\n",
            "в результате результате\n",
            "32\n",
            "29\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818]\n",
            "в результате результате результате\n",
            "32\n",
            "30\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818]\n",
            "в результате результате результате результате\n",
            "32\n",
            "31\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818]\n",
            "в результате результате результате результате в\n",
            "64\n",
            "32\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5]\n",
            "в результате результате результате результате в результате\n",
            "64\n",
            "33\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818]\n",
            "в результате результате результате результате в результате в\n",
            "64\n",
            "34\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5]\n",
            "в результате результате результате результате в результате в результате\n",
            "64\n",
            "35\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва\n",
            "64\n",
            "36\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869]\n",
            "в результате результате результате результате в результате в результате взрыва в\n",
            "64\n",
            "37\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате\n",
            "64\n",
            "38\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в\n",
            "64\n",
            "39\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате\n",
            "64\n",
            "40\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли\n",
            "64\n",
            "41\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли\n",
            "64\n",
            "42\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли\n",
            "64\n",
            "43\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли\n",
            "64\n",
            "44\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли\n",
            "64\n",
            "45\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли\n",
            "64\n",
            "46\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли\n",
            "64\n",
            "47\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли\n",
            "64\n",
            "48\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в\n",
            "64\n",
            "49\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате\n",
            "64\n",
            "50\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва\n",
            "64\n",
            "51\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в\n",
            "64\n",
            "52\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате\n",
            "64\n",
            "53\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва\n",
            "64\n",
            "54\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в\n",
            "64\n",
            "55\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате\n",
            "64\n",
            "56\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва\n",
            "64\n",
            "57\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в\n",
            "64\n",
            "58\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате\n",
            "64\n",
            "59\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли\n",
            "64\n",
            "60\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли\n",
            "64\n",
            "61\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли\n",
            "64\n",
            "62\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в\n",
            "64\n",
            "63\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате\n",
            "128\n",
            "64\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли\n",
            "128\n",
            "65\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли\n",
            "128\n",
            "66\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли\n",
            "128\n",
            "67\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли\n",
            "128\n",
            "68\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли\n",
            "128\n",
            "69\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли\n",
            "128\n",
            "70\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в\n",
            "128\n",
            "71\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате\n",
            "128\n",
            "72\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли\n",
            "128\n",
            "73\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в\n",
            "128\n",
            "74\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате\n",
            "128\n",
            "75\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли\n",
            "128\n",
            "76\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в\n",
            "128\n",
            "77\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "78\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "79\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "80\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "81\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "82\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "83\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "84\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "85\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "86\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "87\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "88\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "89\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "90\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "91\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "92\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "93\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "94\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в\n",
            "128\n",
            "95\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате\n",
            "128\n",
            "96\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818]\n",
            "в результате результате результате результате в результате в результате взрыва в результате в результате погибли погибли погибли погибли погибли погибли погибли погибли в результате взрыва в результате взрыва в результате взрыва в результате погибли погибли погибли в результате погибли погибли погибли погибли погибли погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли в результате погибли\n",
            "128\n",
            "97\n",
            "[5317, 3090, 278, 2130, 18, 14638, 408, 9828, 12479, 5, 9291, 15468, 710, 26, 7054, 15945, 258, 5, 1801, 15945, 1628, 1215, 15945, 785, 1006, 15949, 5, 818, 818, 818, 818, 5, 818, 5, 818, 2869, 5, 818, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 2869, 5, 818, 2869, 5, 818, 2869, 5, 818, 1515, 1515, 1515, 5, 818, 1515, 1515, 1515, 1515, 1515, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515, 5, 818, 1515]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-28fe3024bd30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"подразделения мчс россии завершили ликвидацию последствий пожаров в населенных пунктах хакасии, сообщает в четверг, 23 апреля, риа новости.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-ed2cb67223e2>\u001b[0m in \u001b[0;36mgreedy_decode\u001b[0;34m(input_sentence, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcur_output\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEOS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Get next symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mcur_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_symbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_output_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Append next symbol to original sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcur_output_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_output_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-58ee115bfca9>\u001b[0m in \u001b[0;36mnext_symbol\u001b[0;34m(cur_output_tokens, model)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# model expects a tuple containing two padded tensors (with batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_with_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_with_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m# HINT: output has shape (1, padded_length, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# To get log_probs you need to index output with 0 in the first dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, weights, state, rng)\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m  \u001b[0;31m# Needed if the model wasn't fully initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;31m# Note that zip silently truncates its result if lengths don't match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m       \u001b[0msub_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_out\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/assert_shape.py\u001b[0m in \u001b[0;36mforward_wrapper\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_assert_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_assert_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trax/layers/core.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m                          f'instead got: {self.weights}')\n\u001b[1;32m     94\u001b[0m       \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m  \u001b[0;31m# Affine map.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[0mcontract_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb_ndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3510\u001b[0m   \u001b[0mbatch_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3511\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontract_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mdot_general\u001b[0;34m(lhs, rhs, dimension_numbers, precision)\u001b[0m\n\u001b[1;32m    668\u001b[0m   return dot_general_p.bind(lhs, rhs,\n\u001b[1;32m    669\u001b[0m                             \u001b[0mdimension_numbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontract_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                             precision=_canonicalize_precision(precision))\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mtop_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    352\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_nans\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx"
      },
      "source": [
        "print(greedy_decode(eval_input, model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW"
      },
      "source": [
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(train_input)[None, :],\r\n",
        "                                        temperature=0.0, max_length=20)\r\n",
        "print(wrapper.fill(output[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yXlt4_BezWi"
      },
      "source": [
        "output = decoding.autoregressive_sample(model, inputs=np.array(eval_input)[None, :],\r\n",
        "                                        temperature=0.0, max_length=20)\r\n",
        "print(wrapper.fill(output[0]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}