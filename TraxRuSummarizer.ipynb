{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOo8MnkwwkfFyaHDt6jxWpT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff83a236-739d-46cb-cdfd-61c6b84e8149"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 15.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 15.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 48.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 49.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 60.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 42.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 58.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 55.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "4597e40d-ba13-47b9-c776-f5fd3e86c0db"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "for i in tqdm(range(data.shape[0])):\r\n",
        "    if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file for tokenizer training\r\n",
        "with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "    file.write('\\n'.join(text_full))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "                               --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "sp = spm.SentencePieceProcessor()\r\n",
        "sp.load('bpe.model')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcS3BZXUdU2L"
      },
      "source": [
        "s0 = text_pairs[10][0]\r\n",
        "text_list = wrapper.wrap(s0[:300])\r\n",
        "for line in text_list:\r\n",
        "    print(line)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# tokenizer check\r\n",
        "print('encode: text => id:')\r\n",
        "print(sp.encode_as_pieces(s0[:300]))\r\n",
        "print('')\r\n",
        "print(sp.encode_as_ids(s0[:300]))\r\n",
        "print('')\r\n",
        "print('decode: id => text:')\r\n",
        "print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "print('')\r\n",
        "print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "print(f'Pad id: {sp.pad_id()}')\r\n",
        "print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "print(f'Unknown id: {sp.unk_id()}')\r\n",
        "print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLsMmUFFnHA",
        "outputId": "1e096ef5-ee03-4e4e-9cf4-5a8541fe759f"
      },
      "source": [
        "# loading prepared set to save time\r\n",
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n",
            "в киеве исключили новый раунд минских переговоров\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "6962823b-1e10-4179-9846-0cac6592232f"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC79r_7EPy6",
        "outputId": "73d05d13-d422-4267-f0e7-7b3d616aa175"
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d581cbd9-43e5-4e46-d1de-d1ed9c1b5247"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/') # loading pre-prepared model to save time\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        "    # trax.data.FilterByLength(2048)\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fb3fd8-d2ab-4660-b18b-1bdbd0c8838e"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 5022    16 12427  1749  8434  4981 15949     1     0  4380     4 12975\n",
            " 14141     5  8369   200   191  6292  7151     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "a394007b-e9ee-4438-cdeb-3b208d98a0aa"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "1d9739ec-a6d7-4b9f-ce12-8df451cb8cf0"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5172,     5,  5489,   846,    57,  1689,  1679,  8727,  2224,\n",
              "        1280,     5,  1813,  5192,  1237,   315,  4572,  2617, 15945,\n",
              "         939,  2328,  2371,  9806,  2346,  4011,  3399, 15932, 15020,\n",
              "          25,   227,  2072,   730,  6686,   519,    86, 12175, 12344,\n",
              "       15941,   771,    70,  3639,    86, 15960,    31, 11055,  3948,\n",
              "        9996, 15945,   258,   110,  1234,   371, 13556,  3801,  6173,\n",
              "          18,  5192,    25,  4600,  6373,   511,  5223,  2441,  1810,\n",
              "          11,  2421,   212,  9996,  2617, 15949,   752,   150,  6042,\n",
              "        2824,  6612,  1332,    57,   483, 15945,  2246,  3212,     5,\n",
              "       12344,  2782,   130,  4274,  3567,  3573, 15949,     5,  6381,\n",
              "        3487, 15945,    79,  5601,  8130,   442,  4257,   126, 15945,\n",
              "           5,  2405,  1194,  5065,  4810,   166, 15945,  2351,    10,\n",
              "       10761,  4318,   563,  7345,    75,  6847,   124,     5,  7386,\n",
              "        1925, 12213,   218, 15945,  2085,  7930,    57,   483, 15949,\n",
              "         505,  5172, 15064,    25, 15586,   992,  4561, 15960,  3241,\n",
              "        3120,  1179,   528,  1001,   823,  5073,  5192,    16,  5583,\n",
              "       15954,  5796,   580,  6617,  2747,  7922,     5,  4084,   106,\n",
              "        5521, 15949,  1740,  1664, 15945,   586,  5303,  4487,  5270,\n",
              "          17,  9317,   110,   473,   101,  1615,  2810,    17, 12101,\n",
              "         291,   110,   300,   206,   543,  9552,  6072,   291,   110,\n",
              "       15944,  3468,  7496,   557, 12850,   522, 13324, 15972,    16,\n",
              "         110,   274,  7777,   353,   371,  2325,  1741,     5,  3057,\n",
              "        2333,   173, 15945,  6686,   519,    86, 12344,  2782,    83,\n",
              "           4, 15751,    15,  6805, 10911, 14462,    25,    25,    90,\n",
              "       15933,   111,   227,  2072,   730,    57,   634,    70,  1887,\n",
              "        3537,  3142, 15949, 15476,     9,  7419,     5,  3899, 15945,\n",
              "         939,   237,  4373,  9288,   125,   194,   330, 15945,   227,\n",
              "        1162,   799,  2261,  1217,  1248,  6068, 15945,    16,   236,\n",
              "        3819, 15949,   107,  1095,  2333,   173,     5,    23,  1786,\n",
              "       11766,   462,  1759,  6037,   260,    86, 15960,    31,    57,\n",
              "          43,  4114,   132,    75,  6847,   124,    59,  3454,  1209,\n",
              "         940,  3161,    57,  8201,  5712,  7940,    64, 15341,  8544,\n",
              "          10,  9821,  9698,  2876, 15945,   533,  8214,  1919,   150,\n",
              "       15949,  9192, 15960,  2747,    57,  1127,  2724,   183, 11262,\n",
              "        4011,  4258,   149,  6688,  1162,  5044,  5551, 15949,     1,\n",
              "           0,  5192,   846,    57,  3540,  8992,    46,  7604,    59,\n",
              "        3454,   249,  1209,    91, 15950,    17,    57,   101,   555,\n",
              "         193,  1526,  2731,     1,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "abdd132c-4ced-47a0-dcaf-baac2d135e3a"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.0001)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.0001), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "79d45b04-b17f-4eff-9059-5298e6d32626"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  200100: Ran 100 train steps in 72.92 secs\n",
            "Step  200100: train CrossEntropyLoss |  3.49680829\n",
            "Step  200100: eval  CrossEntropyLoss |  3.80252838\n",
            "Step  200100: eval          Accuracy |  0.35514018\n",
            "\n",
            "Step  200200: Ran 100 train steps in 35.47 secs\n",
            "Step  200200: train CrossEntropyLoss |  3.39891481\n",
            "Step  200200: eval  CrossEntropyLoss |  3.47128797\n",
            "Step  200200: eval          Accuracy |  0.49532709\n",
            "\n",
            "Step  200300: Ran 100 train steps in 52.42 secs\n",
            "Step  200300: train CrossEntropyLoss |  3.30069256\n",
            "Step  200300: eval  CrossEntropyLoss |  3.36671329\n",
            "Step  200300: eval          Accuracy |  0.48999998\n",
            "\n",
            "Step  200400: Ran 100 train steps in 35.49 secs\n",
            "Step  200400: train CrossEntropyLoss |  3.37178683\n",
            "Step  200400: eval  CrossEntropyLoss |  3.49869657\n",
            "Step  200400: eval          Accuracy |  0.43203884\n",
            "\n",
            "Step  200500: Ran 100 train steps in 35.61 secs\n",
            "Step  200500: train CrossEntropyLoss |  3.21909213\n",
            "Step  200500: eval  CrossEntropyLoss |  3.35392380\n",
            "Step  200500: eval          Accuracy |  0.46938774\n",
            "\n",
            "Step  200600: Ran 100 train steps in 35.52 secs\n",
            "Step  200600: train CrossEntropyLoss |  3.25453830\n",
            "Step  200600: eval  CrossEntropyLoss |  3.55792809\n",
            "Step  200600: eval          Accuracy |  0.44444448\n",
            "\n",
            "Step  200700: Ran 100 train steps in 35.39 secs\n",
            "Step  200700: train CrossEntropyLoss |  3.16823912\n",
            "Step  200700: eval  CrossEntropyLoss |  3.48802853\n",
            "Step  200700: eval          Accuracy |  0.43859649\n",
            "\n",
            "Step  200800: Ran 100 train steps in 35.48 secs\n",
            "Step  200800: train CrossEntropyLoss |  3.13981748\n",
            "Step  200800: eval  CrossEntropyLoss |  2.60321856\n",
            "Step  200800: eval          Accuracy |  0.50943398\n",
            "\n",
            "Step  200900: Ran 100 train steps in 35.58 secs\n",
            "Step  200900: train CrossEntropyLoss |  3.11577535\n",
            "Step  200900: eval  CrossEntropyLoss |  3.26970530\n",
            "Step  200900: eval          Accuracy |  0.48598129\n",
            "\n",
            "Step  201000: Ran 100 train steps in 35.43 secs\n",
            "Step  201000: train CrossEntropyLoss |  3.11035967\n",
            "Step  201000: eval  CrossEntropyLoss |  3.61276245\n",
            "Step  201000: eval          Accuracy |  0.45049503\n",
            "\n",
            "Step  201100: Ran 100 train steps in 35.81 secs\n",
            "Step  201100: train CrossEntropyLoss |  3.16334486\n",
            "Step  201100: eval  CrossEntropyLoss |  3.12761116\n",
            "Step  201100: eval          Accuracy |  0.51428574\n",
            "\n",
            "Step  201200: Ran 100 train steps in 35.55 secs\n",
            "Step  201200: train CrossEntropyLoss |  3.08029079\n",
            "Step  201200: eval  CrossEntropyLoss |  4.12763548\n",
            "Step  201200: eval          Accuracy |  0.32352942\n",
            "\n",
            "Step  201300: Ran 100 train steps in 35.79 secs\n",
            "Step  201300: train CrossEntropyLoss |  3.07407045\n",
            "Step  201300: eval  CrossEntropyLoss |  3.14641619\n",
            "Step  201300: eval          Accuracy |  0.49523813\n",
            "\n",
            "Step  201400: Ran 100 train steps in 35.71 secs\n",
            "Step  201400: train CrossEntropyLoss |  3.10512447\n",
            "Step  201400: eval  CrossEntropyLoss |  2.99321747\n",
            "Step  201400: eval          Accuracy |  0.47142860\n",
            "\n",
            "Step  201500: Ran 100 train steps in 35.53 secs\n",
            "Step  201500: train CrossEntropyLoss |  3.18742037\n",
            "Step  201500: eval  CrossEntropyLoss |  3.27842450\n",
            "Step  201500: eval          Accuracy |  0.48598129\n",
            "\n",
            "Step  201600: Ran 100 train steps in 35.64 secs\n",
            "Step  201600: train CrossEntropyLoss |  3.02733850\n",
            "Step  201600: eval  CrossEntropyLoss |  3.60571337\n",
            "Step  201600: eval          Accuracy |  0.37719297\n",
            "\n",
            "Step  201700: Ran 100 train steps in 35.57 secs\n",
            "Step  201700: train CrossEntropyLoss |  3.00043917\n",
            "Step  201700: eval  CrossEntropyLoss |  2.91000772\n",
            "Step  201700: eval          Accuracy |  0.56521738\n",
            "\n",
            "Step  201800: Ran 100 train steps in 35.74 secs\n",
            "Step  201800: train CrossEntropyLoss |  3.03981805\n",
            "Step  201800: eval  CrossEntropyLoss |  3.05452347\n",
            "Step  201800: eval          Accuracy |  0.51196176\n",
            "\n",
            "Step  201900: Ran 100 train steps in 35.72 secs\n",
            "Step  201900: train CrossEntropyLoss |  3.01722050\n",
            "Step  201900: eval  CrossEntropyLoss |  2.84831047\n",
            "Step  201900: eval          Accuracy |  0.43877551\n",
            "\n",
            "Step  202000: Ran 100 train steps in 35.66 secs\n",
            "Step  202000: train CrossEntropyLoss |  2.99123430\n",
            "Step  202000: eval  CrossEntropyLoss |  3.16419530\n",
            "Step  202000: eval          Accuracy |  0.50226247\n",
            "\n",
            "Step  202100: Ran 100 train steps in 35.75 secs\n",
            "Step  202100: train CrossEntropyLoss |  3.01581931\n",
            "Step  202100: eval  CrossEntropyLoss |  4.22566605\n",
            "Step  202100: eval          Accuracy |  0.37037039\n",
            "\n",
            "Step  202200: Ran 100 train steps in 35.72 secs\n",
            "Step  202200: train CrossEntropyLoss |  3.00296044\n",
            "Step  202200: eval  CrossEntropyLoss |  2.50120282\n",
            "Step  202200: eval          Accuracy |  0.57522124\n",
            "\n",
            "Step  202300: Ran 100 train steps in 35.79 secs\n",
            "Step  202300: train CrossEntropyLoss |  2.99131060\n",
            "Step  202300: eval  CrossEntropyLoss |  3.12649322\n",
            "Step  202300: eval          Accuracy |  0.49107146\n",
            "\n",
            "Step  202400: Ran 100 train steps in 35.78 secs\n",
            "Step  202400: train CrossEntropyLoss |  3.04585099\n",
            "Step  202400: eval  CrossEntropyLoss |  3.20133615\n",
            "Step  202400: eval          Accuracy |  0.48543689\n",
            "\n",
            "Step  202500: Ran 100 train steps in 35.86 secs\n",
            "Step  202500: train CrossEntropyLoss |  2.96943831\n",
            "Step  202500: eval  CrossEntropyLoss |  3.31925011\n",
            "Step  202500: eval          Accuracy |  0.44554454\n",
            "\n",
            "Step  202600: Ran 100 train steps in 35.74 secs\n",
            "Step  202600: train CrossEntropyLoss |  3.02753162\n",
            "Step  202600: eval  CrossEntropyLoss |  2.80562615\n",
            "Step  202600: eval          Accuracy |  0.49029127\n",
            "\n",
            "Step  202700: Ran 100 train steps in 35.75 secs\n",
            "Step  202700: train CrossEntropyLoss |  2.95857239\n",
            "Step  202700: eval  CrossEntropyLoss |  2.74036646\n",
            "Step  202700: eval          Accuracy |  0.55172414\n",
            "\n",
            "Step  202800: Ran 100 train steps in 35.66 secs\n",
            "Step  202800: train CrossEntropyLoss |  2.96891069\n",
            "Step  202800: eval  CrossEntropyLoss |  3.85248542\n",
            "Step  202800: eval          Accuracy |  0.37931034\n",
            "\n",
            "Step  202900: Ran 100 train steps in 35.87 secs\n",
            "Step  202900: train CrossEntropyLoss |  2.97485018\n",
            "Step  202900: eval  CrossEntropyLoss |  3.04617667\n",
            "Step  202900: eval          Accuracy |  0.53271025\n",
            "\n",
            "Step  203000: Ran 100 train steps in 35.95 secs\n",
            "Step  203000: train CrossEntropyLoss |  3.00778747\n",
            "Step  203000: eval  CrossEntropyLoss |  3.13629580\n",
            "Step  203000: eval          Accuracy |  0.48039219\n",
            "\n",
            "Step  203100: Ran 100 train steps in 35.82 secs\n",
            "Step  203100: train CrossEntropyLoss |  2.93002272\n",
            "Step  203100: eval  CrossEntropyLoss |  3.48966050\n",
            "Step  203100: eval          Accuracy |  0.41116750\n",
            "\n",
            "Step  203200: Ran 100 train steps in 35.87 secs\n",
            "Step  203200: train CrossEntropyLoss |  2.94307256\n",
            "Step  203200: eval  CrossEntropyLoss |  3.25150037\n",
            "Step  203200: eval          Accuracy |  0.46031749\n",
            "\n",
            "Step  203300: Ran 100 train steps in 35.75 secs\n",
            "Step  203300: train CrossEntropyLoss |  2.93310785\n",
            "Step  203300: eval  CrossEntropyLoss |  3.21162868\n",
            "Step  203300: eval          Accuracy |  0.45794392\n",
            "\n",
            "Step  203400: Ran 100 train steps in 35.93 secs\n",
            "Step  203400: train CrossEntropyLoss |  2.87421417\n",
            "Step  203400: eval  CrossEntropyLoss |  2.70395160\n",
            "Step  203400: eval          Accuracy |  0.50943398\n",
            "\n",
            "Step  203500: Ran 100 train steps in 35.95 secs\n",
            "Step  203500: train CrossEntropyLoss |  2.92934346\n",
            "Step  203500: eval  CrossEntropyLoss |  2.94306183\n",
            "Step  203500: eval          Accuracy |  0.50505048\n",
            "\n",
            "Step  203600: Ran 100 train steps in 35.83 secs\n",
            "Step  203600: train CrossEntropyLoss |  2.95028043\n",
            "Step  203600: eval  CrossEntropyLoss |  2.61943316\n",
            "Step  203600: eval          Accuracy |  0.56666672\n",
            "\n",
            "Step  203700: Ran 100 train steps in 35.76 secs\n",
            "Step  203700: train CrossEntropyLoss |  2.92518306\n",
            "Step  203700: eval  CrossEntropyLoss |  3.48006320\n",
            "Step  203700: eval          Accuracy |  0.43636364\n",
            "\n",
            "Step  203800: Ran 100 train steps in 35.74 secs\n",
            "Step  203800: train CrossEntropyLoss |  2.92127991\n",
            "Step  203800: eval  CrossEntropyLoss |  2.63404274\n",
            "Step  203800: eval          Accuracy |  0.56310678\n",
            "\n",
            "Step  203900: Ran 100 train steps in 35.95 secs\n",
            "Step  203900: train CrossEntropyLoss |  2.87648559\n",
            "Step  203900: eval  CrossEntropyLoss |  3.33864403\n",
            "Step  203900: eval          Accuracy |  0.43010753\n",
            "\n",
            "Step  204000: Ran 100 train steps in 35.68 secs\n",
            "Step  204000: train CrossEntropyLoss |  2.85794926\n",
            "Step  204000: eval  CrossEntropyLoss |  3.36523557\n",
            "Step  204000: eval          Accuracy |  0.43781093\n",
            "\n",
            "Step  204100: Ran 100 train steps in 35.65 secs\n",
            "Step  204100: train CrossEntropyLoss |  2.86496401\n",
            "Step  204100: eval  CrossEntropyLoss |  3.09742570\n",
            "Step  204100: eval          Accuracy |  0.52336448\n",
            "\n",
            "Step  204200: Ran 100 train steps in 35.91 secs\n",
            "Step  204200: train CrossEntropyLoss |  2.85803008\n",
            "Step  204200: eval  CrossEntropyLoss |  3.18787599\n",
            "Step  204200: eval          Accuracy |  0.47058827\n",
            "\n",
            "Step  204300: Ran 100 train steps in 35.60 secs\n",
            "Step  204300: train CrossEntropyLoss |  2.89355040\n",
            "Step  204300: eval  CrossEntropyLoss |  2.86141825\n",
            "Step  204300: eval          Accuracy |  0.51376146\n",
            "\n",
            "Step  204400: Ran 100 train steps in 35.74 secs\n",
            "Step  204400: train CrossEntropyLoss |  2.82686424\n",
            "Step  204400: eval  CrossEntropyLoss |  3.21300530\n",
            "Step  204400: eval          Accuracy |  0.45833334\n",
            "\n",
            "Step  204500: Ran 100 train steps in 35.84 secs\n",
            "Step  204500: train CrossEntropyLoss |  2.92606378\n",
            "Step  204500: eval  CrossEntropyLoss |  2.37479186\n",
            "Step  204500: eval          Accuracy |  0.54210526\n",
            "\n",
            "Step  204600: Ran 100 train steps in 35.71 secs\n",
            "Step  204600: train CrossEntropyLoss |  2.81520772\n",
            "Step  204600: eval  CrossEntropyLoss |  3.36133194\n",
            "Step  204600: eval          Accuracy |  0.39166668\n",
            "\n",
            "Step  204700: Ran 100 train steps in 35.85 secs\n",
            "Step  204700: train CrossEntropyLoss |  2.96205926\n",
            "Step  204700: eval  CrossEntropyLoss |  3.42702961\n",
            "Step  204700: eval          Accuracy |  0.46226415\n",
            "\n",
            "Step  204800: Ran 100 train steps in 35.74 secs\n",
            "Step  204800: train CrossEntropyLoss |  2.81880617\n",
            "Step  204800: eval  CrossEntropyLoss |  2.41605067\n",
            "Step  204800: eval          Accuracy |  0.54954958\n",
            "\n",
            "Step  204900: Ran 100 train steps in 35.68 secs\n",
            "Step  204900: train CrossEntropyLoss |  2.87065458\n",
            "Step  204900: eval  CrossEntropyLoss |  2.21464038\n",
            "Step  204900: eval          Accuracy |  0.58181816\n",
            "\n",
            "Step  205000: Ran 100 train steps in 35.63 secs\n",
            "Step  205000: train CrossEntropyLoss |  2.85986829\n",
            "Step  205000: eval  CrossEntropyLoss |  2.69186783\n",
            "Step  205000: eval          Accuracy |  0.55284560\n",
            "\n",
            "Step  205100: Ran 100 train steps in 35.71 secs\n",
            "Step  205100: train CrossEntropyLoss |  2.82198668\n",
            "Step  205100: eval  CrossEntropyLoss |  2.36166048\n",
            "Step  205100: eval          Accuracy |  0.63157898\n",
            "\n",
            "Step  205200: Ran 100 train steps in 35.83 secs\n",
            "Step  205200: train CrossEntropyLoss |  2.86006594\n",
            "Step  205200: eval  CrossEntropyLoss |  2.67015386\n",
            "Step  205200: eval          Accuracy |  0.52475244\n",
            "\n",
            "Step  205300: Ran 100 train steps in 35.77 secs\n",
            "Step  205300: train CrossEntropyLoss |  2.82863164\n",
            "Step  205300: eval  CrossEntropyLoss |  3.61065531\n",
            "Step  205300: eval          Accuracy |  0.40566039\n",
            "\n",
            "Step  205400: Ran 100 train steps in 35.74 secs\n",
            "Step  205400: train CrossEntropyLoss |  2.90855575\n",
            "Step  205400: eval  CrossEntropyLoss |  2.59490442\n",
            "Step  205400: eval          Accuracy |  0.54634148\n",
            "\n",
            "Step  205500: Ran 100 train steps in 35.79 secs\n",
            "Step  205500: train CrossEntropyLoss |  2.87441587\n",
            "Step  205500: eval  CrossEntropyLoss |  3.13445783\n",
            "Step  205500: eval          Accuracy |  0.44565219\n",
            "\n",
            "Step  205600: Ran 100 train steps in 35.88 secs\n",
            "Step  205600: train CrossEntropyLoss |  2.81540012\n",
            "Step  205600: eval  CrossEntropyLoss |  3.17565966\n",
            "Step  205600: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  205700: Ran 100 train steps in 35.79 secs\n",
            "Step  205700: train CrossEntropyLoss |  2.82522416\n",
            "Step  205700: eval  CrossEntropyLoss |  3.09995484\n",
            "Step  205700: eval          Accuracy |  0.49751243\n",
            "\n",
            "Step  205800: Ran 100 train steps in 35.65 secs\n",
            "Step  205800: train CrossEntropyLoss |  2.82687593\n",
            "Step  205800: eval  CrossEntropyLoss |  3.24919271\n",
            "Step  205800: eval          Accuracy |  0.45794392\n",
            "\n",
            "Step  205900: Ran 100 train steps in 35.77 secs\n",
            "Step  205900: train CrossEntropyLoss |  2.83620667\n",
            "Step  205900: eval  CrossEntropyLoss |  2.94891977\n",
            "Step  205900: eval          Accuracy |  0.47126436\n",
            "\n",
            "Step  206000: Ran 100 train steps in 35.71 secs\n",
            "Step  206000: train CrossEntropyLoss |  2.74714875\n",
            "Step  206000: eval  CrossEntropyLoss |  3.11460042\n",
            "Step  206000: eval          Accuracy |  0.50961542\n",
            "\n",
            "Step  206100: Ran 100 train steps in 35.85 secs\n",
            "Step  206100: train CrossEntropyLoss |  2.83261323\n",
            "Step  206100: eval  CrossEntropyLoss |  3.05945754\n",
            "Step  206100: eval          Accuracy |  0.47887325\n",
            "\n",
            "Step  206200: Ran 100 train steps in 35.70 secs\n",
            "Step  206200: train CrossEntropyLoss |  2.80417085\n",
            "Step  206200: eval  CrossEntropyLoss |  2.36609411\n",
            "Step  206200: eval          Accuracy |  0.58095241\n",
            "\n",
            "Step  206300: Ran 100 train steps in 35.81 secs\n",
            "Step  206300: train CrossEntropyLoss |  2.84588075\n",
            "Step  206300: eval  CrossEntropyLoss |  3.36710739\n",
            "Step  206300: eval          Accuracy |  0.47272727\n",
            "\n",
            "Step  206400: Ran 100 train steps in 35.65 secs\n",
            "Step  206400: train CrossEntropyLoss |  2.85285568\n",
            "Step  206400: eval  CrossEntropyLoss |  2.96422911\n",
            "Step  206400: eval          Accuracy |  0.49753693\n",
            "\n",
            "Step  206500: Ran 100 train steps in 35.85 secs\n",
            "Step  206500: train CrossEntropyLoss |  2.73868465\n",
            "Step  206500: eval  CrossEntropyLoss |  3.42121077\n",
            "Step  206500: eval          Accuracy |  0.38541669\n",
            "\n",
            "Step  206600: Ran 100 train steps in 35.75 secs\n",
            "Step  206600: train CrossEntropyLoss |  2.80697346\n",
            "Step  206600: eval  CrossEntropyLoss |  3.09328151\n",
            "Step  206600: eval          Accuracy |  0.46391755\n",
            "\n",
            "Step  206700: Ran 100 train steps in 35.66 secs\n",
            "Step  206700: train CrossEntropyLoss |  2.77359939\n",
            "Step  206700: eval  CrossEntropyLoss |  3.56332731\n",
            "Step  206700: eval          Accuracy |  0.40350878\n",
            "\n",
            "Step  206800: Ran 100 train steps in 35.70 secs\n",
            "Step  206800: train CrossEntropyLoss |  2.88011265\n",
            "Step  206800: eval  CrossEntropyLoss |  3.40500736\n",
            "Step  206800: eval          Accuracy |  0.36521739\n",
            "\n",
            "Step  206900: Ran 100 train steps in 35.72 secs\n",
            "Step  206900: train CrossEntropyLoss |  2.76675558\n",
            "Step  206900: eval  CrossEntropyLoss |  3.02530956\n",
            "Step  206900: eval          Accuracy |  0.47154474\n",
            "\n",
            "Step  207000: Ran 100 train steps in 35.75 secs\n",
            "Step  207000: train CrossEntropyLoss |  2.75437188\n",
            "Step  207000: eval  CrossEntropyLoss |  2.91857076\n",
            "Step  207000: eval          Accuracy |  0.46327683\n",
            "\n",
            "Step  207100: Ran 100 train steps in 35.76 secs\n",
            "Step  207100: train CrossEntropyLoss |  2.93444848\n",
            "Step  207100: eval  CrossEntropyLoss |  3.54579759\n",
            "Step  207100: eval          Accuracy |  0.44144145\n",
            "\n",
            "Step  207200: Ran 100 train steps in 35.86 secs\n",
            "Step  207200: train CrossEntropyLoss |  2.80450106\n",
            "Step  207200: eval  CrossEntropyLoss |  3.30402327\n",
            "Step  207200: eval          Accuracy |  0.50909090\n",
            "\n",
            "Step  207300: Ran 100 train steps in 35.98 secs\n",
            "Step  207300: train CrossEntropyLoss |  2.80681634\n",
            "Step  207300: eval  CrossEntropyLoss |  2.77509308\n",
            "Step  207300: eval          Accuracy |  0.53038675\n",
            "\n",
            "Step  207400: Ran 100 train steps in 36.02 secs\n",
            "Step  207400: train CrossEntropyLoss |  2.79526401\n",
            "Step  207400: eval  CrossEntropyLoss |  2.76716900\n",
            "Step  207400: eval          Accuracy |  0.51960784\n",
            "\n",
            "Step  207500: Ran 100 train steps in 35.90 secs\n",
            "Step  207500: train CrossEntropyLoss |  2.79657865\n",
            "Step  207500: eval  CrossEntropyLoss |  3.20226717\n",
            "Step  207500: eval          Accuracy |  0.48148149\n",
            "\n",
            "Step  207600: Ran 100 train steps in 35.85 secs\n",
            "Step  207600: train CrossEntropyLoss |  2.82002735\n",
            "Step  207600: eval  CrossEntropyLoss |  3.28224921\n",
            "Step  207600: eval          Accuracy |  0.45026177\n",
            "\n",
            "Step  207700: Ran 100 train steps in 35.73 secs\n",
            "Step  207700: train CrossEntropyLoss |  2.79931688\n",
            "Step  207700: eval  CrossEntropyLoss |  3.41242552\n",
            "Step  207700: eval          Accuracy |  0.46846849\n",
            "\n",
            "Step  207800: Ran 100 train steps in 35.89 secs\n",
            "Step  207800: train CrossEntropyLoss |  2.88023472\n",
            "Step  207800: eval  CrossEntropyLoss |  2.85274291\n",
            "Step  207800: eval          Accuracy |  0.52500004\n",
            "\n",
            "Step  207900: Ran 100 train steps in 35.95 secs\n",
            "Step  207900: train CrossEntropyLoss |  2.84455204\n",
            "Step  207900: eval  CrossEntropyLoss |  2.05210400\n",
            "Step  207900: eval          Accuracy |  0.65094340\n",
            "\n",
            "Step  208000: Ran 100 train steps in 35.87 secs\n",
            "Step  208000: train CrossEntropyLoss |  2.84580588\n",
            "Step  208000: eval  CrossEntropyLoss |  2.94110918\n",
            "Step  208000: eval          Accuracy |  0.50232559\n",
            "\n",
            "Step  208100: Ran 100 train steps in 36.02 secs\n",
            "Step  208100: train CrossEntropyLoss |  2.81643081\n",
            "Step  208100: eval  CrossEntropyLoss |  2.57244182\n",
            "Step  208100: eval          Accuracy |  0.57522124\n",
            "\n",
            "Step  208200: Ran 100 train steps in 36.33 secs\n",
            "Step  208200: train CrossEntropyLoss |  2.89853621\n",
            "Step  208200: eval  CrossEntropyLoss |  3.03106761\n",
            "Step  208200: eval          Accuracy |  0.46728972\n",
            "\n",
            "Step  208300: Ran 100 train steps in 36.24 secs\n",
            "Step  208300: train CrossEntropyLoss |  2.80295348\n",
            "Step  208300: eval  CrossEntropyLoss |  2.61782694\n",
            "Step  208300: eval          Accuracy |  0.58128077\n",
            "\n",
            "Step  208400: Ran 100 train steps in 36.12 secs\n",
            "Step  208400: train CrossEntropyLoss |  2.79229546\n",
            "Step  208400: eval  CrossEntropyLoss |  2.80461597\n",
            "Step  208400: eval          Accuracy |  0.52808988\n",
            "\n",
            "Step  208500: Ran 100 train steps in 36.02 secs\n",
            "Step  208500: train CrossEntropyLoss |  2.86040068\n",
            "Step  208500: eval  CrossEntropyLoss |  2.80454659\n",
            "Step  208500: eval          Accuracy |  0.52459019\n",
            "\n",
            "Step  208600: Ran 100 train steps in 35.83 secs\n",
            "Step  208600: train CrossEntropyLoss |  2.80438900\n",
            "Step  208600: eval  CrossEntropyLoss |  2.88588667\n",
            "Step  208600: eval          Accuracy |  0.51351351\n",
            "\n",
            "Step  208700: Ran 100 train steps in 35.84 secs\n",
            "Step  208700: train CrossEntropyLoss |  2.79919648\n",
            "Step  208700: eval  CrossEntropyLoss |  2.76686430\n",
            "Step  208700: eval          Accuracy |  0.51111108\n",
            "\n",
            "Step  208800: Ran 100 train steps in 35.94 secs\n",
            "Step  208800: train CrossEntropyLoss |  2.77048755\n",
            "Step  208800: eval  CrossEntropyLoss |  3.57776427\n",
            "Step  208800: eval          Accuracy |  0.41414142\n",
            "\n",
            "Step  208900: Ran 100 train steps in 35.94 secs\n",
            "Step  208900: train CrossEntropyLoss |  2.80248094\n",
            "Step  208900: eval  CrossEntropyLoss |  2.90750194\n",
            "Step  208900: eval          Accuracy |  0.51546395\n",
            "\n",
            "Step  209000: Ran 100 train steps in 36.03 secs\n",
            "Step  209000: train CrossEntropyLoss |  2.74566317\n",
            "Step  209000: eval  CrossEntropyLoss |  2.95756006\n",
            "Step  209000: eval          Accuracy |  0.53097343\n",
            "\n",
            "Step  209100: Ran 100 train steps in 35.83 secs\n",
            "Step  209100: train CrossEntropyLoss |  2.75384426\n",
            "Step  209100: eval  CrossEntropyLoss |  2.70765114\n",
            "Step  209100: eval          Accuracy |  0.52500004\n",
            "\n",
            "Step  209200: Ran 100 train steps in 35.76 secs\n",
            "Step  209200: train CrossEntropyLoss |  2.83631229\n",
            "Step  209200: eval  CrossEntropyLoss |  3.18536711\n",
            "Step  209200: eval          Accuracy |  0.48623851\n",
            "\n",
            "Step  209300: Ran 100 train steps in 35.73 secs\n",
            "Step  209300: train CrossEntropyLoss |  2.81288505\n",
            "Step  209300: eval  CrossEntropyLoss |  2.30818295\n",
            "Step  209300: eval          Accuracy |  0.60683763\n",
            "\n",
            "Step  209400: Ran 100 train steps in 35.68 secs\n",
            "Step  209400: train CrossEntropyLoss |  2.84557509\n",
            "Step  209400: eval  CrossEntropyLoss |  2.71806598\n",
            "Step  209400: eval          Accuracy |  0.54098362\n",
            "\n",
            "Step  209500: Ran 100 train steps in 35.76 secs\n",
            "Step  209500: train CrossEntropyLoss |  2.79790640\n",
            "Step  209500: eval  CrossEntropyLoss |  2.66651177\n",
            "Step  209500: eval          Accuracy |  0.50943398\n",
            "\n",
            "Step  209600: Ran 100 train steps in 35.67 secs\n",
            "Step  209600: train CrossEntropyLoss |  2.78647161\n",
            "Step  209600: eval  CrossEntropyLoss |  2.52463865\n",
            "Step  209600: eval          Accuracy |  0.56390977\n",
            "\n",
            "Step  209700: Ran 100 train steps in 35.74 secs\n",
            "Step  209700: train CrossEntropyLoss |  2.81223178\n",
            "Step  209700: eval  CrossEntropyLoss |  2.55447507\n",
            "Step  209700: eval          Accuracy |  0.55364805\n",
            "\n",
            "Step  209800: Ran 100 train steps in 35.79 secs\n",
            "Step  209800: train CrossEntropyLoss |  2.73925948\n",
            "Step  209800: eval  CrossEntropyLoss |  3.03129864\n",
            "Step  209800: eval          Accuracy |  0.50458711\n",
            "\n",
            "Step  209900: Ran 100 train steps in 36.01 secs\n",
            "Step  209900: train CrossEntropyLoss |  2.75603867\n",
            "Step  209900: eval  CrossEntropyLoss |  3.23539591\n",
            "Step  209900: eval          Accuracy |  0.46363634\n",
            "\n",
            "Step  210000: Ran 100 train steps in 35.72 secs\n",
            "Step  210000: train CrossEntropyLoss |  2.67578650\n",
            "Step  210000: eval  CrossEntropyLoss |  2.28340125\n",
            "Step  210000: eval          Accuracy |  0.59139782\n",
            "\n",
            "Step  210100: Ran 100 train steps in 35.83 secs\n",
            "Step  210100: train CrossEntropyLoss |  2.74529076\n",
            "Step  210100: eval  CrossEntropyLoss |  2.94685578\n",
            "Step  210100: eval          Accuracy |  0.51764709\n",
            "\n",
            "Step  210200: Ran 100 train steps in 35.86 secs\n",
            "Step  210200: train CrossEntropyLoss |  2.79075742\n",
            "Step  210200: eval  CrossEntropyLoss |  2.53403068\n",
            "Step  210200: eval          Accuracy |  0.52830189\n",
            "\n",
            "Step  210300: Ran 100 train steps in 35.72 secs\n",
            "Step  210300: train CrossEntropyLoss |  2.79011869\n",
            "Step  210300: eval  CrossEntropyLoss |  2.27625251\n",
            "Step  210300: eval          Accuracy |  0.58064514\n",
            "\n",
            "Step  210400: Ran 100 train steps in 35.85 secs\n",
            "Step  210400: train CrossEntropyLoss |  2.85130000\n",
            "Step  210400: eval  CrossEntropyLoss |  3.80841756\n",
            "Step  210400: eval          Accuracy |  0.39215687\n",
            "\n",
            "Step  210500: Ran 100 train steps in 35.83 secs\n",
            "Step  210500: train CrossEntropyLoss |  2.76016021\n",
            "Step  210500: eval  CrossEntropyLoss |  2.98546863\n",
            "Step  210500: eval          Accuracy |  0.52261305\n",
            "\n",
            "Step  210600: Ran 100 train steps in 35.87 secs\n",
            "Step  210600: train CrossEntropyLoss |  2.80091023\n",
            "Step  210600: eval  CrossEntropyLoss |  2.72568440\n",
            "Step  210600: eval          Accuracy |  0.48113209\n",
            "\n",
            "Step  210700: Ran 100 train steps in 35.73 secs\n",
            "Step  210700: train CrossEntropyLoss |  2.77235675\n",
            "Step  210700: eval  CrossEntropyLoss |  2.99263191\n",
            "Step  210700: eval          Accuracy |  0.47457626\n",
            "\n",
            "Step  210800: Ran 100 train steps in 35.98 secs\n",
            "Step  210800: train CrossEntropyLoss |  2.76149106\n",
            "Step  210800: eval  CrossEntropyLoss |  2.61975694\n",
            "Step  210800: eval          Accuracy |  0.54545456\n",
            "\n",
            "Step  210900: Ran 100 train steps in 35.73 secs\n",
            "Step  210900: train CrossEntropyLoss |  2.81001830\n",
            "Step  210900: eval  CrossEntropyLoss |  3.28463745\n",
            "Step  210900: eval          Accuracy |  0.46875000\n",
            "\n",
            "Step  211000: Ran 100 train steps in 35.90 secs\n",
            "Step  211000: train CrossEntropyLoss |  2.81741285\n",
            "Step  211000: eval  CrossEntropyLoss |  3.41852641\n",
            "Step  211000: eval          Accuracy |  0.42857143\n",
            "\n",
            "Step  211100: Ran 100 train steps in 35.72 secs\n",
            "Step  211100: train CrossEntropyLoss |  2.75306082\n",
            "Step  211100: eval  CrossEntropyLoss |  3.58190584\n",
            "Step  211100: eval          Accuracy |  0.43956044\n",
            "\n",
            "Step  211200: Ran 100 train steps in 35.71 secs\n",
            "Step  211200: train CrossEntropyLoss |  2.76151085\n",
            "Step  211200: eval  CrossEntropyLoss |  2.43408036\n",
            "Step  211200: eval          Accuracy |  0.53301889\n",
            "\n",
            "Step  211300: Ran 100 train steps in 35.71 secs\n",
            "Step  211300: train CrossEntropyLoss |  2.73789978\n",
            "Step  211300: eval  CrossEntropyLoss |  2.80405140\n",
            "Step  211300: eval          Accuracy |  0.47413793\n",
            "\n",
            "Step  211400: Ran 100 train steps in 35.71 secs\n",
            "Step  211400: train CrossEntropyLoss |  2.80229688\n",
            "Step  211400: eval  CrossEntropyLoss |  2.58410478\n",
            "Step  211400: eval          Accuracy |  0.57522124\n",
            "\n",
            "Step  211500: Ran 100 train steps in 35.79 secs\n",
            "Step  211500: train CrossEntropyLoss |  2.77805805\n",
            "Step  211500: eval  CrossEntropyLoss |  3.29803109\n",
            "Step  211500: eval          Accuracy |  0.47619051\n",
            "\n",
            "Step  211600: Ran 100 train steps in 35.73 secs\n",
            "Step  211600: train CrossEntropyLoss |  2.79928088\n",
            "Step  211600: eval  CrossEntropyLoss |  2.54922271\n",
            "Step  211600: eval          Accuracy |  0.53499997\n",
            "\n",
            "Step  211700: Ran 100 train steps in 36.02 secs\n",
            "Step  211700: train CrossEntropyLoss |  2.76377583\n",
            "Step  211700: eval  CrossEntropyLoss |  2.38740492\n",
            "Step  211700: eval          Accuracy |  0.59803921\n",
            "\n",
            "Step  211800: Ran 100 train steps in 35.75 secs\n",
            "Step  211800: train CrossEntropyLoss |  2.79119802\n",
            "Step  211800: eval  CrossEntropyLoss |  2.19073319\n",
            "Step  211800: eval          Accuracy |  0.62841529\n",
            "\n",
            "Step  211900: Ran 100 train steps in 35.94 secs\n",
            "Step  211900: train CrossEntropyLoss |  2.77146268\n",
            "Step  211900: eval  CrossEntropyLoss |  2.83962560\n",
            "Step  211900: eval          Accuracy |  0.49999997\n",
            "\n",
            "Step  212000: Ran 100 train steps in 35.76 secs\n",
            "Step  212000: train CrossEntropyLoss |  2.72704983\n",
            "Step  212000: eval  CrossEntropyLoss |  2.40804958\n",
            "Step  212000: eval          Accuracy |  0.47826087\n",
            "\n",
            "Step  212100: Ran 100 train steps in 35.85 secs\n",
            "Step  212100: train CrossEntropyLoss |  2.74187946\n",
            "Step  212100: eval  CrossEntropyLoss |  2.74514627\n",
            "Step  212100: eval          Accuracy |  0.48780492\n",
            "\n",
            "Step  212200: Ran 100 train steps in 35.73 secs\n",
            "Step  212200: train CrossEntropyLoss |  2.76393509\n",
            "Step  212200: eval  CrossEntropyLoss |  3.12434721\n",
            "Step  212200: eval          Accuracy |  0.47342995\n",
            "\n",
            "Step  212300: Ran 100 train steps in 35.67 secs\n",
            "Step  212300: train CrossEntropyLoss |  2.79026580\n",
            "Step  212300: eval  CrossEntropyLoss |  3.13621855\n",
            "Step  212300: eval          Accuracy |  0.54098362\n",
            "\n",
            "Step  212400: Ran 100 train steps in 35.75 secs\n",
            "Step  212400: train CrossEntropyLoss |  2.80921817\n",
            "Step  212400: eval  CrossEntropyLoss |  3.31103516\n",
            "Step  212400: eval          Accuracy |  0.47422683\n",
            "\n",
            "Step  212500: Ran 100 train steps in 35.77 secs\n",
            "Step  212500: train CrossEntropyLoss |  2.71918631\n",
            "Step  212500: eval  CrossEntropyLoss |  3.08348489\n",
            "Step  212500: eval          Accuracy |  0.43434343\n",
            "\n",
            "Step  212600: Ran 100 train steps in 35.84 secs\n",
            "Step  212600: train CrossEntropyLoss |  2.70986986\n",
            "Step  212600: eval  CrossEntropyLoss |  2.83191061\n",
            "Step  212600: eval          Accuracy |  0.52380955\n",
            "\n",
            "Step  212700: Ran 100 train steps in 35.72 secs\n",
            "Step  212700: train CrossEntropyLoss |  2.77372074\n",
            "Step  212700: eval  CrossEntropyLoss |  2.90116167\n",
            "Step  212700: eval          Accuracy |  0.48936167\n",
            "\n",
            "Step  212800: Ran 100 train steps in 35.94 secs\n",
            "Step  212800: train CrossEntropyLoss |  2.76412845\n",
            "Step  212800: eval  CrossEntropyLoss |  2.29411173\n",
            "Step  212800: eval          Accuracy |  0.59292036\n",
            "\n",
            "Step  212900: Ran 100 train steps in 35.80 secs\n",
            "Step  212900: train CrossEntropyLoss |  2.78249478\n",
            "Step  212900: eval  CrossEntropyLoss |  2.39895105\n",
            "Step  212900: eval          Accuracy |  0.51546395\n",
            "\n",
            "Step  213000: Ran 100 train steps in 35.82 secs\n",
            "Step  213000: train CrossEntropyLoss |  2.77630210\n",
            "Step  213000: eval  CrossEntropyLoss |  2.77061892\n",
            "Step  213000: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  213100: Ran 100 train steps in 35.76 secs\n",
            "Step  213100: train CrossEntropyLoss |  2.76245642\n",
            "Step  213100: eval  CrossEntropyLoss |  2.78084064\n",
            "Step  213100: eval          Accuracy |  0.50697672\n",
            "\n",
            "Step  213200: Ran 100 train steps in 35.93 secs\n",
            "Step  213200: train CrossEntropyLoss |  2.74905729\n",
            "Step  213200: eval  CrossEntropyLoss |  2.32506657\n",
            "Step  213200: eval          Accuracy |  0.60194176\n",
            "\n",
            "Step  213300: Ran 100 train steps in 35.90 secs\n",
            "Step  213300: train CrossEntropyLoss |  2.74691319\n",
            "Step  213300: eval  CrossEntropyLoss |  2.69199061\n",
            "Step  213300: eval          Accuracy |  0.55102038\n",
            "\n",
            "Step  213400: Ran 100 train steps in 36.17 secs\n",
            "Step  213400: train CrossEntropyLoss |  2.78415895\n",
            "Step  213400: eval  CrossEntropyLoss |  2.62209940\n",
            "Step  213400: eval          Accuracy |  0.55208337\n",
            "\n",
            "Step  213500: Ran 100 train steps in 36.06 secs\n",
            "Step  213500: train CrossEntropyLoss |  2.75065303\n",
            "Step  213500: eval  CrossEntropyLoss |  2.56306505\n",
            "Step  213500: eval          Accuracy |  0.51851851\n",
            "\n",
            "Step  213600: Ran 100 train steps in 35.78 secs\n",
            "Step  213600: train CrossEntropyLoss |  2.74431968\n",
            "Step  213600: eval  CrossEntropyLoss |  2.58924198\n",
            "Step  213600: eval          Accuracy |  0.54901963\n",
            "\n",
            "Step  213700: Ran 100 train steps in 35.76 secs\n",
            "Step  213700: train CrossEntropyLoss |  2.70896935\n",
            "Step  213700: eval  CrossEntropyLoss |  3.04167032\n",
            "Step  213700: eval          Accuracy |  0.45098040\n",
            "\n",
            "Step  213800: Ran 100 train steps in 35.86 secs\n",
            "Step  213800: train CrossEntropyLoss |  2.71315980\n",
            "Step  213800: eval  CrossEntropyLoss |  2.18269610\n",
            "Step  213800: eval          Accuracy |  0.61000001\n",
            "\n",
            "Step  213900: Ran 100 train steps in 35.98 secs\n",
            "Step  213900: train CrossEntropyLoss |  2.70736313\n",
            "Step  213900: eval  CrossEntropyLoss |  2.86462235\n",
            "Step  213900: eval          Accuracy |  0.47572815\n",
            "\n",
            "Step  214000: Ran 100 train steps in 35.82 secs\n",
            "Step  214000: train CrossEntropyLoss |  2.76526737\n",
            "Step  214000: eval  CrossEntropyLoss |  3.27867961\n",
            "Step  214000: eval          Accuracy |  0.50877196\n",
            "\n",
            "Step  214100: Ran 100 train steps in 35.77 secs\n",
            "Step  214100: train CrossEntropyLoss |  2.79113030\n",
            "Step  214100: eval  CrossEntropyLoss |  2.21049786\n",
            "Step  214100: eval          Accuracy |  0.56310678\n",
            "\n",
            "Step  214200: Ran 100 train steps in 35.76 secs\n",
            "Step  214200: train CrossEntropyLoss |  2.82921553\n",
            "Step  214200: eval  CrossEntropyLoss |  2.81692624\n",
            "Step  214200: eval          Accuracy |  0.50515467\n",
            "\n",
            "Step  214300: Ran 100 train steps in 35.89 secs\n",
            "Step  214300: train CrossEntropyLoss |  2.75550103\n",
            "Step  214300: eval  CrossEntropyLoss |  2.37635398\n",
            "Step  214300: eval          Accuracy |  0.54301077\n",
            "\n",
            "Step  214400: Ran 100 train steps in 35.73 secs\n",
            "Step  214400: train CrossEntropyLoss |  2.74863791\n",
            "Step  214400: eval  CrossEntropyLoss |  3.31920838\n",
            "Step  214400: eval          Accuracy |  0.47115386\n",
            "\n",
            "Step  214500: Ran 100 train steps in 35.86 secs\n",
            "Step  214500: train CrossEntropyLoss |  2.76848507\n",
            "Step  214500: eval  CrossEntropyLoss |  2.85838723\n",
            "Step  214500: eval          Accuracy |  0.52631581\n",
            "\n",
            "Step  214600: Ran 100 train steps in 35.81 secs\n",
            "Step  214600: train CrossEntropyLoss |  2.72174716\n",
            "Step  214600: eval  CrossEntropyLoss |  2.83132911\n",
            "Step  214600: eval          Accuracy |  0.49504951\n",
            "\n",
            "Step  214700: Ran 100 train steps in 35.72 secs\n",
            "Step  214700: train CrossEntropyLoss |  2.74660969\n",
            "Step  214700: eval  CrossEntropyLoss |  2.77568150\n",
            "Step  214700: eval          Accuracy |  0.52284265\n",
            "\n",
            "Step  214800: Ran 100 train steps in 35.74 secs\n",
            "Step  214800: train CrossEntropyLoss |  2.72917008\n",
            "Step  214800: eval  CrossEntropyLoss |  3.31710696\n",
            "Step  214800: eval          Accuracy |  0.46226415\n",
            "\n",
            "Step  214900: Ran 100 train steps in 35.76 secs\n",
            "Step  214900: train CrossEntropyLoss |  2.75342321\n",
            "Step  214900: eval  CrossEntropyLoss |  2.58257174\n",
            "Step  214900: eval          Accuracy |  0.55769235\n",
            "\n",
            "Step  215000: Ran 100 train steps in 35.88 secs\n",
            "Step  215000: train CrossEntropyLoss |  2.72014737\n",
            "Step  215000: eval  CrossEntropyLoss |  3.00397611\n",
            "Step  215000: eval          Accuracy |  0.50925928\n",
            "\n",
            "Step  215100: Ran 100 train steps in 36.20 secs\n",
            "Step  215100: train CrossEntropyLoss |  2.77755332\n",
            "Step  215100: eval  CrossEntropyLoss |  2.77911925\n",
            "Step  215100: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  215200: Ran 100 train steps in 35.85 secs\n",
            "Step  215200: train CrossEntropyLoss |  2.73902035\n",
            "Step  215200: eval  CrossEntropyLoss |  3.30725980\n",
            "Step  215200: eval          Accuracy |  0.47115386\n",
            "\n",
            "Step  215300: Ran 100 train steps in 36.07 secs\n",
            "Step  215300: train CrossEntropyLoss |  2.75463104\n",
            "Step  215300: eval  CrossEntropyLoss |  2.81509161\n",
            "Step  215300: eval          Accuracy |  0.53333336\n",
            "\n",
            "Step  215400: Ran 100 train steps in 36.06 secs\n",
            "Step  215400: train CrossEntropyLoss |  2.81542301\n",
            "Step  215400: eval  CrossEntropyLoss |  2.81508780\n",
            "Step  215400: eval          Accuracy |  0.57731962\n",
            "\n",
            "Step  215500: Ran 100 train steps in 36.01 secs\n",
            "Step  215500: train CrossEntropyLoss |  2.75133872\n",
            "Step  215500: eval  CrossEntropyLoss |  2.90076923\n",
            "Step  215500: eval          Accuracy |  0.49238577\n",
            "\n",
            "Step  215600: Ran 100 train steps in 35.88 secs\n",
            "Step  215600: train CrossEntropyLoss |  2.75240350\n",
            "Step  215600: eval  CrossEntropyLoss |  2.23339319\n",
            "Step  215600: eval          Accuracy |  0.65306121\n",
            "\n",
            "Step  215700: Ran 100 train steps in 36.03 secs\n",
            "Step  215700: train CrossEntropyLoss |  2.73095918\n",
            "Step  215700: eval  CrossEntropyLoss |  2.42013955\n",
            "Step  215700: eval          Accuracy |  0.56603777\n",
            "\n",
            "Step  215800: Ran 100 train steps in 35.85 secs\n",
            "Step  215800: train CrossEntropyLoss |  2.64792633\n",
            "Step  215800: eval  CrossEntropyLoss |  3.12218499\n",
            "Step  215800: eval          Accuracy |  0.43243244\n",
            "\n",
            "Step  215900: Ran 100 train steps in 35.97 secs\n",
            "Step  215900: train CrossEntropyLoss |  2.78058028\n",
            "Step  215900: eval  CrossEntropyLoss |  2.61532450\n",
            "Step  215900: eval          Accuracy |  0.52153111\n",
            "\n",
            "Step  216000: Ran 100 train steps in 36.08 secs\n",
            "Step  216000: train CrossEntropyLoss |  2.73425078\n",
            "Step  216000: eval  CrossEntropyLoss |  2.15008569\n",
            "Step  216000: eval          Accuracy |  0.58715594\n",
            "\n",
            "Step  216100: Ran 100 train steps in 35.92 secs\n",
            "Step  216100: train CrossEntropyLoss |  2.71566868\n",
            "Step  216100: eval  CrossEntropyLoss |  2.49466372\n",
            "Step  216100: eval          Accuracy |  0.51136363\n",
            "\n",
            "Step  216200: Ran 100 train steps in 36.04 secs\n",
            "Step  216200: train CrossEntropyLoss |  2.72469711\n",
            "Step  216200: eval  CrossEntropyLoss |  3.17382240\n",
            "Step  216200: eval          Accuracy |  0.46226415\n",
            "\n",
            "Step  216300: Ran 100 train steps in 35.91 secs\n",
            "Step  216300: train CrossEntropyLoss |  2.66992998\n",
            "Step  216300: eval  CrossEntropyLoss |  2.71851778\n",
            "Step  216300: eval          Accuracy |  0.52830189\n",
            "\n",
            "Step  216400: Ran 100 train steps in 36.12 secs\n",
            "Step  216400: train CrossEntropyLoss |  2.77847171\n",
            "Step  216400: eval  CrossEntropyLoss |  3.14522648\n",
            "Step  216400: eval          Accuracy |  0.49999997\n",
            "\n",
            "Step  216500: Ran 100 train steps in 36.05 secs\n",
            "Step  216500: train CrossEntropyLoss |  2.69883084\n",
            "Step  216500: eval  CrossEntropyLoss |  2.85043430\n",
            "Step  216500: eval          Accuracy |  0.54054058\n",
            "\n",
            "Step  216600: Ran 100 train steps in 35.97 secs\n",
            "Step  216600: train CrossEntropyLoss |  2.79668427\n",
            "Step  216600: eval  CrossEntropyLoss |  3.53408670\n",
            "Step  216600: eval          Accuracy |  0.39622641\n",
            "\n",
            "Step  216700: Ran 100 train steps in 35.92 secs\n",
            "Step  216700: train CrossEntropyLoss |  2.75727892\n",
            "Step  216700: eval  CrossEntropyLoss |  2.66378927\n",
            "Step  216700: eval          Accuracy |  0.50427353\n",
            "\n",
            "Step  216800: Ran 100 train steps in 36.07 secs\n",
            "Step  216800: train CrossEntropyLoss |  2.77391005\n",
            "Step  216800: eval  CrossEntropyLoss |  3.17510939\n",
            "Step  216800: eval          Accuracy |  0.45789474\n",
            "\n",
            "Step  216900: Ran 100 train steps in 36.05 secs\n",
            "Step  216900: train CrossEntropyLoss |  2.72864079\n",
            "Step  216900: eval  CrossEntropyLoss |  2.85704279\n",
            "Step  216900: eval          Accuracy |  0.48571432\n",
            "\n",
            "Step  217000: Ran 100 train steps in 36.14 secs\n",
            "Step  217000: train CrossEntropyLoss |  2.79601717\n",
            "Step  217000: eval  CrossEntropyLoss |  2.47115993\n",
            "Step  217000: eval          Accuracy |  0.52631581\n",
            "\n",
            "Step  217100: Ran 100 train steps in 36.04 secs\n",
            "Step  217100: train CrossEntropyLoss |  2.69347382\n",
            "Step  217100: eval  CrossEntropyLoss |  2.32041359\n",
            "Step  217100: eval          Accuracy |  0.59793818\n",
            "\n",
            "Step  217200: Ran 100 train steps in 36.10 secs\n",
            "Step  217200: train CrossEntropyLoss |  2.75783348\n",
            "Step  217200: eval  CrossEntropyLoss |  2.35118103\n",
            "Step  217200: eval          Accuracy |  0.59042549\n",
            "\n",
            "Step  217300: Ran 100 train steps in 36.18 secs\n",
            "Step  217300: train CrossEntropyLoss |  2.73100543\n",
            "Step  217300: eval  CrossEntropyLoss |  2.47046709\n",
            "Step  217300: eval          Accuracy |  0.53260869\n",
            "\n",
            "Step  217400: Ran 100 train steps in 36.10 secs\n",
            "Step  217400: train CrossEntropyLoss |  2.72250319\n",
            "Step  217400: eval  CrossEntropyLoss |  2.39664912\n",
            "Step  217400: eval          Accuracy |  0.58823532\n",
            "\n",
            "Step  217500: Ran 100 train steps in 36.06 secs\n",
            "Step  217500: train CrossEntropyLoss |  2.73520613\n",
            "Step  217500: eval  CrossEntropyLoss |  2.77120018\n",
            "Step  217500: eval          Accuracy |  0.53731340\n",
            "\n",
            "Step  217600: Ran 100 train steps in 36.06 secs\n",
            "Step  217600: train CrossEntropyLoss |  2.65955448\n",
            "Step  217600: eval  CrossEntropyLoss |  2.84927011\n",
            "Step  217600: eval          Accuracy |  0.54310346\n",
            "\n",
            "Step  217700: Ran 100 train steps in 36.25 secs\n",
            "Step  217700: train CrossEntropyLoss |  2.69901848\n",
            "Step  217700: eval  CrossEntropyLoss |  2.56713867\n",
            "Step  217700: eval          Accuracy |  0.59999996\n",
            "\n",
            "Step  217800: Ran 100 train steps in 36.04 secs\n",
            "Step  217800: train CrossEntropyLoss |  2.72050023\n",
            "Step  217800: eval  CrossEntropyLoss |  2.58406925\n",
            "Step  217800: eval          Accuracy |  0.59183675\n",
            "\n",
            "Step  217900: Ran 100 train steps in 36.01 secs\n",
            "Step  217900: train CrossEntropyLoss |  2.65807390\n",
            "Step  217900: eval  CrossEntropyLoss |  2.43232417\n",
            "Step  217900: eval          Accuracy |  0.54500002\n",
            "\n",
            "Step  218000: Ran 100 train steps in 36.18 secs\n",
            "Step  218000: train CrossEntropyLoss |  2.69757938\n",
            "Step  218000: eval  CrossEntropyLoss |  2.49837375\n",
            "Step  218000: eval          Accuracy |  0.59504128\n",
            "\n",
            "Step  218100: Ran 100 train steps in 36.28 secs\n",
            "Step  218100: train CrossEntropyLoss |  2.72126389\n",
            "Step  218100: eval  CrossEntropyLoss |  2.82303882\n",
            "Step  218100: eval          Accuracy |  0.53763443\n",
            "\n",
            "Step  218200: Ran 100 train steps in 36.48 secs\n",
            "Step  218200: train CrossEntropyLoss |  2.75105333\n",
            "Step  218200: eval  CrossEntropyLoss |  2.45286250\n",
            "Step  218200: eval          Accuracy |  0.58415842\n",
            "\n",
            "Step  218300: Ran 100 train steps in 36.22 secs\n",
            "Step  218300: train CrossEntropyLoss |  2.62819481\n",
            "Step  218300: eval  CrossEntropyLoss |  3.18615770\n",
            "Step  218300: eval          Accuracy |  0.45116279\n",
            "\n",
            "Step  218400: Ran 100 train steps in 36.32 secs\n",
            "Step  218400: train CrossEntropyLoss |  2.75554991\n",
            "Step  218400: eval  CrossEntropyLoss |  3.15128946\n",
            "Step  218400: eval          Accuracy |  0.41176471\n",
            "\n",
            "Step  218500: Ran 100 train steps in 36.28 secs\n",
            "Step  218500: train CrossEntropyLoss |  2.70069122\n",
            "Step  218500: eval  CrossEntropyLoss |  2.80522108\n",
            "Step  218500: eval          Accuracy |  0.55208337\n",
            "\n",
            "Step  218600: Ran 100 train steps in 36.29 secs\n",
            "Step  218600: train CrossEntropyLoss |  2.72034335\n",
            "Step  218600: eval  CrossEntropyLoss |  2.36553645\n",
            "Step  218600: eval          Accuracy |  0.54464287\n",
            "\n",
            "Step  218700: Ran 100 train steps in 36.19 secs\n",
            "Step  218700: train CrossEntropyLoss |  2.69056106\n",
            "Step  218700: eval  CrossEntropyLoss |  2.58398271\n",
            "Step  218700: eval          Accuracy |  0.52631581\n",
            "\n",
            "Step  218800: Ran 100 train steps in 36.37 secs\n",
            "Step  218800: train CrossEntropyLoss |  2.72628140\n",
            "Step  218800: eval  CrossEntropyLoss |  2.71754265\n",
            "Step  218800: eval          Accuracy |  0.55339807\n",
            "\n",
            "Step  218900: Ran 100 train steps in 36.48 secs\n",
            "Step  218900: train CrossEntropyLoss |  2.70384264\n",
            "Step  218900: eval  CrossEntropyLoss |  2.73897743\n",
            "Step  218900: eval          Accuracy |  0.53773588\n",
            "\n",
            "Step  219000: Ran 100 train steps in 36.35 secs\n",
            "Step  219000: train CrossEntropyLoss |  2.68274474\n",
            "Step  219000: eval  CrossEntropyLoss |  2.28804517\n",
            "Step  219000: eval          Accuracy |  0.58415842\n",
            "\n",
            "Step  219100: Ran 100 train steps in 36.30 secs\n",
            "Step  219100: train CrossEntropyLoss |  2.66826463\n",
            "Step  219100: eval  CrossEntropyLoss |  2.88799119\n",
            "Step  219100: eval          Accuracy |  0.49738219\n",
            "\n",
            "Step  219200: Ran 100 train steps in 36.26 secs\n",
            "Step  219200: train CrossEntropyLoss |  2.67541456\n",
            "Step  219200: eval  CrossEntropyLoss |  3.00449252\n",
            "Step  219200: eval          Accuracy |  0.48543689\n",
            "\n",
            "Step  219300: Ran 100 train steps in 36.23 secs\n",
            "Step  219300: train CrossEntropyLoss |  2.73888564\n",
            "Step  219300: eval  CrossEntropyLoss |  2.46246910\n",
            "Step  219300: eval          Accuracy |  0.53043479\n",
            "\n",
            "Step  219400: Ran 100 train steps in 36.33 secs\n",
            "Step  219400: train CrossEntropyLoss |  2.77074027\n",
            "Step  219400: eval  CrossEntropyLoss |  2.59726524\n",
            "Step  219400: eval          Accuracy |  0.50495046\n",
            "\n",
            "Step  219500: Ran 100 train steps in 36.32 secs\n",
            "Step  219500: train CrossEntropyLoss |  2.70713067\n",
            "Step  219500: eval  CrossEntropyLoss |  3.88649392\n",
            "Step  219500: eval          Accuracy |  0.41441444\n",
            "\n",
            "Step  219600: Ran 100 train steps in 36.22 secs\n",
            "Step  219600: train CrossEntropyLoss |  2.69605470\n",
            "Step  219600: eval  CrossEntropyLoss |  2.49539900\n",
            "Step  219600: eval          Accuracy |  0.55963302\n",
            "\n",
            "Step  219700: Ran 100 train steps in 36.19 secs\n",
            "Step  219700: train CrossEntropyLoss |  2.69571090\n",
            "Step  219700: eval  CrossEntropyLoss |  2.65976882\n",
            "Step  219700: eval          Accuracy |  0.59677416\n",
            "\n",
            "Step  219800: Ran 100 train steps in 36.22 secs\n",
            "Step  219800: train CrossEntropyLoss |  2.67672968\n",
            "Step  219800: eval  CrossEntropyLoss |  2.68883371\n",
            "Step  219800: eval          Accuracy |  0.53953487\n",
            "\n",
            "Step  219900: Ran 100 train steps in 36.16 secs\n",
            "Step  219900: train CrossEntropyLoss |  2.74964142\n",
            "Step  219900: eval  CrossEntropyLoss |  2.37874293\n",
            "Step  219900: eval          Accuracy |  0.54545456\n",
            "\n",
            "Step  220000: Ran 100 train steps in 36.33 secs\n",
            "Step  220000: train CrossEntropyLoss |  2.71864653\n",
            "Step  220000: eval  CrossEntropyLoss |  2.80688596\n",
            "Step  220000: eval          Accuracy |  0.55882353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ab3925-596e-49f2-8ee8-0b7e5e72260f"
      },
      "source": [
        "eval_article1 = eval_text_pairs[6][0]\r\n",
        "eval_summary1 = eval_text_pairs[6][1]\r\n",
        "print(wrapper.fill(eval_article1))\r\n",
        "print('')\r\n",
        "eval_article2 = eval_text_pairs[4][0]\r\n",
        "eval_summary2 = eval_text_pairs[4][1]\r\n",
        "print(wrapper.fill(eval_article2))\r\n",
        "print('')\r\n",
        "eval_article3 = eval_text_pairs[7][0]\r\n",
        "eval_summary3 = eval_text_pairs[7][1]\r\n",
        "print(wrapper.fill(eval_article3))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "швейцарская часовая компания audemars piguet представила новую модель\n",
            "из коллекции royal oak. как сообщает luxurylaunches, речь идет о часах\n",
            "с вечным календарем. официальная презентация пройдет в рамках\n",
            "международного салона высокого часового искусства sihh, который\n",
            "проходит в женеве. часы выполнены из черной керамики с механизмом\n",
            "калибра 5134. примерная стоимость часов составляет порядка 85 тысяч\n",
            "долларов. audemars piguet специализируется на производстве люксовых\n",
            "часов. компания была основана в 1875 году, ее штаб-квартира\n",
            "располагается в женеве. с 1999 года фирма является официальным\n",
            "спонсором скачек queen elizabeth ii cup, а также команды alinghi\n",
            "sailing team по парусному спорту. среди знаменитостей, у кого есть\n",
            "часы бренда: футболисты лионель месси и криштиану роналду, гонщики\n",
            "михаэль шумахер и ярно трулли, а также актеры арнольд шварценеггер и\n",
            "хью джекман.\n",
            "\n",
            "американская general motors приняла решение отозвать свои\n",
            "электромобили 1997-1998 года выпуска из-за неполадки, которая может\n",
            "привести к пожару при перезарядке аккумуляторных батарей, передает\n",
            "france presse. речь идет приблизительно о тысяче электромобилей\n",
            "\"первого поколения\", производство которых было налажено с 1997 года.\n",
            "положение настолько серьезно, что конструкторы обратились к владельцам\n",
            "машин с просьбой не перезаряжать батареи до приезда официального\n",
            "представителя компании, который заберет электромобиль на доработку.\n",
            "между тем general motors не собирается прекращать выпуск этих машин,\n",
            "отмечая, что подобные проблемы, как правило, свойственны практически\n",
            "всем машинам этого класса.\n",
            "\n",
            "власти китая требуют отмены американских санкций в отношении банка\n",
            "kunlun, которые были введены 31 июля. об этом сообщает \"синьхуа\" со\n",
            "ссылкой на заявление пресс-секретаря мида кнр цинь ган (qin gang). во\n",
            "вторник сша ввели новые экономические санкции в отношении нефтяных\n",
            "компаний и цб ирана. американские власти запретили компаниям страны\n",
            "закупать нефтехимическую продукцию у тегерана и сотрудничать с\n",
            "национальной иранской нефтяной компанией и иранской компанией по\n",
            "международной торговле нефтью. также был введен запрет на финансовые\n",
            "операции с цб ирана, в том числе на торговлю ценными бумагами и\n",
            "драгоценными металлами. запретительные меры были введены также в\n",
            "отношении банка kunlun и иракского исламского банка elaf. по версии\n",
            "белого дома, эти финорганизации проводили операции с иранскими\n",
            "банками, внесенными ранее в \"черный список\" сша. цинь ган в своем\n",
            "заявлении отметил, что торговые отношения тегерана и пекина являются\n",
            "полностью прозрачными и не имеют никакого отношения к иранской ядерной\n",
            "программе. \"позиция китая в отношении нераспространения [атомного\n",
            "оружия] твердая и ясная\", - добавил представитель мида. сша и ес\n",
            "вводят санкции в отношении ирана, чтобы лишить его средств на\n",
            "предполагаемую разработку ядерного оружия под видом \"мирного атома\". в\n",
            "тегеране утверждают, что ядерная программа носит исключительно мирный\n",
            "характер.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e7228c-a294-49d1-af39-2564e9b28e3f"
      },
      "source": [
        "print(eval_summary1)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article1, model)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "дом audemars piguet оснастил часы вечным календарем\n",
            "\n",
            "\n",
            "a\n",
            "au\n",
            "aud\n",
            "audem\n",
            "audemars\n",
            "audemars p\n",
            "audemars pig\n",
            "audemars pigu\n",
            "audemars piguet\n",
            "audemars piguet представила\n",
            "audemars piguet представила часы\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e2a90b1-cf8b-4736-8fe1-04860c012204"
      },
      "source": [
        "print(eval_summary2)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article2, model)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "general motors отзывает электромобили\n",
            "\n",
            "\n",
            "gener\n",
            "general\n",
            "general mot\n",
            "general motors\n",
            "general motors отозва\n",
            "general motors отозвала\n",
            "general motors отозвала свои\n",
            "general motors отозвала свои электро\n",
            "general motors отозвала свои электромоби\n",
            "general motors отозвала свои электромобили\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elJMh60rN5g4",
        "outputId": "6f78a3d2-58a8-419f-9f74-1c3bc9f0135c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(eval_summary3)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article3, model)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "пекин потребовал от сша снять санкции с китайского банка\n",
            "\n",
            "\n",
            "китай\n",
            "китай пригрозил\n",
            "китай пригрозил санк\n",
            "китай пригрозил санкциями\n",
            "китай пригрозил санкциями за\n",
            "китай пригрозил санкциями за санкции\n",
            "китай пригрозил санкциями за санкции против\n",
            "китай пригрозил санкциями за санкции против банка\n",
            "китай пригрозил санкциями за санкции против банка k\n",
            "китай пригрозил санкциями за санкции против банка kun\n",
            "китай пригрозил санкциями за санкции против банка kunl\n",
            "китай пригрозил санкциями за санкции против банка kunlun\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdhIoFBbamUs"
      },
      "source": [
        "### Hystory generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zr8tbtwda-R"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrulXb2Jfsh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ccda6a-43e4-40f0-e15b-c6dd0318e817"
      },
      "source": [
        "print(wrapper.fill(eval_article3[:300]+'...'), '\\n')\r\n",
        "print('Заголовок:', eval_summary3)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "власти китая требуют отмены американских санкций в отношении банка\n",
            "kunlun, которые были введены 31 июля. об этом сообщает \"синьхуа\" со\n",
            "ссылкой на заявление пресс-секретаря мида кнр цинь ган (qin gang). во\n",
            "вторник сша ввели новые экономические санкции в отношении нефтяных\n",
            "компаний и цб ирана. америка... \n",
            "\n",
            "Заголовок: пекин потребовал от сша снять санкции с китайского банка\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XMeYA6CaAUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d91b00-9bcb-45c1-a075-b7a6db44e22d"
      },
      "source": [
        "for s in range(20, 240, 20):\r\n",
        "    model = SumTransformer(mode='eval')\r\n",
        "    model.init_from_file('/content/drive/MyDrive/model/model' + str(s) + '.pkl.gz', weights_only=True)\r\n",
        "    clear_output(wait=False)\r\n",
        "    print('Эпоха: {0}, {1} шагов'.format(int(s/20), s*1000))\r\n",
        "    _ = greedy_decode(eval_article3, model)    "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Эпоха: 11, 220000 шагов\n",
            "\n",
            "китай\n",
            "китай пригрозил\n",
            "китай пригрозил санк\n",
            "китай пригрозил санкциями\n",
            "китай пригрозил санкциями за\n",
            "китай пригрозил санкциями за санкции\n",
            "китай пригрозил санкциями за санкции против\n",
            "китай пригрозил санкциями за санкции против банка\n",
            "китай пригрозил санкциями за санкции против банка k\n",
            "китай пригрозил санкциями за санкции против банка kun\n",
            "китай пригрозил санкциями за санкции против банка kunl\n",
            "китай пригрозил санкциями за санкции против банка kunlun\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}