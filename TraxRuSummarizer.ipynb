{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLcWO4N+owEng5u1wqA59I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1a56b4-fec5-42d6-db84-78e01988991f"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 17.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 54.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 48.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 55.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 57.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 49.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 58.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 50.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "5dabfa80-32f4-4483-d128-86000f5d0909"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "# data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "# data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "# data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "# data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "# text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "# for i in tqdm(range(data.shape[0])):\r\n",
        "    # if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        # text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafd90d3-592e-44a8-d602-30424903033f"
      },
      "source": [
        "# text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "9a443a1c-929d-46b7-a380-16c307c65131"
      },
      "source": [
        "# s0 = text_pairs[10][0]\r\n",
        "# text_list = wrapper.wrap(s0[:300])\r\n",
        "# for line in text_list:\r\n",
        "#     print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLsMmUFFnHA",
        "outputId": "f3867e97-6fab-41ae-f3e1-dbc9de56fb36"
      },
      "source": [
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n",
            "в киеве исключили новый раунд минских переговоров\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "c9200bc2-b5ae-4239-d5b8-f7cd7fe13475"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC79r_7EPy6",
        "outputId": "cf2398a2-9e24-4044-bc4e-862518b746cf"
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f23a5e-6bb6-424e-b077-396f848b8d8f"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1cfbe3a-0a54-43f3-acef-5165d933943a"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ddc32b9-4d51-483e-9184-00991e1388af"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d501ffd-40cc-4b47-bcf4-fa627a5247ff"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4642 15949     1     0  8548    64    52  3118 13004   131  1836  6848\n",
            "  8765     5  2729  2662  5236  4234  2650     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "0b609f68-57ef-4820-9c53-1b00db82761e"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "31041c1b-1a89-46ba-b228-6f8bd6e0adb7"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  908,  1493,  8236,  1491, 15970,  4186,  2773,  8240,  1830,\n",
              "         298,  1189,  5512, 12549, 15989, 15965,  3038,   249,  5191,\n",
              "         111,  5124,   158,   125, 15926,   642, 15949,   769,   696,\n",
              "        7394,  8862,  4712,    10,  1189,  5512, 13512,  4467,  3808,\n",
              "        3773, 10192,    17,  7557,  8707,  8296,   296,   135,    26,\n",
              "       15945,  3995,    10,    16,  1998,   887,  1967, 15945,   258,\n",
              "        1694,  3021,  5248,  3021,  8772, 15949, 12549, 15989, 15965,\n",
              "        3038,  5275,  2802,  8144,    94,   112,    86, 11586,  2184,\n",
              "        2334, 15941, 15945,    25,  1035,  7675,   971,   224,   257,\n",
              "        1255, 15945,   149,   414,    26,   249,  1998,   887,  1967,\n",
              "          16,   691, 15929, 15744,   249,  8470, 15960,    77,    20,\n",
              "         664, 15949,   224,   257,  1255,  1552,  3713,    54,     5,\n",
              "         437,     2, 15938, 15960,    10,    31, 15938,    16,    25,\n",
              "        4320,  2707,   237,  1096, 15929,   118,    70,    65,    33,\n",
              "         201,  7934,  1470,  8296, 15949, 13329,   149, 13836,   306,\n",
              "           4,  7557,    53,  4220,  1260,  1465,    16,   149,   414,\n",
              "          38,   249, 15672, 15949,  1998,   887,  3844,  5468,  1599,\n",
              "          25,    75,    38,  8932,  8272, 14124,    18, 15945,    79,\n",
              "       15724,  3868,  7322,     5,  1189,  5512, 15949,   691, 15929,\n",
              "       15744,  8276,    40, 15940,  3713,   165,    17,  1270,   463,\n",
              "         245,   208, 15945,    79,  2322,    75, 12240,  3862,  2273,\n",
              "       15945,  1552,  4974,   237,  1507,   338,  8048, 15949,  8276,\n",
              "         223,  1552,    25,  4320,  1972,    16,  2886,  2533,    64,\n",
              "       14209,   118,   249,  9445, 15945,   533,  1672, 15867,    61,\n",
              "        1478, 15949,  1189,  5512,   122,  6733,   412,    19, 15945,\n",
              "        7557,  4651,  1891,  8188,    25,   168,  7055,    25,    64,\n",
              "       10535,    38, 15949,     5,   224,   257,  1255, 13377,     7,\n",
              "        9177, 15951,     2, 15932, 15945,   208,    79,  6156,  8335,\n",
              "       15927,  8143,    46,  8572,  8448,   372, 12495, 15930,   370,\n",
              "       15949,  3337,    16,  2061,  6087,   245,    25,  9825,   564,\n",
              "         975,  3408, 15983,     5,  3057, 15949,     1,     0, 15342,\n",
              "        7573,  4750,    21,    19,    56,  3791,  1189,  5512,   249,\n",
              "        1514,    46,  8572,  8448,     1,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "1327f6a3-0e5f-4bff-9b84-7531634592ec"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.0001)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.0001), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "87f2ebe2-2a56-4014-e357-72c4b50c6caf"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  100100: Ran 100 train steps in 84.39 secs\n",
            "Step  100100: train CrossEntropyLoss |  4.94903660\n",
            "Step  100100: eval  CrossEntropyLoss |  5.36827278\n",
            "Step  100100: eval          Accuracy |  0.18181817\n",
            "\n",
            "Step  100200: Ran 100 train steps in 62.86 secs\n",
            "Step  100200: train CrossEntropyLoss |  4.91038370\n",
            "Step  100200: eval  CrossEntropyLoss |  4.49210167\n",
            "Step  100200: eval          Accuracy |  0.27049181\n",
            "\n",
            "Step  100300: Ran 100 train steps in 46.22 secs\n",
            "Step  100300: train CrossEntropyLoss |  4.82918072\n",
            "Step  100300: eval  CrossEntropyLoss |  4.53268623\n",
            "Step  100300: eval          Accuracy |  0.27659574\n",
            "\n",
            "Step  100400: Ran 100 train steps in 47.61 secs\n",
            "Step  100400: train CrossEntropyLoss |  4.88605928\n",
            "Step  100400: eval  CrossEntropyLoss |  5.35978985\n",
            "Step  100400: eval          Accuracy |  0.20754717\n",
            "\n",
            "Step  100500: Ran 100 train steps in 47.19 secs\n",
            "Step  100500: train CrossEntropyLoss |  4.77426338\n",
            "Step  100500: eval  CrossEntropyLoss |  5.31119871\n",
            "Step  100500: eval          Accuracy |  0.21428572\n",
            "\n",
            "Step  100600: Ran 100 train steps in 46.73 secs\n",
            "Step  100600: train CrossEntropyLoss |  4.71789980\n",
            "Step  100600: eval  CrossEntropyLoss |  4.78500557\n",
            "Step  100600: eval          Accuracy |  0.27184466\n",
            "\n",
            "Step  100700: Ran 100 train steps in 47.51 secs\n",
            "Step  100700: train CrossEntropyLoss |  4.77008390\n",
            "Step  100700: eval  CrossEntropyLoss |  4.58589029\n",
            "Step  100700: eval          Accuracy |  0.26424873\n",
            "\n",
            "Step  100800: Ran 100 train steps in 47.11 secs\n",
            "Step  100800: train CrossEntropyLoss |  4.67216825\n",
            "Step  100800: eval  CrossEntropyLoss |  5.30313683\n",
            "Step  100800: eval          Accuracy |  0.24324325\n",
            "\n",
            "Step  100900: Ran 100 train steps in 47.26 secs\n",
            "Step  100900: train CrossEntropyLoss |  4.65733624\n",
            "Step  100900: eval  CrossEntropyLoss |  4.95618057\n",
            "Step  100900: eval          Accuracy |  0.23232323\n",
            "\n",
            "Step  101000: Ran 100 train steps in 47.16 secs\n",
            "Step  101000: train CrossEntropyLoss |  4.68508863\n",
            "Step  101000: eval  CrossEntropyLoss |  4.39113951\n",
            "Step  101000: eval          Accuracy |  0.24770641\n",
            "\n",
            "Step  101100: Ran 100 train steps in 47.01 secs\n",
            "Step  101100: train CrossEntropyLoss |  4.65245390\n",
            "Step  101100: eval  CrossEntropyLoss |  4.55733490\n",
            "Step  101100: eval          Accuracy |  0.30890054\n",
            "\n",
            "Step  101200: Ran 100 train steps in 47.23 secs\n",
            "Step  101200: train CrossEntropyLoss |  4.63736343\n",
            "Step  101200: eval  CrossEntropyLoss |  5.01825237\n",
            "Step  101200: eval          Accuracy |  0.28282827\n",
            "\n",
            "Step  101300: Ran 100 train steps in 47.10 secs\n",
            "Step  101300: train CrossEntropyLoss |  4.64197874\n",
            "Step  101300: eval  CrossEntropyLoss |  4.20741129\n",
            "Step  101300: eval          Accuracy |  0.29687500\n",
            "\n",
            "Step  101400: Ran 100 train steps in 47.35 secs\n",
            "Step  101400: train CrossEntropyLoss |  4.59856749\n",
            "Step  101400: eval  CrossEntropyLoss |  3.99644303\n",
            "Step  101400: eval          Accuracy |  0.36283186\n",
            "\n",
            "Step  101500: Ran 100 train steps in 47.26 secs\n",
            "Step  101500: train CrossEntropyLoss |  4.60819530\n",
            "Step  101500: eval  CrossEntropyLoss |  3.88572812\n",
            "Step  101500: eval          Accuracy |  0.34715030\n",
            "\n",
            "Step  101600: Ran 100 train steps in 46.98 secs\n",
            "Step  101600: train CrossEntropyLoss |  4.59414768\n",
            "Step  101600: eval  CrossEntropyLoss |  4.79856014\n",
            "Step  101600: eval          Accuracy |  0.25714287\n",
            "\n",
            "Step  101700: Ran 100 train steps in 47.10 secs\n",
            "Step  101700: train CrossEntropyLoss |  4.58061600\n",
            "Step  101700: eval  CrossEntropyLoss |  4.25728178\n",
            "Step  101700: eval          Accuracy |  0.32352942\n",
            "\n",
            "Step  101800: Ran 100 train steps in 47.00 secs\n",
            "Step  101800: train CrossEntropyLoss |  4.53964710\n",
            "Step  101800: eval  CrossEntropyLoss |  5.33939648\n",
            "Step  101800: eval          Accuracy |  0.20661156\n",
            "\n",
            "Step  101900: Ran 100 train steps in 47.20 secs\n",
            "Step  101900: train CrossEntropyLoss |  4.48820877\n",
            "Step  101900: eval  CrossEntropyLoss |  4.69788408\n",
            "Step  101900: eval          Accuracy |  0.26804125\n",
            "\n",
            "Step  102000: Ran 100 train steps in 47.24 secs\n",
            "Step  102000: train CrossEntropyLoss |  4.51195717\n",
            "Step  102000: eval  CrossEntropyLoss |  5.07558537\n",
            "Step  102000: eval          Accuracy |  0.22857144\n",
            "\n",
            "Step  102100: Ran 100 train steps in 47.37 secs\n",
            "Step  102100: train CrossEntropyLoss |  4.58317804\n",
            "Step  102100: eval  CrossEntropyLoss |  4.83068752\n",
            "Step  102100: eval          Accuracy |  0.31428573\n",
            "\n",
            "Step  102200: Ran 100 train steps in 47.19 secs\n",
            "Step  102200: train CrossEntropyLoss |  4.59749031\n",
            "Step  102200: eval  CrossEntropyLoss |  5.04729652\n",
            "Step  102200: eval          Accuracy |  0.21495326\n",
            "\n",
            "Step  102300: Ran 100 train steps in 47.52 secs\n",
            "Step  102300: train CrossEntropyLoss |  4.54717827\n",
            "Step  102300: eval  CrossEntropyLoss |  4.68974686\n",
            "Step  102300: eval          Accuracy |  0.26923078\n",
            "\n",
            "Step  102400: Ran 100 train steps in 47.36 secs\n",
            "Step  102400: train CrossEntropyLoss |  4.51394510\n",
            "Step  102400: eval  CrossEntropyLoss |  4.52420950\n",
            "Step  102400: eval          Accuracy |  0.27272728\n",
            "\n",
            "Step  102500: Ran 100 train steps in 47.04 secs\n",
            "Step  102500: train CrossEntropyLoss |  4.45363188\n",
            "Step  102500: eval  CrossEntropyLoss |  4.88279247\n",
            "Step  102500: eval          Accuracy |  0.22641510\n",
            "\n",
            "Step  102600: Ran 100 train steps in 47.28 secs\n",
            "Step  102600: train CrossEntropyLoss |  4.52117729\n",
            "Step  102600: eval  CrossEntropyLoss |  4.02059460\n",
            "Step  102600: eval          Accuracy |  0.31578949\n",
            "\n",
            "Step  102700: Ran 100 train steps in 47.17 secs\n",
            "Step  102700: train CrossEntropyLoss |  4.53245497\n",
            "Step  102700: eval  CrossEntropyLoss |  4.33862925\n",
            "Step  102700: eval          Accuracy |  0.35344827\n",
            "\n",
            "Step  102800: Ran 100 train steps in 47.15 secs\n",
            "Step  102800: train CrossEntropyLoss |  4.48376894\n",
            "Step  102800: eval  CrossEntropyLoss |  4.28765392\n",
            "Step  102800: eval          Accuracy |  0.35483870\n",
            "\n",
            "Step  102900: Ran 100 train steps in 47.32 secs\n",
            "Step  102900: train CrossEntropyLoss |  4.43510723\n",
            "Step  102900: eval  CrossEntropyLoss |  4.40903997\n",
            "Step  102900: eval          Accuracy |  0.32195121\n",
            "\n",
            "Step  103000: Ran 100 train steps in 47.05 secs\n",
            "Step  103000: train CrossEntropyLoss |  4.49484062\n",
            "Step  103000: eval  CrossEntropyLoss |  4.19992065\n",
            "Step  103000: eval          Accuracy |  0.34482759\n",
            "\n",
            "Step  103100: Ran 100 train steps in 47.45 secs\n",
            "Step  103100: train CrossEntropyLoss |  4.52009106\n",
            "Step  103100: eval  CrossEntropyLoss |  5.21824694\n",
            "Step  103100: eval          Accuracy |  0.25217390\n",
            "\n",
            "Step  103200: Ran 100 train steps in 47.24 secs\n",
            "Step  103200: train CrossEntropyLoss |  4.43689299\n",
            "Step  103200: eval  CrossEntropyLoss |  4.84422827\n",
            "Step  103200: eval          Accuracy |  0.22627737\n",
            "\n",
            "Step  103300: Ran 100 train steps in 47.57 secs\n",
            "Step  103300: train CrossEntropyLoss |  4.49144125\n",
            "Step  103300: eval  CrossEntropyLoss |  3.86908555\n",
            "Step  103300: eval          Accuracy |  0.36585367\n",
            "\n",
            "Step  103400: Ran 100 train steps in 47.50 secs\n",
            "Step  103400: train CrossEntropyLoss |  4.49285412\n",
            "Step  103400: eval  CrossEntropyLoss |  4.43433714\n",
            "Step  103400: eval          Accuracy |  0.25714287\n",
            "\n",
            "Step  103500: Ran 100 train steps in 47.37 secs\n",
            "Step  103500: train CrossEntropyLoss |  4.53301048\n",
            "Step  103500: eval  CrossEntropyLoss |  5.17894268\n",
            "Step  103500: eval          Accuracy |  0.19469026\n",
            "\n",
            "Step  103600: Ran 100 train steps in 47.41 secs\n",
            "Step  103600: train CrossEntropyLoss |  4.49529171\n",
            "Step  103600: eval  CrossEntropyLoss |  4.54052401\n",
            "Step  103600: eval          Accuracy |  0.28431374\n",
            "\n",
            "Step  103700: Ran 100 train steps in 47.56 secs\n",
            "Step  103700: train CrossEntropyLoss |  4.43004704\n",
            "Step  103700: eval  CrossEntropyLoss |  4.18105841\n",
            "Step  103700: eval          Accuracy |  0.29999998\n",
            "\n",
            "Step  103800: Ran 100 train steps in 47.67 secs\n",
            "Step  103800: train CrossEntropyLoss |  4.41487455\n",
            "Step  103800: eval  CrossEntropyLoss |  4.24711180\n",
            "Step  103800: eval          Accuracy |  0.31500000\n",
            "\n",
            "Step  103900: Ran 100 train steps in 47.70 secs\n",
            "Step  103900: train CrossEntropyLoss |  4.51114225\n",
            "Step  103900: eval  CrossEntropyLoss |  4.36798000\n",
            "Step  103900: eval          Accuracy |  0.29464287\n",
            "\n",
            "Step  104000: Ran 100 train steps in 47.51 secs\n",
            "Step  104000: train CrossEntropyLoss |  4.48089409\n",
            "Step  104000: eval  CrossEntropyLoss |  4.58528423\n",
            "Step  104000: eval          Accuracy |  0.23762375\n",
            "\n",
            "Step  104100: Ran 100 train steps in 48.34 secs\n",
            "Step  104100: train CrossEntropyLoss |  4.48725939\n",
            "Step  104100: eval  CrossEntropyLoss |  4.45799494\n",
            "Step  104100: eval          Accuracy |  0.29411766\n",
            "\n",
            "Step  104200: Ran 100 train steps in 48.11 secs\n",
            "Step  104200: train CrossEntropyLoss |  4.42793226\n",
            "Step  104200: eval  CrossEntropyLoss |  4.27921486\n",
            "Step  104200: eval          Accuracy |  0.33809525\n",
            "\n",
            "Step  104300: Ran 100 train steps in 47.73 secs\n",
            "Step  104300: train CrossEntropyLoss |  4.45097113\n",
            "Step  104300: eval  CrossEntropyLoss |  3.97838759\n",
            "Step  104300: eval          Accuracy |  0.36263737\n",
            "\n",
            "Step  104400: Ran 100 train steps in 47.00 secs\n",
            "Step  104400: train CrossEntropyLoss |  4.40990305\n",
            "Step  104400: eval  CrossEntropyLoss |  4.54685926\n",
            "Step  104400: eval          Accuracy |  0.36538464\n",
            "\n",
            "Step  104500: Ran 100 train steps in 47.66 secs\n",
            "Step  104500: train CrossEntropyLoss |  4.47753954\n",
            "Step  104500: eval  CrossEntropyLoss |  4.89733267\n",
            "Step  104500: eval          Accuracy |  0.26999998\n",
            "\n",
            "Step  104600: Ran 100 train steps in 47.27 secs\n",
            "Step  104600: train CrossEntropyLoss |  4.45327044\n",
            "Step  104600: eval  CrossEntropyLoss |  4.68787432\n",
            "Step  104600: eval          Accuracy |  0.26111111\n",
            "\n",
            "Step  104700: Ran 100 train steps in 47.79 secs\n",
            "Step  104700: train CrossEntropyLoss |  4.42979431\n",
            "Step  104700: eval  CrossEntropyLoss |  4.41468954\n",
            "Step  104700: eval          Accuracy |  0.25000000\n",
            "\n",
            "Step  104800: Ran 100 train steps in 47.22 secs\n",
            "Step  104800: train CrossEntropyLoss |  4.36122990\n",
            "Step  104800: eval  CrossEntropyLoss |  4.88601685\n",
            "Step  104800: eval          Accuracy |  0.28440365\n",
            "\n",
            "Step  104900: Ran 100 train steps in 47.38 secs\n",
            "Step  104900: train CrossEntropyLoss |  4.40744066\n",
            "Step  104900: eval  CrossEntropyLoss |  4.60530996\n",
            "Step  104900: eval          Accuracy |  0.23684211\n",
            "\n",
            "Step  105000: Ran 100 train steps in 47.66 secs\n",
            "Step  105000: train CrossEntropyLoss |  4.42491055\n",
            "Step  105000: eval  CrossEntropyLoss |  4.43810987\n",
            "Step  105000: eval          Accuracy |  0.29896909\n",
            "\n",
            "Step  105100: Ran 100 train steps in 47.28 secs\n",
            "Step  105100: train CrossEntropyLoss |  4.39622021\n",
            "Step  105100: eval  CrossEntropyLoss |  5.18226814\n",
            "Step  105100: eval          Accuracy |  0.22807017\n",
            "\n",
            "Step  105200: Ran 100 train steps in 47.63 secs\n",
            "Step  105200: train CrossEntropyLoss |  4.36985874\n",
            "Step  105200: eval  CrossEntropyLoss |  4.70956469\n",
            "Step  105200: eval          Accuracy |  0.27200001\n",
            "\n",
            "Step  105300: Ran 100 train steps in 47.31 secs\n",
            "Step  105300: train CrossEntropyLoss |  4.40625238\n",
            "Step  105300: eval  CrossEntropyLoss |  4.08612633\n",
            "Step  105300: eval          Accuracy |  0.33908045\n",
            "\n",
            "Step  105400: Ran 100 train steps in 47.56 secs\n",
            "Step  105400: train CrossEntropyLoss |  4.41760778\n",
            "Step  105400: eval  CrossEntropyLoss |  4.68609619\n",
            "Step  105400: eval          Accuracy |  0.31707320\n",
            "\n",
            "Step  105500: Ran 100 train steps in 47.35 secs\n",
            "Step  105500: train CrossEntropyLoss |  4.42015839\n",
            "Step  105500: eval  CrossEntropyLoss |  3.52806282\n",
            "Step  105500: eval          Accuracy |  0.37837839\n",
            "\n",
            "Step  105600: Ran 100 train steps in 47.59 secs\n",
            "Step  105600: train CrossEntropyLoss |  4.47421551\n",
            "Step  105600: eval  CrossEntropyLoss |  3.99617982\n",
            "Step  105600: eval          Accuracy |  0.33953488\n",
            "\n",
            "Step  105700: Ran 100 train steps in 47.54 secs\n",
            "Step  105700: train CrossEntropyLoss |  4.41301107\n",
            "Step  105700: eval  CrossEntropyLoss |  4.48405695\n",
            "Step  105700: eval          Accuracy |  0.29743591\n",
            "\n",
            "Step  105800: Ran 100 train steps in 47.68 secs\n",
            "Step  105800: train CrossEntropyLoss |  4.41422510\n",
            "Step  105800: eval  CrossEntropyLoss |  4.51864672\n",
            "Step  105800: eval          Accuracy |  0.25688073\n",
            "\n",
            "Step  105900: Ran 100 train steps in 47.32 secs\n",
            "Step  105900: train CrossEntropyLoss |  4.38210487\n",
            "Step  105900: eval  CrossEntropyLoss |  5.00306225\n",
            "Step  105900: eval          Accuracy |  0.20689654\n",
            "\n",
            "Step  106000: Ran 100 train steps in 47.66 secs\n",
            "Step  106000: train CrossEntropyLoss |  4.36292553\n",
            "Step  106000: eval  CrossEntropyLoss |  5.19039488\n",
            "Step  106000: eval          Accuracy |  0.17857143\n",
            "\n",
            "Step  106100: Ran 100 train steps in 47.54 secs\n",
            "Step  106100: train CrossEntropyLoss |  4.44404554\n",
            "Step  106100: eval  CrossEntropyLoss |  4.85857725\n",
            "Step  106100: eval          Accuracy |  0.27586207\n",
            "\n",
            "Step  106200: Ran 100 train steps in 47.68 secs\n",
            "Step  106200: train CrossEntropyLoss |  4.38724566\n",
            "Step  106200: eval  CrossEntropyLoss |  4.51339531\n",
            "Step  106200: eval          Accuracy |  0.34463277\n",
            "\n",
            "Step  106300: Ran 100 train steps in 47.63 secs\n",
            "Step  106300: train CrossEntropyLoss |  4.38017368\n",
            "Step  106300: eval  CrossEntropyLoss |  4.52695608\n",
            "Step  106300: eval          Accuracy |  0.28571430\n",
            "\n",
            "Step  106400: Ran 100 train steps in 47.42 secs\n",
            "Step  106400: train CrossEntropyLoss |  4.38921547\n",
            "Step  106400: eval  CrossEntropyLoss |  4.98249149\n",
            "Step  106400: eval          Accuracy |  0.19587630\n",
            "\n",
            "Step  106500: Ran 100 train steps in 47.64 secs\n",
            "Step  106500: train CrossEntropyLoss |  4.31880283\n",
            "Step  106500: eval  CrossEntropyLoss |  4.78540087\n",
            "Step  106500: eval          Accuracy |  0.25471699\n",
            "\n",
            "Step  106600: Ran 100 train steps in 47.46 secs\n",
            "Step  106600: train CrossEntropyLoss |  4.33634615\n",
            "Step  106600: eval  CrossEntropyLoss |  4.91768122\n",
            "Step  106600: eval          Accuracy |  0.25438598\n",
            "\n",
            "Step  106700: Ran 100 train steps in 47.58 secs\n",
            "Step  106700: train CrossEntropyLoss |  4.32472897\n",
            "Step  106700: eval  CrossEntropyLoss |  4.03025532\n",
            "Step  106700: eval          Accuracy |  0.34653464\n",
            "\n",
            "Step  106800: Ran 100 train steps in 47.38 secs\n",
            "Step  106800: train CrossEntropyLoss |  4.30615759\n",
            "Step  106800: eval  CrossEntropyLoss |  4.47849178\n",
            "Step  106800: eval          Accuracy |  0.22727272\n",
            "\n",
            "Step  106900: Ran 100 train steps in 47.55 secs\n",
            "Step  106900: train CrossEntropyLoss |  4.41167879\n",
            "Step  106900: eval  CrossEntropyLoss |  4.77425480\n",
            "Step  106900: eval          Accuracy |  0.20000000\n",
            "\n",
            "Step  107000: Ran 100 train steps in 47.63 secs\n",
            "Step  107000: train CrossEntropyLoss |  4.41585732\n",
            "Step  107000: eval  CrossEntropyLoss |  3.99211645\n",
            "Step  107000: eval          Accuracy |  0.30833334\n",
            "\n",
            "Step  107100: Ran 100 train steps in 47.46 secs\n",
            "Step  107100: train CrossEntropyLoss |  4.40874863\n",
            "Step  107100: eval  CrossEntropyLoss |  4.60171461\n",
            "Step  107100: eval          Accuracy |  0.26666668\n",
            "\n",
            "Step  107200: Ran 100 train steps in 47.58 secs\n",
            "Step  107200: train CrossEntropyLoss |  4.31758642\n",
            "Step  107200: eval  CrossEntropyLoss |  4.52705193\n",
            "Step  107200: eval          Accuracy |  0.22935779\n",
            "\n",
            "Step  107300: Ran 100 train steps in 47.46 secs\n",
            "Step  107300: train CrossEntropyLoss |  4.35215902\n",
            "Step  107300: eval  CrossEntropyLoss |  4.19856024\n",
            "Step  107300: eval          Accuracy |  0.29310346\n",
            "\n",
            "Step  107400: Ran 100 train steps in 47.49 secs\n",
            "Step  107400: train CrossEntropyLoss |  4.39507341\n",
            "Step  107400: eval  CrossEntropyLoss |  4.22205687\n",
            "Step  107400: eval          Accuracy |  0.28061223\n",
            "\n",
            "Step  107500: Ran 100 train steps in 47.51 secs\n",
            "Step  107500: train CrossEntropyLoss |  4.29642153\n",
            "Step  107500: eval  CrossEntropyLoss |  4.94633675\n",
            "Step  107500: eval          Accuracy |  0.25454545\n",
            "\n",
            "Step  107600: Ran 100 train steps in 47.43 secs\n",
            "Step  107600: train CrossEntropyLoss |  4.38719177\n",
            "Step  107600: eval  CrossEntropyLoss |  4.39550829\n",
            "Step  107600: eval          Accuracy |  0.33027521\n",
            "\n",
            "Step  107700: Ran 100 train steps in 47.40 secs\n",
            "Step  107700: train CrossEntropyLoss |  4.37867165\n",
            "Step  107700: eval  CrossEntropyLoss |  4.45099258\n",
            "Step  107700: eval          Accuracy |  0.25615764\n",
            "\n",
            "Step  107800: Ran 100 train steps in 47.60 secs\n",
            "Step  107800: train CrossEntropyLoss |  4.37426805\n",
            "Step  107800: eval  CrossEntropyLoss |  4.41418219\n",
            "Step  107800: eval          Accuracy |  0.25471699\n",
            "\n",
            "Step  107900: Ran 100 train steps in 47.44 secs\n",
            "Step  107900: train CrossEntropyLoss |  4.31830168\n",
            "Step  107900: eval  CrossEntropyLoss |  4.80632591\n",
            "Step  107900: eval          Accuracy |  0.25423729\n",
            "\n",
            "Step  108000: Ran 100 train steps in 47.68 secs\n",
            "Step  108000: train CrossEntropyLoss |  4.35256624\n",
            "Step  108000: eval  CrossEntropyLoss |  4.23250818\n",
            "Step  108000: eval          Accuracy |  0.27751198\n",
            "\n",
            "Step  108100: Ran 100 train steps in 47.45 secs\n",
            "Step  108100: train CrossEntropyLoss |  4.36050510\n",
            "Step  108100: eval  CrossEntropyLoss |  4.08433533\n",
            "Step  108100: eval          Accuracy |  0.36448598\n",
            "\n",
            "Step  108200: Ran 100 train steps in 47.59 secs\n",
            "Step  108200: train CrossEntropyLoss |  4.22395229\n",
            "Step  108200: eval  CrossEntropyLoss |  4.97410631\n",
            "Step  108200: eval          Accuracy |  0.25438598\n",
            "\n",
            "Step  108300: Ran 100 train steps in 47.68 secs\n",
            "Step  108300: train CrossEntropyLoss |  4.36951876\n",
            "Step  108300: eval  CrossEntropyLoss |  4.74076653\n",
            "Step  108300: eval          Accuracy |  0.27272725\n",
            "\n",
            "Step  108400: Ran 100 train steps in 47.64 secs\n",
            "Step  108400: train CrossEntropyLoss |  4.36317396\n",
            "Step  108400: eval  CrossEntropyLoss |  4.74923563\n",
            "Step  108400: eval          Accuracy |  0.26262626\n",
            "\n",
            "Step  108500: Ran 100 train steps in 47.48 secs\n",
            "Step  108500: train CrossEntropyLoss |  4.30635548\n",
            "Step  108500: eval  CrossEntropyLoss |  4.31615496\n",
            "Step  108500: eval          Accuracy |  0.28755364\n",
            "\n",
            "Step  108600: Ran 100 train steps in 47.53 secs\n",
            "Step  108600: train CrossEntropyLoss |  4.26298189\n",
            "Step  108600: eval  CrossEntropyLoss |  4.46368265\n",
            "Step  108600: eval          Accuracy |  0.25663716\n",
            "\n",
            "Step  108700: Ran 100 train steps in 48.14 secs\n",
            "Step  108700: train CrossEntropyLoss |  4.27346182\n",
            "Step  108700: eval  CrossEntropyLoss |  4.70238066\n",
            "Step  108700: eval          Accuracy |  0.28282827\n",
            "\n",
            "Step  108800: Ran 100 train steps in 47.47 secs\n",
            "Step  108800: train CrossEntropyLoss |  4.33849716\n",
            "Step  108800: eval  CrossEntropyLoss |  3.97200418\n",
            "Step  108800: eval          Accuracy |  0.30508474\n",
            "\n",
            "Step  108900: Ran 100 train steps in 47.82 secs\n",
            "Step  108900: train CrossEntropyLoss |  4.33474350\n",
            "Step  108900: eval  CrossEntropyLoss |  4.50457430\n",
            "Step  108900: eval          Accuracy |  0.24242425\n",
            "\n",
            "Step  109000: Ran 100 train steps in 47.46 secs\n",
            "Step  109000: train CrossEntropyLoss |  4.37739658\n",
            "Step  109000: eval  CrossEntropyLoss |  3.97039056\n",
            "Step  109000: eval          Accuracy |  0.32673267\n",
            "\n",
            "Step  109100: Ran 100 train steps in 47.55 secs\n",
            "Step  109100: train CrossEntropyLoss |  4.33682489\n",
            "Step  109100: eval  CrossEntropyLoss |  3.98770595\n",
            "Step  109100: eval          Accuracy |  0.34343433\n",
            "\n",
            "Step  109200: Ran 100 train steps in 47.91 secs\n",
            "Step  109200: train CrossEntropyLoss |  4.34034300\n",
            "Step  109200: eval  CrossEntropyLoss |  3.47592092\n",
            "Step  109200: eval          Accuracy |  0.36734694\n",
            "\n",
            "Step  109300: Ran 100 train steps in 47.65 secs\n",
            "Step  109300: train CrossEntropyLoss |  4.39572668\n",
            "Step  109300: eval  CrossEntropyLoss |  3.81426787\n",
            "Step  109300: eval          Accuracy |  0.39705884\n",
            "\n",
            "Step  109400: Ran 100 train steps in 47.50 secs\n",
            "Step  109400: train CrossEntropyLoss |  4.30460691\n",
            "Step  109400: eval  CrossEntropyLoss |  4.25541449\n",
            "Step  109400: eval          Accuracy |  0.25961539\n",
            "\n",
            "Step  109500: Ran 100 train steps in 47.63 secs\n",
            "Step  109500: train CrossEntropyLoss |  4.32068491\n",
            "Step  109500: eval  CrossEntropyLoss |  3.81707716\n",
            "Step  109500: eval          Accuracy |  0.32653061\n",
            "\n",
            "Step  109600: Ran 100 train steps in 47.47 secs\n",
            "Step  109600: train CrossEntropyLoss |  4.27847338\n",
            "Step  109600: eval  CrossEntropyLoss |  4.06712961\n",
            "Step  109600: eval          Accuracy |  0.31481481\n",
            "\n",
            "Step  109700: Ran 100 train steps in 47.53 secs\n",
            "Step  109700: train CrossEntropyLoss |  4.28131390\n",
            "Step  109700: eval  CrossEntropyLoss |  3.90552354\n",
            "Step  109700: eval          Accuracy |  0.34999999\n",
            "\n",
            "Step  109800: Ran 100 train steps in 47.45 secs\n",
            "Step  109800: train CrossEntropyLoss |  4.32402182\n",
            "Step  109800: eval  CrossEntropyLoss |  4.71707869\n",
            "Step  109800: eval          Accuracy |  0.24489796\n",
            "\n",
            "Step  109900: Ran 100 train steps in 47.39 secs\n",
            "Step  109900: train CrossEntropyLoss |  4.38174915\n",
            "Step  109900: eval  CrossEntropyLoss |  4.46881866\n",
            "Step  109900: eval          Accuracy |  0.27461141\n",
            "\n",
            "Step  110000: Ran 100 train steps in 47.40 secs\n",
            "Step  110000: train CrossEntropyLoss |  4.28511906\n",
            "Step  110000: eval  CrossEntropyLoss |  4.40108156\n",
            "Step  110000: eval          Accuracy |  0.29059830\n",
            "\n",
            "Step  110100: Ran 100 train steps in 47.40 secs\n",
            "Step  110100: train CrossEntropyLoss |  4.27196693\n",
            "Step  110100: eval  CrossEntropyLoss |  4.21779251\n",
            "Step  110100: eval          Accuracy |  0.30337080\n",
            "\n",
            "Step  110200: Ran 100 train steps in 47.72 secs\n",
            "Step  110200: train CrossEntropyLoss |  4.31950045\n",
            "Step  110200: eval  CrossEntropyLoss |  4.48027992\n",
            "Step  110200: eval          Accuracy |  0.29032257\n",
            "\n",
            "Step  110300: Ran 100 train steps in 47.65 secs\n",
            "Step  110300: train CrossEntropyLoss |  4.29665327\n",
            "Step  110300: eval  CrossEntropyLoss |  3.96992016\n",
            "Step  110300: eval          Accuracy |  0.33165827\n",
            "\n",
            "Step  110400: Ran 100 train steps in 47.45 secs\n",
            "Step  110400: train CrossEntropyLoss |  4.26528692\n",
            "Step  110400: eval  CrossEntropyLoss |  4.58475971\n",
            "Step  110400: eval          Accuracy |  0.26829270\n",
            "\n",
            "Step  110500: Ran 100 train steps in 47.56 secs\n",
            "Step  110500: train CrossEntropyLoss |  4.25778198\n",
            "Step  110500: eval  CrossEntropyLoss |  4.85298586\n",
            "Step  110500: eval          Accuracy |  0.23999999\n",
            "\n",
            "Step  110600: Ran 100 train steps in 47.45 secs\n",
            "Step  110600: train CrossEntropyLoss |  4.29175377\n",
            "Step  110600: eval  CrossEntropyLoss |  4.69125080\n",
            "Step  110600: eval          Accuracy |  0.24561404\n",
            "\n",
            "Step  110700: Ran 100 train steps in 47.54 secs\n",
            "Step  110700: train CrossEntropyLoss |  4.35138941\n",
            "Step  110700: eval  CrossEntropyLoss |  4.00311852\n",
            "Step  110700: eval          Accuracy |  0.28301889\n",
            "\n",
            "Step  110800: Ran 100 train steps in 47.44 secs\n",
            "Step  110800: train CrossEntropyLoss |  4.35304022\n",
            "Step  110800: eval  CrossEntropyLoss |  4.83637810\n",
            "Step  110800: eval          Accuracy |  0.29687500\n",
            "\n",
            "Step  110900: Ran 100 train steps in 47.59 secs\n",
            "Step  110900: train CrossEntropyLoss |  4.27267647\n",
            "Step  110900: eval  CrossEntropyLoss |  4.78377390\n",
            "Step  110900: eval          Accuracy |  0.27027029\n",
            "\n",
            "Step  111000: Ran 100 train steps in 47.67 secs\n",
            "Step  111000: train CrossEntropyLoss |  4.33205175\n",
            "Step  111000: eval  CrossEntropyLoss |  4.74694920\n",
            "Step  111000: eval          Accuracy |  0.23214287\n",
            "\n",
            "Step  111100: Ran 100 train steps in 47.68 secs\n",
            "Step  111100: train CrossEntropyLoss |  4.28531837\n",
            "Step  111100: eval  CrossEntropyLoss |  4.57773542\n",
            "Step  111100: eval          Accuracy |  0.26315790\n",
            "\n",
            "Step  111200: Ran 100 train steps in 47.58 secs\n",
            "Step  111200: train CrossEntropyLoss |  4.28510332\n",
            "Step  111200: eval  CrossEntropyLoss |  4.10447311\n",
            "Step  111200: eval          Accuracy |  0.33846155\n",
            "\n",
            "Step  111300: Ran 100 train steps in 47.38 secs\n",
            "Step  111300: train CrossEntropyLoss |  4.28063822\n",
            "Step  111300: eval  CrossEntropyLoss |  3.91268754\n",
            "Step  111300: eval          Accuracy |  0.39215687\n",
            "\n",
            "Step  111400: Ran 100 train steps in 47.66 secs\n",
            "Step  111400: train CrossEntropyLoss |  4.27350330\n",
            "Step  111400: eval  CrossEntropyLoss |  3.75070930\n",
            "Step  111400: eval          Accuracy |  0.38095239\n",
            "\n",
            "Step  111500: Ran 100 train steps in 47.53 secs\n",
            "Step  111500: train CrossEntropyLoss |  4.30478382\n",
            "Step  111500: eval  CrossEntropyLoss |  3.65840244\n",
            "Step  111500: eval          Accuracy |  0.37209302\n",
            "\n",
            "Step  111600: Ran 100 train steps in 47.76 secs\n",
            "Step  111600: train CrossEntropyLoss |  4.26357555\n",
            "Step  111600: eval  CrossEntropyLoss |  4.80325079\n",
            "Step  111600: eval          Accuracy |  0.17886180\n",
            "\n",
            "Step  111700: Ran 100 train steps in 47.65 secs\n",
            "Step  111700: train CrossEntropyLoss |  4.22189093\n",
            "Step  111700: eval  CrossEntropyLoss |  4.63036442\n",
            "Step  111700: eval          Accuracy |  0.26213592\n",
            "\n",
            "Step  111800: Ran 100 train steps in 47.29 secs\n",
            "Step  111800: train CrossEntropyLoss |  4.22028303\n",
            "Step  111800: eval  CrossEntropyLoss |  4.07215977\n",
            "Step  111800: eval          Accuracy |  0.37878788\n",
            "\n",
            "Step  111900: Ran 100 train steps in 47.59 secs\n",
            "Step  111900: train CrossEntropyLoss |  4.33206606\n",
            "Step  111900: eval  CrossEntropyLoss |  4.31706142\n",
            "Step  111900: eval          Accuracy |  0.25490198\n",
            "\n",
            "Step  112000: Ran 100 train steps in 47.25 secs\n",
            "Step  112000: train CrossEntropyLoss |  4.17325163\n",
            "Step  112000: eval  CrossEntropyLoss |  4.57701778\n",
            "Step  112000: eval          Accuracy |  0.22772276\n",
            "\n",
            "Step  112100: Ran 100 train steps in 47.60 secs\n",
            "Step  112100: train CrossEntropyLoss |  4.28975201\n",
            "Step  112100: eval  CrossEntropyLoss |  4.26339769\n",
            "Step  112100: eval          Accuracy |  0.31840795\n",
            "\n",
            "Step  112200: Ran 100 train steps in 47.36 secs\n",
            "Step  112200: train CrossEntropyLoss |  4.21458006\n",
            "Step  112200: eval  CrossEntropyLoss |  3.84823251\n",
            "Step  112200: eval          Accuracy |  0.33673468\n",
            "\n",
            "Step  112300: Ran 100 train steps in 47.16 secs\n",
            "Step  112300: train CrossEntropyLoss |  4.29196119\n",
            "Step  112300: eval  CrossEntropyLoss |  3.96310425\n",
            "Step  112300: eval          Accuracy |  0.35245904\n",
            "\n",
            "Step  112400: Ran 100 train steps in 47.29 secs\n",
            "Step  112400: train CrossEntropyLoss |  4.25995827\n",
            "Step  112400: eval  CrossEntropyLoss |  4.57638264\n",
            "Step  112400: eval          Accuracy |  0.25510204\n",
            "\n",
            "Step  112500: Ran 100 train steps in 47.30 secs\n",
            "Step  112500: train CrossEntropyLoss |  4.26832247\n",
            "Step  112500: eval  CrossEntropyLoss |  4.32139015\n",
            "Step  112500: eval          Accuracy |  0.27751198\n",
            "\n",
            "Step  112600: Ran 100 train steps in 47.59 secs\n",
            "Step  112600: train CrossEntropyLoss |  4.24350309\n",
            "Step  112600: eval  CrossEntropyLoss |  3.95046902\n",
            "Step  112600: eval          Accuracy |  0.36274511\n",
            "\n",
            "Step  112700: Ran 100 train steps in 47.41 secs\n",
            "Step  112700: train CrossEntropyLoss |  4.28675413\n",
            "Step  112700: eval  CrossEntropyLoss |  4.25148582\n",
            "Step  112700: eval          Accuracy |  0.37142858\n",
            "\n",
            "Step  112800: Ran 100 train steps in 47.22 secs\n",
            "Step  112800: train CrossEntropyLoss |  4.25249147\n",
            "Step  112800: eval  CrossEntropyLoss |  4.10416126\n",
            "Step  112800: eval          Accuracy |  0.33333334\n",
            "\n",
            "Step  112900: Ran 100 train steps in 47.31 secs\n",
            "Step  112900: train CrossEntropyLoss |  4.16531610\n",
            "Step  112900: eval  CrossEntropyLoss |  4.44395494\n",
            "Step  112900: eval          Accuracy |  0.28318584\n",
            "\n",
            "Step  113000: Ran 100 train steps in 47.41 secs\n",
            "Step  113000: train CrossEntropyLoss |  4.26777792\n",
            "Step  113000: eval  CrossEntropyLoss |  3.99648237\n",
            "Step  113000: eval          Accuracy |  0.34259260\n",
            "\n",
            "Step  113100: Ran 100 train steps in 47.40 secs\n",
            "Step  113100: train CrossEntropyLoss |  4.28594542\n",
            "Step  113100: eval  CrossEntropyLoss |  3.12303257\n",
            "Step  113100: eval          Accuracy |  0.40566039\n",
            "\n",
            "Step  113200: Ran 100 train steps in 47.58 secs\n",
            "Step  113200: train CrossEntropyLoss |  4.28118086\n",
            "Step  113200: eval  CrossEntropyLoss |  3.66256523\n",
            "Step  113200: eval          Accuracy |  0.39583334\n",
            "\n",
            "Step  113300: Ran 100 train steps in 47.39 secs\n",
            "Step  113300: train CrossEntropyLoss |  4.28627348\n",
            "Step  113300: eval  CrossEntropyLoss |  3.75615668\n",
            "Step  113300: eval          Accuracy |  0.32352942\n",
            "\n",
            "Step  113400: Ran 100 train steps in 47.26 secs\n",
            "Step  113400: train CrossEntropyLoss |  4.22348356\n",
            "Step  113400: eval  CrossEntropyLoss |  4.29905558\n",
            "Step  113400: eval          Accuracy |  0.31632653\n",
            "\n",
            "Step  113500: Ran 100 train steps in 47.54 secs\n",
            "Step  113500: train CrossEntropyLoss |  4.21954107\n",
            "Step  113500: eval  CrossEntropyLoss |  5.05464602\n",
            "Step  113500: eval          Accuracy |  0.22900763\n",
            "\n",
            "Step  113600: Ran 100 train steps in 47.43 secs\n",
            "Step  113600: train CrossEntropyLoss |  4.25415087\n",
            "Step  113600: eval  CrossEntropyLoss |  4.31003761\n",
            "Step  113600: eval          Accuracy |  0.31304348\n",
            "\n",
            "Step  113700: Ran 100 train steps in 47.46 secs\n",
            "Step  113700: train CrossEntropyLoss |  4.20649624\n",
            "Step  113700: eval  CrossEntropyLoss |  4.27032518\n",
            "Step  113700: eval          Accuracy |  0.30327871\n",
            "\n",
            "Step  113800: Ran 100 train steps in 47.37 secs\n",
            "Step  113800: train CrossEntropyLoss |  4.20850706\n",
            "Step  113800: eval  CrossEntropyLoss |  4.43877411\n",
            "Step  113800: eval          Accuracy |  0.24043715\n",
            "\n",
            "Step  113900: Ran 100 train steps in 47.44 secs\n",
            "Step  113900: train CrossEntropyLoss |  4.25610542\n",
            "Step  113900: eval  CrossEntropyLoss |  4.12502337\n",
            "Step  113900: eval          Accuracy |  0.31372550\n",
            "\n",
            "Step  114000: Ran 100 train steps in 47.62 secs\n",
            "Step  114000: train CrossEntropyLoss |  4.14995146\n",
            "Step  114000: eval  CrossEntropyLoss |  4.24850082\n",
            "Step  114000: eval          Accuracy |  0.25999999\n",
            "\n",
            "Step  114100: Ran 100 train steps in 47.42 secs\n",
            "Step  114100: train CrossEntropyLoss |  4.19446611\n",
            "Step  114100: eval  CrossEntropyLoss |  3.98719406\n",
            "Step  114100: eval          Accuracy |  0.32999998\n",
            "\n",
            "Step  114200: Ran 100 train steps in 47.41 secs\n",
            "Step  114200: train CrossEntropyLoss |  4.18928862\n",
            "Step  114200: eval  CrossEntropyLoss |  3.62556887\n",
            "Step  114200: eval          Accuracy |  0.31067961\n",
            "\n",
            "Step  114300: Ran 100 train steps in 47.48 secs\n",
            "Step  114300: train CrossEntropyLoss |  4.17695713\n",
            "Step  114300: eval  CrossEntropyLoss |  4.54356718\n",
            "Step  114300: eval          Accuracy |  0.28828830\n",
            "\n",
            "Step  114400: Ran 100 train steps in 47.76 secs\n",
            "Step  114400: train CrossEntropyLoss |  4.26411200\n",
            "Step  114400: eval  CrossEntropyLoss |  4.19477701\n",
            "Step  114400: eval          Accuracy |  0.33333334\n",
            "\n",
            "Step  114500: Ran 100 train steps in 47.64 secs\n",
            "Step  114500: train CrossEntropyLoss |  4.17745304\n",
            "Step  114500: eval  CrossEntropyLoss |  4.32694674\n",
            "Step  114500: eval          Accuracy |  0.28318584\n",
            "\n",
            "Step  114600: Ran 100 train steps in 47.71 secs\n",
            "Step  114600: train CrossEntropyLoss |  4.22838020\n",
            "Step  114600: eval  CrossEntropyLoss |  3.94420409\n",
            "Step  114600: eval          Accuracy |  0.36538464\n",
            "\n",
            "Step  114700: Ran 100 train steps in 47.57 secs\n",
            "Step  114700: train CrossEntropyLoss |  4.23261976\n",
            "Step  114700: eval  CrossEntropyLoss |  3.94623256\n",
            "Step  114700: eval          Accuracy |  0.32195121\n",
            "\n",
            "Step  114800: Ran 100 train steps in 47.84 secs\n",
            "Step  114800: train CrossEntropyLoss |  4.23638248\n",
            "Step  114800: eval  CrossEntropyLoss |  3.91489744\n",
            "Step  114800: eval          Accuracy |  0.37254903\n",
            "\n",
            "Step  114900: Ran 100 train steps in 47.52 secs\n",
            "Step  114900: train CrossEntropyLoss |  4.17727089\n",
            "Step  114900: eval  CrossEntropyLoss |  3.71555376\n",
            "Step  114900: eval          Accuracy |  0.35051548\n",
            "\n",
            "Step  115000: Ran 100 train steps in 47.57 secs\n",
            "Step  115000: train CrossEntropyLoss |  4.15846205\n",
            "Step  115000: eval  CrossEntropyLoss |  3.62342095\n",
            "Step  115000: eval          Accuracy |  0.37113404\n",
            "\n",
            "Step  115100: Ran 100 train steps in 47.47 secs\n",
            "Step  115100: train CrossEntropyLoss |  4.20535374\n",
            "Step  115100: eval  CrossEntropyLoss |  4.09533167\n",
            "Step  115100: eval          Accuracy |  0.31132075\n",
            "\n",
            "Step  115200: Ran 100 train steps in 47.59 secs\n",
            "Step  115200: train CrossEntropyLoss |  4.18629646\n",
            "Step  115200: eval  CrossEntropyLoss |  3.90302539\n",
            "Step  115200: eval          Accuracy |  0.32967034\n",
            "\n",
            "Step  115300: Ran 100 train steps in 47.72 secs\n",
            "Step  115300: train CrossEntropyLoss |  4.21136618\n",
            "Step  115300: eval  CrossEntropyLoss |  4.33831644\n",
            "Step  115300: eval          Accuracy |  0.31192660\n",
            "\n",
            "Step  115400: Ran 100 train steps in 47.56 secs\n",
            "Step  115400: train CrossEntropyLoss |  4.23590136\n",
            "Step  115400: eval  CrossEntropyLoss |  4.14270639\n",
            "Step  115400: eval          Accuracy |  0.29999998\n",
            "\n",
            "Step  115500: Ran 100 train steps in 47.85 secs\n",
            "Step  115500: train CrossEntropyLoss |  4.18029547\n",
            "Step  115500: eval  CrossEntropyLoss |  4.18314505\n",
            "Step  115500: eval          Accuracy |  0.30167601\n",
            "\n",
            "Step  115600: Ran 100 train steps in 47.41 secs\n",
            "Step  115600: train CrossEntropyLoss |  4.28519535\n",
            "Step  115600: eval  CrossEntropyLoss |  4.37016916\n",
            "Step  115600: eval          Accuracy |  0.26016262\n",
            "\n",
            "Step  115700: Ran 100 train steps in 47.64 secs\n",
            "Step  115700: train CrossEntropyLoss |  4.21799755\n",
            "Step  115700: eval  CrossEntropyLoss |  4.52864504\n",
            "Step  115700: eval          Accuracy |  0.25217390\n",
            "\n",
            "Step  115800: Ran 100 train steps in 47.71 secs\n",
            "Step  115800: train CrossEntropyLoss |  4.18783188\n",
            "Step  115800: eval  CrossEntropyLoss |  3.93999529\n",
            "Step  115800: eval          Accuracy |  0.31924883\n",
            "\n",
            "Step  115900: Ran 100 train steps in 47.68 secs\n",
            "Step  115900: train CrossEntropyLoss |  4.14781094\n",
            "Step  115900: eval  CrossEntropyLoss |  4.18272877\n",
            "Step  115900: eval          Accuracy |  0.33644858\n",
            "\n",
            "Step  116000: Ran 100 train steps in 47.58 secs\n",
            "Step  116000: train CrossEntropyLoss |  4.21230316\n",
            "Step  116000: eval  CrossEntropyLoss |  4.28168058\n",
            "Step  116000: eval          Accuracy |  0.29999998\n",
            "\n",
            "Step  116100: Ran 100 train steps in 47.54 secs\n",
            "Step  116100: train CrossEntropyLoss |  4.22136784\n",
            "Step  116100: eval  CrossEntropyLoss |  3.60169554\n",
            "Step  116100: eval          Accuracy |  0.38053098\n",
            "\n",
            "Step  116200: Ran 100 train steps in 47.66 secs\n",
            "Step  116200: train CrossEntropyLoss |  4.14637947\n",
            "Step  116200: eval  CrossEntropyLoss |  4.04666901\n",
            "Step  116200: eval          Accuracy |  0.32620323\n",
            "\n",
            "Step  116300: Ran 100 train steps in 47.57 secs\n",
            "Step  116300: train CrossEntropyLoss |  4.14521980\n",
            "Step  116300: eval  CrossEntropyLoss |  4.36216736\n",
            "Step  116300: eval          Accuracy |  0.34166670\n",
            "\n",
            "Step  116400: Ran 100 train steps in 47.81 secs\n",
            "Step  116400: train CrossEntropyLoss |  4.23380423\n",
            "Step  116400: eval  CrossEntropyLoss |  4.38989162\n",
            "Step  116400: eval          Accuracy |  0.27884617\n",
            "\n",
            "Step  116500: Ran 100 train steps in 47.74 secs\n",
            "Step  116500: train CrossEntropyLoss |  4.16364479\n",
            "Step  116500: eval  CrossEntropyLoss |  4.10771322\n",
            "Step  116500: eval          Accuracy |  0.28712872\n",
            "\n",
            "Step  116600: Ran 100 train steps in 47.64 secs\n",
            "Step  116600: train CrossEntropyLoss |  4.20301580\n",
            "Step  116600: eval  CrossEntropyLoss |  4.80878210\n",
            "Step  116600: eval          Accuracy |  0.24271844\n",
            "\n",
            "Step  116700: Ran 100 train steps in 47.89 secs\n",
            "Step  116700: train CrossEntropyLoss |  4.17123890\n",
            "Step  116700: eval  CrossEntropyLoss |  4.26333189\n",
            "Step  116700: eval          Accuracy |  0.26499999\n",
            "\n",
            "Step  116800: Ran 100 train steps in 47.65 secs\n",
            "Step  116800: train CrossEntropyLoss |  4.12011576\n",
            "Step  116800: eval  CrossEntropyLoss |  4.26958895\n",
            "Step  116800: eval          Accuracy |  0.34313726\n",
            "\n",
            "Step  116900: Ran 100 train steps in 47.54 secs\n",
            "Step  116900: train CrossEntropyLoss |  4.13903522\n",
            "Step  116900: eval  CrossEntropyLoss |  4.13728476\n",
            "Step  116900: eval          Accuracy |  0.28421053\n",
            "\n",
            "Step  117000: Ran 100 train steps in 47.71 secs\n",
            "Step  117000: train CrossEntropyLoss |  4.15404749\n",
            "Step  117000: eval  CrossEntropyLoss |  3.53728032\n",
            "Step  117000: eval          Accuracy |  0.40186915\n",
            "\n",
            "Step  117100: Ran 100 train steps in 47.70 secs\n",
            "Step  117100: train CrossEntropyLoss |  4.19233322\n",
            "Step  117100: eval  CrossEntropyLoss |  3.92660189\n",
            "Step  117100: eval          Accuracy |  0.38659796\n",
            "\n",
            "Step  117200: Ran 100 train steps in 47.75 secs\n",
            "Step  117200: train CrossEntropyLoss |  4.15642977\n",
            "Step  117200: eval  CrossEntropyLoss |  4.13448763\n",
            "Step  117200: eval          Accuracy |  0.35869566\n",
            "\n",
            "Step  117300: Ran 100 train steps in 47.80 secs\n",
            "Step  117300: train CrossEntropyLoss |  4.17251682\n",
            "Step  117300: eval  CrossEntropyLoss |  4.83378363\n",
            "Step  117300: eval          Accuracy |  0.26262626\n",
            "\n",
            "Step  117400: Ran 100 train steps in 47.64 secs\n",
            "Step  117400: train CrossEntropyLoss |  4.15273905\n",
            "Step  117400: eval  CrossEntropyLoss |  3.78448319\n",
            "Step  117400: eval          Accuracy |  0.34975368\n",
            "\n",
            "Step  117500: Ran 100 train steps in 47.97 secs\n",
            "Step  117500: train CrossEntropyLoss |  4.12064695\n",
            "Step  117500: eval  CrossEntropyLoss |  4.36809921\n",
            "Step  117500: eval          Accuracy |  0.32631579\n",
            "\n",
            "Step  117600: Ran 100 train steps in 47.69 secs\n",
            "Step  117600: train CrossEntropyLoss |  4.11353779\n",
            "Step  117600: eval  CrossEntropyLoss |  3.83019781\n",
            "Step  117600: eval          Accuracy |  0.37500003\n",
            "\n",
            "Step  117700: Ran 100 train steps in 47.83 secs\n",
            "Step  117700: train CrossEntropyLoss |  4.12564278\n",
            "Step  117700: eval  CrossEntropyLoss |  4.45151043\n",
            "Step  117700: eval          Accuracy |  0.32999998\n",
            "\n",
            "Step  117800: Ran 100 train steps in 47.86 secs\n",
            "Step  117800: train CrossEntropyLoss |  4.08786535\n",
            "Step  117800: eval  CrossEntropyLoss |  4.36287785\n",
            "Step  117800: eval          Accuracy |  0.33057851\n",
            "\n",
            "Step  117900: Ran 100 train steps in 48.02 secs\n",
            "Step  117900: train CrossEntropyLoss |  4.10958385\n",
            "Step  117900: eval  CrossEntropyLoss |  3.86829090\n",
            "Step  117900: eval          Accuracy |  0.32367149\n",
            "\n",
            "Step  118000: Ran 100 train steps in 48.09 secs\n",
            "Step  118000: train CrossEntropyLoss |  4.08538771\n",
            "Step  118000: eval  CrossEntropyLoss |  3.61702156\n",
            "Step  118000: eval          Accuracy |  0.41935483\n",
            "\n",
            "Step  118100: Ran 100 train steps in 47.72 secs\n",
            "Step  118100: train CrossEntropyLoss |  4.11260748\n",
            "Step  118100: eval  CrossEntropyLoss |  4.66576099\n",
            "Step  118100: eval          Accuracy |  0.23809525\n",
            "\n",
            "Step  118200: Ran 100 train steps in 47.75 secs\n",
            "Step  118200: train CrossEntropyLoss |  4.04804564\n",
            "Step  118200: eval  CrossEntropyLoss |  4.71268415\n",
            "Step  118200: eval          Accuracy |  0.27826086\n",
            "\n",
            "Step  118300: Ran 100 train steps in 47.71 secs\n",
            "Step  118300: train CrossEntropyLoss |  4.12780476\n",
            "Step  118300: eval  CrossEntropyLoss |  3.94644737\n",
            "Step  118300: eval          Accuracy |  0.36815920\n",
            "\n",
            "Step  118400: Ran 100 train steps in 47.99 secs\n",
            "Step  118400: train CrossEntropyLoss |  4.13567448\n",
            "Step  118400: eval  CrossEntropyLoss |  4.26959705\n",
            "Step  118400: eval          Accuracy |  0.35087720\n",
            "\n",
            "Step  118500: Ran 100 train steps in 47.74 secs\n",
            "Step  118500: train CrossEntropyLoss |  4.04163218\n",
            "Step  118500: eval  CrossEntropyLoss |  3.88263583\n",
            "Step  118500: eval          Accuracy |  0.36363634\n",
            "\n",
            "Step  118600: Ran 100 train steps in 47.87 secs\n",
            "Step  118600: train CrossEntropyLoss |  4.16042423\n",
            "Step  118600: eval  CrossEntropyLoss |  4.31993723\n",
            "Step  118600: eval          Accuracy |  0.28571430\n",
            "\n",
            "Step  118700: Ran 100 train steps in 47.65 secs\n",
            "Step  118700: train CrossEntropyLoss |  4.15502405\n",
            "Step  118700: eval  CrossEntropyLoss |  3.94257522\n",
            "Step  118700: eval          Accuracy |  0.39108911\n",
            "\n",
            "Step  118800: Ran 100 train steps in 47.57 secs\n",
            "Step  118800: train CrossEntropyLoss |  4.15476513\n",
            "Step  118800: eval  CrossEntropyLoss |  4.70881701\n",
            "Step  118800: eval          Accuracy |  0.27619049\n",
            "\n",
            "Step  118900: Ran 100 train steps in 47.49 secs\n",
            "Step  118900: train CrossEntropyLoss |  4.07575560\n",
            "Step  118900: eval  CrossEntropyLoss |  4.34403133\n",
            "Step  118900: eval          Accuracy |  0.32743362\n",
            "\n",
            "Step  119000: Ran 100 train steps in 47.67 secs\n",
            "Step  119000: train CrossEntropyLoss |  4.05753279\n",
            "Step  119000: eval  CrossEntropyLoss |  4.57490444\n",
            "Step  119000: eval          Accuracy |  0.30392158\n",
            "\n",
            "Step  119100: Ran 100 train steps in 47.65 secs\n",
            "Step  119100: train CrossEntropyLoss |  4.10626554\n",
            "Step  119100: eval  CrossEntropyLoss |  4.14954901\n",
            "Step  119100: eval          Accuracy |  0.33035716\n",
            "\n",
            "Step  119200: Ran 100 train steps in 47.73 secs\n",
            "Step  119200: train CrossEntropyLoss |  4.06738138\n",
            "Step  119200: eval  CrossEntropyLoss |  4.16557455\n",
            "Step  119200: eval          Accuracy |  0.35428572\n",
            "\n",
            "Step  119300: Ran 100 train steps in 47.71 secs\n",
            "Step  119300: train CrossEntropyLoss |  4.08759403\n",
            "Step  119300: eval  CrossEntropyLoss |  4.41401005\n",
            "Step  119300: eval          Accuracy |  0.30434784\n",
            "\n",
            "Step  119400: Ran 100 train steps in 47.60 secs\n",
            "Step  119400: train CrossEntropyLoss |  4.07759094\n",
            "Step  119400: eval  CrossEntropyLoss |  4.50207996\n",
            "Step  119400: eval          Accuracy |  0.32142860\n",
            "\n",
            "Step  119500: Ran 100 train steps in 47.58 secs\n",
            "Step  119500: train CrossEntropyLoss |  4.09236908\n",
            "Step  119500: eval  CrossEntropyLoss |  4.03582859\n",
            "Step  119500: eval          Accuracy |  0.31282052\n",
            "\n",
            "Step  119600: Ran 100 train steps in 47.49 secs\n",
            "Step  119600: train CrossEntropyLoss |  4.14477634\n",
            "Step  119600: eval  CrossEntropyLoss |  4.23349619\n",
            "Step  119600: eval          Accuracy |  0.28000000\n",
            "\n",
            "Step  119700: Ran 100 train steps in 47.93 secs\n",
            "Step  119700: train CrossEntropyLoss |  4.07773066\n",
            "Step  119700: eval  CrossEntropyLoss |  4.19948483\n",
            "Step  119700: eval          Accuracy |  0.34234235\n",
            "\n",
            "Step  119800: Ran 100 train steps in 47.63 secs\n",
            "Step  119800: train CrossEntropyLoss |  4.05393410\n",
            "Step  119800: eval  CrossEntropyLoss |  4.31937838\n",
            "Step  119800: eval          Accuracy |  0.30392158\n",
            "\n",
            "Step  119900: Ran 100 train steps in 47.78 secs\n",
            "Step  119900: train CrossEntropyLoss |  4.11109018\n",
            "Step  119900: eval  CrossEntropyLoss |  3.75137711\n",
            "Step  119900: eval          Accuracy |  0.34375000\n",
            "\n",
            "Step  120000: Ran 100 train steps in 47.60 secs\n",
            "Step  120000: train CrossEntropyLoss |  4.06115246\n",
            "Step  120000: eval  CrossEntropyLoss |  3.75534296\n",
            "Step  120000: eval          Accuracy |  0.36842108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "# !rsync -a /content/drive/MyDrive/model2/ ~/\r\n",
        "\r\n",
        "# copy the model to Google Drive\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model/\r\n",
        "\r\n",
        "# sync Google Drive dir with the train dir\r\n",
        "# !rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c76587b-69ec-4bc5-fd6f-c0fa62d95ddf"
      },
      "source": [
        "train_article = train_text_pairs[5][0]\r\n",
        "train_summary = train_text_pairs[5][1]\r\n",
        "print(wrapper.fill(train_article))\r\n",
        "print('')\r\n",
        "eval_article = eval_text_pairs[1][0]\r\n",
        "eval_summary = eval_text_pairs[1][1]\r\n",
        "print(wrapper.fill(eval_article))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые придумали новый способ взаимодействия с графеном, который\n",
            "позволяет избавиться от \"слипающихся\" листов. статья ученых появилась\n",
            "в журнале acs nano, а ее краткое изложение приводится на сайте северо-\n",
            "западного университета, сотрудники которого принимали участие в\n",
            "работе. известно, что основной трудностью при работе с графеновыми\n",
            "листами является то, что при соприкосновении они слипаются под\n",
            "воздействием сил ван-дер-ваальса между собой при наложении друг на\n",
            "друга. это приводит к потере большинства уникальных свойств материала.\n",
            "для решения подобной проблемы, например, некоторые исследователи\n",
            "кладут между листами прокладки из другого материала, однако такое\n",
            "решение часто не слишком эффективно - атомы прокладки могут\n",
            "образовывать связи с атомами углерода в графене, что снова приводит к\n",
            "появлению дефектов в материале. в рамках нового исследования ученые\n",
            "предложили использовать графен не в виде ровных листов, а в виде\n",
            "смятых в комок листов. по словам исследователей, в подобном виде\n",
            "графен ведет себя как бумажные комки в мусорной корзине - несмотря на\n",
            "достаточно плотное расположение, поверхности листов, из которых они\n",
            "состоят, не соприкасаются. расчеты показывают, что при подобной\n",
            "упаковке листов графен сохраняет около 45 процентов исходной площади\n",
            "поверхности. для сравнения, при других способах организации удается\n",
            "спасти не более 16 процентов площади. графен как теоретическая\n",
            "абстракция рассматривался еще в конце 20-х годов прошлого века.\n",
            "начиная с 1960-х годов, он выступал в качестве удобной математической\n",
            "модели для расчетов в квантовой механике. впервые графен получили на\n",
            "практике константин новоселов и андрей гейм в 2004 году.\n",
            "\n",
            "сша планируют сократить численность военного контингента в южной\n",
            "корее. по информации корейского министерства иностранных дел, к концу\n",
            "2005 года из страны будет выведена треть американского контингента,\n",
            "составляющего в настоящее время 37500 военнослужащих, сообщает\n",
            "reuters. всего к концу 2005 года страну покинут 12500 американских\n",
            "солдат. 3600 из них продолжат службу в ираке. глава корейского мид\n",
            "отметил, что сша подходят к выводу войск очень внимательно, так как\n",
            "ситуация на полуострове остается напряженной. тем не менее, сша пошли\n",
            "навстречу желанию властей южной кореи иметь более независимую армию, и\n",
            "обещают оказать им в этом всяческое содействие. собственные силы южной\n",
            "кореи составляют на сегодняшний день 690 000 человек. армия северной\n",
            "кореи насчитывает 1 100 000 военнослужащих.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea3040b-1d67-437c-cede-74b0574f5631"
      },
      "source": [
        "# checking first symbol generation\r\n",
        "print(detokenize([next_symbol(tokenize(train_article)+[0], model)]))\r\n",
        "print(detokenize([next_symbol(tokenize(eval_article)+[0], model)]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые\n",
            "сша\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        \r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6289c8e7-83ce-46b4-f8fd-ee04ff0e2c99"
      },
      "source": [
        "print(train_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(train_article, model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые предложили использовать графен в мятом виде\n",
            "\n",
            "\n",
            "ученые\n",
            "ученые нашли\n",
            "ученые нашли способ\n",
            "ученые нашли способ борьбы\n",
            "ученые нашли способ борьбы с\n",
            "ученые нашли способ борьбы с \"\n",
            "ученые нашли способ борьбы с \"не\n",
            "ученые нашли способ борьбы с \"непа\n",
            "ученые нашли способ борьбы с \"непами\n",
            "ученые нашли способ борьбы с \"непами\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d06e978-1071-442a-d501-e6450698d25c"
      },
      "source": [
        "print(eval_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article, model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша на треть сократят корейскую группировку\n",
            "\n",
            "\n",
            "сша\n",
            "сша будут\n",
            "сша будут строить\n",
            "сша будут строить оружие\n",
            "сша будут строить оружие в\n",
            "сша будут строить оружие в кндр\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L83lEskk4L7"
      },
      "source": [
        "model = SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='predict',\r\n",
        "                  ff_activation=tl.Relu)\r\n",
        "\r\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSDbAXjlF2f"
      },
      "source": [
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True, input_signature=shape11)\r\n",
        "\r\n",
        "# save the starting state\r\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f11ac1f-a934-467a-bc7b-91e7e301d113"
      },
      "source": [
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "model.state = STARTING_STATE\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(tokenize(eval_summary))[None, :],\r\n",
        "                                        temperature=0.2, max_length=20)\r\n",
        "print(wrapper.fill(detokenize(output[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша кубометров кубометров кубометров кубометров кубометров кубометров\n",
            "кубометров кубометров кубометров кубометров кубометров кубометров\n",
            "кубометров кубометров кубометров кубометров кубометров кубометров\n",
            "кубометров\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}