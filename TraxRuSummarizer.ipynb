{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP7gLQmtppigd1JcuBH266O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4668cb-fa8d-4a46-c68e-3725bed7499b"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▋                               | 10kB 21.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 28.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 25.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 40kB 22.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51kB 22.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 61kB 16.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 71kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 81kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 92kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 102kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 112kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 122kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 133kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 143kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 153kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 163kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 174kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 184kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 194kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 204kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 215kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 225kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 235kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 245kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 256kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 266kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 276kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 286kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 296kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 307kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 317kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 327kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 337kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 348kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 358kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 368kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 378kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 389kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 399kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 409kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 419kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 430kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 440kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 450kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 460kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 471kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 481kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 491kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 501kB 16.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 512kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 522kB 16.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 49.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 52.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 53.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 50.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 41.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 49.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "525ec763-995c-43a4-e193-a4ccefdbe9e0"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "0c35d547-c1c4-477b-afa4-0396822c2c62"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "2a3ca215-98a4-4d19-b474-84d25cbe766a"
      },
      "source": [
        "data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9f000db198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "e8b96490-7e22-469f-a212-0b89e94fffa1"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "for i in tqdm(range(data.shape[0])):\r\n",
        "    if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12257.31it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241dad8b-56fa-415f-e080-aa9559271518"
      },
      "source": [
        "text_pairs[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "0c05f24c-1409-4d17-c0b4-f08348e40926"
      },
      "source": [
        "s0 = text_pairs[10][0]\r\n",
        "text_list = wrapper.wrap(s0[:300])\r\n",
        "for line in text_list:\r\n",
        "    print(line)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "8e640a13-da1a-4943-b7e3-ba774338cede"
      },
      "source": [
        "# inintial shuffling\r\n",
        "random.shuffle(text_pairs)\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e587aceb-56b6-402d-a673-be878d5a273c"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64cb6bec-6eea-49a7-bad5-6df9626e881a"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b375535-9531-4ff4-e212-27eb925285ad"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775e0a46-5865-4c1d-e605-615b827da765"
      },
      "source": [
        "tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "print('tokenized:')\r\n",
        "print(tokenized)\r\n",
        "print('len=', len(tokenized))\r\n",
        "detokenized = detokenize(tokenized)\r\n",
        "print('detokenized:')\r\n",
        "print(detokenized)\r\n",
        "print('len=', len(detokenized.split()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized:\n",
            "[3044, 11, 1627, 1080, 25, 1445, 288, 4205, 5442, 15945, 939, 11463, 1410, 164, 13393, 164, 8476, 1]\n",
            "len= 18\n",
            "detokenized:\n",
            "сведения о пассажирах на всех видах транспорта, где используются\n",
            "именные проездные билеты\n",
            "len= 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA-D8rF6JANP",
        "outputId": "b3e7778c-faef-402c-ec7f-1b3d9b5fe9eb"
      },
      "source": [
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10159   432    36  1151 15949     1     0  6309  3861    59  5884  2573\n",
            " 12635    17  2431    63  3644  1997  7516     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [256, 512, 1024]\r\n",
        "batch_sizes = [16, 8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "fd143f86-c7ab-4348-bc34-2ac96869a76a"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "43937877-5c8f-4462-c9be-148432485bee"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5559,   268,   110,  6990, 15972,  7817,  3658,  1068,  7216,\n",
              "         188,   110,   118,  1152,   556, 15972,     5, 11938, 15937,\n",
              "        9103,  1749,  1394, 15960,  8127, 15949,    63,   226,   258,\n",
              "        3999,   110,  2719, 15949,    53,   371,  2781,  5693,  1808,\n",
              "        1215,     5,  2386, 15960,  4522,    16,  7349,  9126,   110,\n",
              "        8602, 15972,    48,  2756,   441,  8912,  1644,     5,  2662,\n",
              "        7282, 13006, 15996,    25, 12532, 15947,  4727, 13851,  5381,\n",
              "         906,   251, 15949,  4020,   733,  8273,  7216,   509, 14791,\n",
              "         303,   241,   128,  1944,  8949,  1064,   319,  8453,   296,\n",
              "        1828, 10980, 15949,    46,  2370,  3465,    70,  2899,  2667,\n",
              "        2309,    83,  6399,  5580, 12591, 14790,   221,    16,   102,\n",
              "        2615,   110,  6990, 15972, 10509,   210,    85,  8912,    25,\n",
              "        6088, 15960, 15947,  4727, 13851,  3149,  3636,   710, 15934,\n",
              "       15935, 15945,    41,    25,   645,  6617, 15947,   906,   251,\n",
              "       11584, 15934, 11930,    30,    16,  8714, 15808,  1644,   210,\n",
              "         441,  8912,  3588,  1324, 13162,   110,  6990, 15972,  6474,\n",
              "        6221,  4458, 15945,  3731,    25,  5986,   928,     5,   868,\n",
              "        9079,    16,    25,  1105,  8372, 10195,    22,    75,  7682,\n",
              "        1661,   841,  3979, 15949,   110,   118,  1152, 15946, 15972,\n",
              "           4,   142, 12296, 11279,    25, 11040, 15937,  1814,   210,\n",
              "           5,  6123,  8247,   111,  5187, 15949,   296,  1828,  2522,\n",
              "        8035,     5,   110,   118,  1152, 15946, 15972,    86,   110,\n",
              "        5766,   297, 15972,   167,  5766,   252, 13355,   219, 15977,\n",
              "           5,  3087,  1820,   173, 15949,    25,  6254,  7631,  2747,\n",
              "        1064,   319,  3262,  5187,    46,  7216,   268,  3168, 15945,\n",
              "           5,  1035,   236,  5580,   733, 12410, 15949,    85,  1110,\n",
              "         110,  6990, 15972,  5984,     5,  8316,  4234,   278,     4,\n",
              "        3979, 15945,  2781,  2856,     5, 11509,    16,  5890,     5,\n",
              "        1238,  3016,  8131, 15949,     1,     0,   110,  6990, 15972,\n",
              "           4, 13330,  2756, 10692,   110,   118,  1152, 15946, 15972,\n",
              "          16, 11104,    25,  5986,   928,     5,   868,  9079,     1,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "5ec92274-b732-4130-8900-da75ba396d1e"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    # lr_schedule = trax.supervised.lr_schedules.constant(0.00015)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.00015), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "e077908d-8074-46b9-e2ad-529a16dc7d78"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 37412480\n",
            "Step      1: Ran 1 train steps in 41.65 secs\n",
            "Step      1: train CrossEntropyLoss |  9.70907497\n",
            "Step      1: eval  CrossEntropyLoss |  9.74453735\n",
            "Step      1: eval          Accuracy |  0.00000000\n",
            "\n",
            "Step    100: Ran 99 train steps in 66.21 secs\n",
            "Step    100: train CrossEntropyLoss |  9.61032772\n",
            "Step    100: eval  CrossEntropyLoss |  9.40427589\n",
            "Step    100: eval          Accuracy |  0.09195402\n",
            "\n",
            "Step    200: Ran 100 train steps in 50.33 secs\n",
            "Step    200: train CrossEntropyLoss |  9.28285027\n",
            "Step    200: eval  CrossEntropyLoss |  9.13938427\n",
            "Step    200: eval          Accuracy |  0.08247423\n",
            "\n",
            "Step    300: Ran 100 train steps in 69.00 secs\n",
            "Step    300: train CrossEntropyLoss |  9.02209854\n",
            "Step    300: eval  CrossEntropyLoss |  8.93210506\n",
            "Step    300: eval          Accuracy |  0.06666667\n",
            "\n",
            "Step    400: Ran 100 train steps in 50.62 secs\n",
            "Step    400: train CrossEntropyLoss |  8.77995872\n",
            "Step    400: eval  CrossEntropyLoss |  8.64559364\n",
            "Step    400: eval          Accuracy |  0.07619048\n",
            "\n",
            "Step    500: Ran 100 train steps in 50.66 secs\n",
            "Step    500: train CrossEntropyLoss |  8.55285358\n",
            "Step    500: eval  CrossEntropyLoss |  8.52448177\n",
            "Step    500: eval          Accuracy |  0.07843138\n",
            "\n",
            "Step    600: Ran 100 train steps in 50.70 secs\n",
            "Step    600: train CrossEntropyLoss |  8.33431911\n",
            "Step    600: eval  CrossEntropyLoss |  8.41315556\n",
            "Step    600: eval          Accuracy |  0.08163265\n",
            "\n",
            "Step    700: Ran 100 train steps in 50.69 secs\n",
            "Step    700: train CrossEntropyLoss |  8.15660954\n",
            "Step    700: eval  CrossEntropyLoss |  7.96026516\n",
            "Step    700: eval          Accuracy |  0.08465609\n",
            "\n",
            "Step    800: Ran 100 train steps in 51.01 secs\n",
            "Step    800: train CrossEntropyLoss |  8.01577473\n",
            "Step    800: eval  CrossEntropyLoss |  8.16311932\n",
            "Step    800: eval          Accuracy |  0.07692308\n",
            "\n",
            "Step    900: Ran 100 train steps in 50.77 secs\n",
            "Step    900: train CrossEntropyLoss |  7.96055937\n",
            "Step    900: eval  CrossEntropyLoss |  7.85527468\n",
            "Step    900: eval          Accuracy |  0.07766990\n",
            "\n",
            "Step   1000: Ran 100 train steps in 50.57 secs\n",
            "Step   1000: train CrossEntropyLoss |  7.93394947\n",
            "Step   1000: eval  CrossEntropyLoss |  8.11075020\n",
            "Step   1000: eval          Accuracy |  0.07619048\n",
            "\n",
            "Step   1100: Ran 100 train steps in 50.67 secs\n",
            "Step   1100: train CrossEntropyLoss |  7.90678501\n",
            "Step   1100: eval  CrossEntropyLoss |  7.66420078\n",
            "Step   1100: eval          Accuracy |  0.07407407\n",
            "\n",
            "Step   1200: Ran 100 train steps in 50.64 secs\n",
            "Step   1200: train CrossEntropyLoss |  7.92320299\n",
            "Step   1200: eval  CrossEntropyLoss |  8.04715157\n",
            "Step   1200: eval          Accuracy |  0.08163265\n",
            "\n",
            "Step   1300: Ran 100 train steps in 50.87 secs\n",
            "Step   1300: train CrossEntropyLoss |  7.90704775\n",
            "Step   1300: eval  CrossEntropyLoss |  7.95726824\n",
            "Step   1300: eval          Accuracy |  0.06779661\n",
            "\n",
            "Step   1400: Ran 100 train steps in 50.66 secs\n",
            "Step   1400: train CrossEntropyLoss |  7.88396263\n",
            "Step   1400: eval  CrossEntropyLoss |  7.76349688\n",
            "Step   1400: eval          Accuracy |  0.08888889\n",
            "\n",
            "Step   1500: Ran 100 train steps in 50.69 secs\n",
            "Step   1500: train CrossEntropyLoss |  7.91332626\n",
            "Step   1500: eval  CrossEntropyLoss |  7.88370037\n",
            "Step   1500: eval          Accuracy |  0.06956521\n",
            "\n",
            "Step   1600: Ran 100 train steps in 50.64 secs\n",
            "Step   1600: train CrossEntropyLoss |  7.90948725\n",
            "Step   1600: eval  CrossEntropyLoss |  7.74781275\n",
            "Step   1600: eval          Accuracy |  0.06349207\n",
            "\n",
            "Step   1700: Ran 100 train steps in 50.52 secs\n",
            "Step   1700: train CrossEntropyLoss |  7.90234613\n",
            "Step   1700: eval  CrossEntropyLoss |  7.96433735\n",
            "Step   1700: eval          Accuracy |  0.07920792\n",
            "\n",
            "Step   1800: Ran 100 train steps in 50.73 secs\n",
            "Step   1800: train CrossEntropyLoss |  7.93917084\n",
            "Step   1800: eval  CrossEntropyLoss |  7.76069832\n",
            "Step   1800: eval          Accuracy |  0.07729468\n",
            "\n",
            "Step   1900: Ran 100 train steps in 50.58 secs\n",
            "Step   1900: train CrossEntropyLoss |  7.87905884\n",
            "Step   1900: eval  CrossEntropyLoss |  7.67072344\n",
            "Step   1900: eval          Accuracy |  0.07079646\n",
            "\n",
            "Step   2000: Ran 100 train steps in 50.57 secs\n",
            "Step   2000: train CrossEntropyLoss |  7.91266346\n",
            "Step   2000: eval  CrossEntropyLoss |  7.95748091\n",
            "Step   2000: eval          Accuracy |  0.08000000\n",
            "\n",
            "Step   2100: Ran 100 train steps in 50.94 secs\n",
            "Step   2100: train CrossEntropyLoss |  7.91048384\n",
            "Step   2100: eval  CrossEntropyLoss |  7.79145050\n",
            "Step   2100: eval          Accuracy |  0.07619048\n",
            "\n",
            "Step   2200: Ran 100 train steps in 50.31 secs\n",
            "Step   2200: train CrossEntropyLoss |  7.90654707\n",
            "Step   2200: eval  CrossEntropyLoss |  7.75964928\n",
            "Step   2200: eval          Accuracy |  0.07920792\n",
            "\n",
            "Step   2300: Ran 100 train steps in 50.61 secs\n",
            "Step   2300: train CrossEntropyLoss |  7.91851664\n",
            "Step   2300: eval  CrossEntropyLoss |  7.86068916\n",
            "Step   2300: eval          Accuracy |  0.07111111\n",
            "\n",
            "Step   2400: Ran 100 train steps in 50.24 secs\n",
            "Step   2400: train CrossEntropyLoss |  7.89930630\n",
            "Step   2400: eval  CrossEntropyLoss |  8.27771282\n",
            "Step   2400: eval          Accuracy |  0.08888889\n",
            "\n",
            "Step   2500: Ran 100 train steps in 50.61 secs\n",
            "Step   2500: train CrossEntropyLoss |  7.89966726\n",
            "Step   2500: eval  CrossEntropyLoss |  8.19725323\n",
            "Step   2500: eval          Accuracy |  0.07079646\n",
            "\n",
            "Step   2600: Ran 100 train steps in 50.78 secs\n",
            "Step   2600: train CrossEntropyLoss |  7.90327215\n",
            "Step   2600: eval  CrossEntropyLoss |  7.77688456\n",
            "Step   2600: eval          Accuracy |  0.06504066\n",
            "\n",
            "Step   2700: Ran 100 train steps in 50.77 secs\n",
            "Step   2700: train CrossEntropyLoss |  7.92077017\n",
            "Step   2700: eval  CrossEntropyLoss |  7.82705879\n",
            "Step   2700: eval          Accuracy |  0.07692308\n",
            "\n",
            "Step   2800: Ran 100 train steps in 50.81 secs\n",
            "Step   2800: train CrossEntropyLoss |  7.90044212\n",
            "Step   2800: eval  CrossEntropyLoss |  8.13942623\n",
            "Step   2800: eval          Accuracy |  0.08465609\n",
            "\n",
            "Step   2900: Ran 100 train steps in 50.63 secs\n",
            "Step   2900: train CrossEntropyLoss |  7.90929937\n",
            "Step   2900: eval  CrossEntropyLoss |  8.03588772\n",
            "Step   2900: eval          Accuracy |  0.05925926\n",
            "\n",
            "Step   3000: Ran 100 train steps in 50.66 secs\n",
            "Step   3000: train CrossEntropyLoss |  7.91475677\n",
            "Step   3000: eval  CrossEntropyLoss |  8.35626793\n",
            "Step   3000: eval          Accuracy |  0.09183674\n",
            "\n",
            "Step   3100: Ran 100 train steps in 50.71 secs\n",
            "Step   3100: train CrossEntropyLoss |  7.88084888\n",
            "Step   3100: eval  CrossEntropyLoss |  7.75157213\n",
            "Step   3100: eval          Accuracy |  0.09947644\n",
            "\n",
            "Step   3200: Ran 100 train steps in 50.65 secs\n",
            "Step   3200: train CrossEntropyLoss |  7.85961103\n",
            "Step   3200: eval  CrossEntropyLoss |  7.78821993\n",
            "Step   3200: eval          Accuracy |  0.07079646\n",
            "\n",
            "Step   3300: Ran 100 train steps in 50.75 secs\n",
            "Step   3300: train CrossEntropyLoss |  7.85485029\n",
            "Step   3300: eval  CrossEntropyLoss |  7.72721624\n",
            "Step   3300: eval          Accuracy |  0.08421053\n",
            "\n",
            "Step   3400: Ran 100 train steps in 50.62 secs\n",
            "Step   3400: train CrossEntropyLoss |  7.82309055\n",
            "Step   3400: eval  CrossEntropyLoss |  7.90032816\n",
            "Step   3400: eval          Accuracy |  0.08256880\n",
            "\n",
            "Step   3500: Ran 100 train steps in 50.52 secs\n",
            "Step   3500: train CrossEntropyLoss |  7.80466986\n",
            "Step   3500: eval  CrossEntropyLoss |  7.66650963\n",
            "Step   3500: eval          Accuracy |  0.08133972\n",
            "\n",
            "Step   3600: Ran 100 train steps in 50.58 secs\n",
            "Step   3600: train CrossEntropyLoss |  7.79298925\n",
            "Step   3600: eval  CrossEntropyLoss |  7.80921984\n",
            "Step   3600: eval          Accuracy |  0.07627118\n",
            "\n",
            "Step   3700: Ran 100 train steps in 50.75 secs\n",
            "Step   3700: train CrossEntropyLoss |  7.76240301\n",
            "Step   3700: eval  CrossEntropyLoss |  7.67397785\n",
            "Step   3700: eval          Accuracy |  0.08333334\n",
            "\n",
            "Step   3800: Ran 100 train steps in 50.38 secs\n",
            "Step   3800: train CrossEntropyLoss |  7.73306274\n",
            "Step   3800: eval  CrossEntropyLoss |  7.48598099\n",
            "Step   3800: eval          Accuracy |  0.08035715\n",
            "\n",
            "Step   3900: Ran 100 train steps in 50.63 secs\n",
            "Step   3900: train CrossEntropyLoss |  7.70576382\n",
            "Step   3900: eval  CrossEntropyLoss |  7.58051157\n",
            "Step   3900: eval          Accuracy |  0.06666667\n",
            "\n",
            "Step   4000: Ran 100 train steps in 50.32 secs\n",
            "Step   4000: train CrossEntropyLoss |  7.68735266\n",
            "Step   4000: eval  CrossEntropyLoss |  7.57113028\n",
            "Step   4000: eval          Accuracy |  0.09500000\n",
            "\n",
            "Step   4100: Ran 100 train steps in 50.18 secs\n",
            "Step   4100: train CrossEntropyLoss |  7.68899870\n",
            "Step   4100: eval  CrossEntropyLoss |  7.62386847\n",
            "Step   4100: eval          Accuracy |  0.07438016\n",
            "\n",
            "Step   4200: Ran 100 train steps in 50.26 secs\n",
            "Step   4200: train CrossEntropyLoss |  7.66344595\n",
            "Step   4200: eval  CrossEntropyLoss |  7.53114176\n",
            "Step   4200: eval          Accuracy |  0.09734513\n",
            "\n",
            "Step   4300: Ran 100 train steps in 50.36 secs\n",
            "Step   4300: train CrossEntropyLoss |  7.64479589\n",
            "Step   4300: eval  CrossEntropyLoss |  7.87310696\n",
            "Step   4300: eval          Accuracy |  0.07305937\n",
            "\n",
            "Step   4400: Ran 100 train steps in 50.36 secs\n",
            "Step   4400: train CrossEntropyLoss |  7.67854595\n",
            "Step   4400: eval  CrossEntropyLoss |  7.57349157\n",
            "Step   4400: eval          Accuracy |  0.07079646\n",
            "\n",
            "Step   4500: Ran 100 train steps in 50.35 secs\n",
            "Step   4500: train CrossEntropyLoss |  7.61193037\n",
            "Step   4500: eval  CrossEntropyLoss |  7.55854893\n",
            "Step   4500: eval          Accuracy |  0.07142857\n",
            "\n",
            "Step   4600: Ran 100 train steps in 50.07 secs\n",
            "Step   4600: train CrossEntropyLoss |  7.64130974\n",
            "Step   4600: eval  CrossEntropyLoss |  7.71639395\n",
            "Step   4600: eval          Accuracy |  0.07272727\n",
            "\n",
            "Step   4700: Ran 100 train steps in 50.20 secs\n",
            "Step   4700: train CrossEntropyLoss |  7.58348227\n",
            "Step   4700: eval  CrossEntropyLoss |  7.42627907\n",
            "Step   4700: eval          Accuracy |  0.11351351\n",
            "\n",
            "Step   4800: Ran 100 train steps in 49.98 secs\n",
            "Step   4800: train CrossEntropyLoss |  7.56830740\n",
            "Step   4800: eval  CrossEntropyLoss |  7.72519016\n",
            "Step   4800: eval          Accuracy |  0.09000000\n",
            "\n",
            "Step   4900: Ran 100 train steps in 50.23 secs\n",
            "Step   4900: train CrossEntropyLoss |  7.58420753\n",
            "Step   4900: eval  CrossEntropyLoss |  7.49004793\n",
            "Step   4900: eval          Accuracy |  0.08849557\n",
            "\n",
            "Step   5000: Ran 100 train steps in 50.31 secs\n",
            "Step   5000: train CrossEntropyLoss |  7.54016018\n",
            "Step   5000: eval  CrossEntropyLoss |  7.65888929\n",
            "Step   5000: eval          Accuracy |  0.08333334\n",
            "\n",
            "Step   5100: Ran 100 train steps in 50.27 secs\n",
            "Step   5100: train CrossEntropyLoss |  7.52970695\n",
            "Step   5100: eval  CrossEntropyLoss |  7.54789925\n",
            "Step   5100: eval          Accuracy |  0.09009010\n",
            "\n",
            "Step   5200: Ran 100 train steps in 50.14 secs\n",
            "Step   5200: train CrossEntropyLoss |  7.50680161\n",
            "Step   5200: eval  CrossEntropyLoss |  7.32234859\n",
            "Step   5200: eval          Accuracy |  0.07964602\n",
            "\n",
            "Step   5300: Ran 100 train steps in 50.03 secs\n",
            "Step   5300: train CrossEntropyLoss |  7.46652794\n",
            "Step   5300: eval  CrossEntropyLoss |  7.19228458\n",
            "Step   5300: eval          Accuracy |  0.09708738\n",
            "\n",
            "Step   5400: Ran 100 train steps in 50.10 secs\n",
            "Step   5400: train CrossEntropyLoss |  7.47071838\n",
            "Step   5400: eval  CrossEntropyLoss |  7.43519449\n",
            "Step   5400: eval          Accuracy |  0.08695652\n",
            "\n",
            "Step   5500: Ran 100 train steps in 50.20 secs\n",
            "Step   5500: train CrossEntropyLoss |  7.48320961\n",
            "Step   5500: eval  CrossEntropyLoss |  7.57916546\n",
            "Step   5500: eval          Accuracy |  0.08035715\n",
            "\n",
            "Step   5600: Ran 100 train steps in 49.96 secs\n",
            "Step   5600: train CrossEntropyLoss |  7.40854216\n",
            "Step   5600: eval  CrossEntropyLoss |  7.46986961\n",
            "Step   5600: eval          Accuracy |  0.09359606\n",
            "\n",
            "Step   5700: Ran 100 train steps in 50.09 secs\n",
            "Step   5700: train CrossEntropyLoss |  7.40883350\n",
            "Step   5700: eval  CrossEntropyLoss |  7.48021746\n",
            "Step   5700: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step   5800: Ran 100 train steps in 50.28 secs\n",
            "Step   5800: train CrossEntropyLoss |  7.43357897\n",
            "Step   5800: eval  CrossEntropyLoss |  7.38342285\n",
            "Step   5800: eval          Accuracy |  0.07920792\n",
            "\n",
            "Step   5900: Ran 100 train steps in 50.00 secs\n",
            "Step   5900: train CrossEntropyLoss |  7.41293478\n",
            "Step   5900: eval  CrossEntropyLoss |  7.21892977\n",
            "Step   5900: eval          Accuracy |  0.09278351\n",
            "\n",
            "Step   6000: Ran 100 train steps in 50.12 secs\n",
            "Step   6000: train CrossEntropyLoss |  7.39699221\n",
            "Step   6000: eval  CrossEntropyLoss |  7.49105835\n",
            "Step   6000: eval          Accuracy |  0.07920792\n",
            "\n",
            "Step   6100: Ran 100 train steps in 50.11 secs\n",
            "Step   6100: train CrossEntropyLoss |  7.38197374\n",
            "Step   6100: eval  CrossEntropyLoss |  7.84136724\n",
            "Step   6100: eval          Accuracy |  0.08333334\n",
            "\n",
            "Step   6200: Ran 100 train steps in 49.83 secs\n",
            "Step   6200: train CrossEntropyLoss |  7.35555792\n",
            "Step   6200: eval  CrossEntropyLoss |  7.34014702\n",
            "Step   6200: eval          Accuracy |  0.08181818\n",
            "\n",
            "Step   6300: Ran 100 train steps in 50.24 secs\n",
            "Step   6300: train CrossEntropyLoss |  7.32896662\n",
            "Step   6300: eval  CrossEntropyLoss |  7.38555479\n",
            "Step   6300: eval          Accuracy |  0.08585858\n",
            "\n",
            "Step   6400: Ran 100 train steps in 49.91 secs\n",
            "Step   6400: train CrossEntropyLoss |  7.31454229\n",
            "Step   6400: eval  CrossEntropyLoss |  7.40544081\n",
            "Step   6400: eval          Accuracy |  0.09615385\n",
            "\n",
            "Step   6500: Ran 100 train steps in 49.93 secs\n",
            "Step   6500: train CrossEntropyLoss |  7.31427574\n",
            "Step   6500: eval  CrossEntropyLoss |  6.98586369\n",
            "Step   6500: eval          Accuracy |  0.07920792\n",
            "\n",
            "Step   6600: Ran 100 train steps in 50.08 secs\n",
            "Step   6600: train CrossEntropyLoss |  7.33476162\n",
            "Step   6600: eval  CrossEntropyLoss |  7.25725508\n",
            "Step   6600: eval          Accuracy |  0.09326425\n",
            "\n",
            "Step   6700: Ran 100 train steps in 49.92 secs\n",
            "Step   6700: train CrossEntropyLoss |  7.30105209\n",
            "Step   6700: eval  CrossEntropyLoss |  7.32086802\n",
            "Step   6700: eval          Accuracy |  0.09615385\n",
            "\n",
            "Step   6800: Ran 100 train steps in 50.25 secs\n",
            "Step   6800: train CrossEntropyLoss |  7.28307581\n",
            "Step   6800: eval  CrossEntropyLoss |  7.43468332\n",
            "Step   6800: eval          Accuracy |  0.08653846\n",
            "\n",
            "Step   6900: Ran 100 train steps in 50.30 secs\n",
            "Step   6900: train CrossEntropyLoss |  7.31350613\n",
            "Step   6900: eval  CrossEntropyLoss |  7.26337481\n",
            "Step   6900: eval          Accuracy |  0.10204081\n",
            "\n",
            "Step   7000: Ran 100 train steps in 50.21 secs\n",
            "Step   7000: train CrossEntropyLoss |  7.27999306\n",
            "Step   7000: eval  CrossEntropyLoss |  7.47920990\n",
            "Step   7000: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step   7100: Ran 100 train steps in 50.16 secs\n",
            "Step   7100: train CrossEntropyLoss |  7.24382114\n",
            "Step   7100: eval  CrossEntropyLoss |  7.28454399\n",
            "Step   7100: eval          Accuracy |  0.09644670\n",
            "\n",
            "Step   7200: Ran 100 train steps in 49.98 secs\n",
            "Step   7200: train CrossEntropyLoss |  7.24481916\n",
            "Step   7200: eval  CrossEntropyLoss |  7.56390572\n",
            "Step   7200: eval          Accuracy |  0.08910891\n",
            "\n",
            "Step   7300: Ran 100 train steps in 50.28 secs\n",
            "Step   7300: train CrossEntropyLoss |  7.18404484\n",
            "Step   7300: eval  CrossEntropyLoss |  7.24070024\n",
            "Step   7300: eval          Accuracy |  0.08080808\n",
            "\n",
            "Step   7400: Ran 100 train steps in 50.41 secs\n",
            "Step   7400: train CrossEntropyLoss |  7.20534897\n",
            "Step   7400: eval  CrossEntropyLoss |  7.00565195\n",
            "Step   7400: eval          Accuracy |  0.09482758\n",
            "\n",
            "Step   7500: Ran 100 train steps in 50.93 secs\n",
            "Step   7500: train CrossEntropyLoss |  7.16313696\n",
            "Step   7500: eval  CrossEntropyLoss |  7.09054327\n",
            "Step   7500: eval          Accuracy |  0.07547170\n",
            "\n",
            "Step   7600: Ran 100 train steps in 50.60 secs\n",
            "Step   7600: train CrossEntropyLoss |  7.14196205\n",
            "Step   7600: eval  CrossEntropyLoss |  6.75712681\n",
            "Step   7600: eval          Accuracy |  0.13471504\n",
            "\n",
            "Step   7700: Ran 100 train steps in 50.53 secs\n",
            "Step   7700: train CrossEntropyLoss |  7.17603827\n",
            "Step   7700: eval  CrossEntropyLoss |  6.88154459\n",
            "Step   7700: eval          Accuracy |  0.12000000\n",
            "\n",
            "Step   7800: Ran 100 train steps in 50.52 secs\n",
            "Step   7800: train CrossEntropyLoss |  7.14208651\n",
            "Step   7800: eval  CrossEntropyLoss |  7.09695435\n",
            "Step   7800: eval          Accuracy |  0.10280374\n",
            "\n",
            "Step   7900: Ran 100 train steps in 50.70 secs\n",
            "Step   7900: train CrossEntropyLoss |  7.12308168\n",
            "Step   7900: eval  CrossEntropyLoss |  7.01826048\n",
            "Step   7900: eval          Accuracy |  0.10215054\n",
            "\n",
            "Step   8000: Ran 100 train steps in 50.60 secs\n",
            "Step   8000: train CrossEntropyLoss |  7.13587141\n",
            "Step   8000: eval  CrossEntropyLoss |  7.02863407\n",
            "Step   8000: eval          Accuracy |  0.10169491\n",
            "\n",
            "Step   8100: Ran 100 train steps in 50.70 secs\n",
            "Step   8100: train CrossEntropyLoss |  7.09420347\n",
            "Step   8100: eval  CrossEntropyLoss |  7.21812677\n",
            "Step   8100: eval          Accuracy |  0.08333334\n",
            "\n",
            "Step   8200: Ran 100 train steps in 50.74 secs\n",
            "Step   8200: train CrossEntropyLoss |  7.07500648\n",
            "Step   8200: eval  CrossEntropyLoss |  7.25758219\n",
            "Step   8200: eval          Accuracy |  0.08247423\n",
            "\n",
            "Step   8300: Ran 100 train steps in 50.69 secs\n",
            "Step   8300: train CrossEntropyLoss |  7.08791780\n",
            "Step   8300: eval  CrossEntropyLoss |  6.92950964\n",
            "Step   8300: eval          Accuracy |  0.10204081\n",
            "\n",
            "Step   8400: Ran 100 train steps in 50.37 secs\n",
            "Step   8400: train CrossEntropyLoss |  7.06563377\n",
            "Step   8400: eval  CrossEntropyLoss |  6.90335989\n",
            "Step   8400: eval          Accuracy |  0.08888888\n",
            "\n",
            "Step   8500: Ran 100 train steps in 50.36 secs\n",
            "Step   8500: train CrossEntropyLoss |  7.06926441\n",
            "Step   8500: eval  CrossEntropyLoss |  6.97782373\n",
            "Step   8500: eval          Accuracy |  0.12962963\n",
            "\n",
            "Step   8600: Ran 100 train steps in 50.10 secs\n",
            "Step   8600: train CrossEntropyLoss |  7.06682396\n",
            "Step   8600: eval  CrossEntropyLoss |  7.05326605\n",
            "Step   8600: eval          Accuracy |  0.10909091\n",
            "\n",
            "Step   8700: Ran 100 train steps in 50.40 secs\n",
            "Step   8700: train CrossEntropyLoss |  7.05302763\n",
            "Step   8700: eval  CrossEntropyLoss |  6.73311186\n",
            "Step   8700: eval          Accuracy |  0.09999999\n",
            "\n",
            "Step   8800: Ran 100 train steps in 50.47 secs\n",
            "Step   8800: train CrossEntropyLoss |  7.04687691\n",
            "Step   8800: eval  CrossEntropyLoss |  6.84893608\n",
            "Step   8800: eval          Accuracy |  0.09448819\n",
            "\n",
            "Step   8900: Ran 100 train steps in 50.41 secs\n",
            "Step   8900: train CrossEntropyLoss |  7.01268530\n",
            "Step   8900: eval  CrossEntropyLoss |  6.78324890\n",
            "Step   8900: eval          Accuracy |  0.09243698\n",
            "\n",
            "Step   9000: Ran 100 train steps in 50.40 secs\n",
            "Step   9000: train CrossEntropyLoss |  7.02156401\n",
            "Step   9000: eval  CrossEntropyLoss |  7.15859604\n",
            "Step   9000: eval          Accuracy |  0.10714287\n",
            "\n",
            "Step   9100: Ran 100 train steps in 50.11 secs\n",
            "Step   9100: train CrossEntropyLoss |  6.94178438\n",
            "Step   9100: eval  CrossEntropyLoss |  7.02688313\n",
            "Step   9100: eval          Accuracy |  0.12093023\n",
            "\n",
            "Step   9200: Ran 100 train steps in 50.16 secs\n",
            "Step   9200: train CrossEntropyLoss |  6.95488644\n",
            "Step   9200: eval  CrossEntropyLoss |  7.05229855\n",
            "Step   9200: eval          Accuracy |  0.12149532\n",
            "\n",
            "Step   9300: Ran 100 train steps in 50.40 secs\n",
            "Step   9300: train CrossEntropyLoss |  6.95789909\n",
            "Step   9300: eval  CrossEntropyLoss |  6.95966244\n",
            "Step   9300: eval          Accuracy |  0.08411215\n",
            "\n",
            "Step   9400: Ran 100 train steps in 50.19 secs\n",
            "Step   9400: train CrossEntropyLoss |  6.97016764\n",
            "Step   9400: eval  CrossEntropyLoss |  7.15905333\n",
            "Step   9400: eval          Accuracy |  0.11578948\n",
            "\n",
            "Step   9500: Ran 100 train steps in 50.19 secs\n",
            "Step   9500: train CrossEntropyLoss |  6.93134689\n",
            "Step   9500: eval  CrossEntropyLoss |  6.49445963\n",
            "Step   9500: eval          Accuracy |  0.14673914\n",
            "\n",
            "Step   9600: Ran 100 train steps in 50.05 secs\n",
            "Step   9600: train CrossEntropyLoss |  6.90981483\n",
            "Step   9600: eval  CrossEntropyLoss |  7.12522745\n",
            "Step   9600: eval          Accuracy |  0.09000000\n",
            "\n",
            "Step   9700: Ran 100 train steps in 50.23 secs\n",
            "Step   9700: train CrossEntropyLoss |  6.88021088\n",
            "Step   9700: eval  CrossEntropyLoss |  7.08778286\n",
            "Step   9700: eval          Accuracy |  0.10891089\n",
            "\n",
            "Step   9800: Ran 100 train steps in 50.25 secs\n",
            "Step   9800: train CrossEntropyLoss |  6.94774580\n",
            "Step   9800: eval  CrossEntropyLoss |  7.03793621\n",
            "Step   9800: eval          Accuracy |  0.07079646\n",
            "\n",
            "Step   9900: Ran 100 train steps in 50.19 secs\n",
            "Step   9900: train CrossEntropyLoss |  6.92012978\n",
            "Step   9900: eval  CrossEntropyLoss |  6.86000490\n",
            "Step   9900: eval          Accuracy |  0.10606061\n",
            "\n",
            "Step  10000: Ran 100 train steps in 50.33 secs\n",
            "Step  10000: train CrossEntropyLoss |  6.88648176\n",
            "Step  10000: eval  CrossEntropyLoss |  7.36187172\n",
            "Step  10000: eval          Accuracy |  0.11711712\n",
            "\n",
            "Step  10100: Ran 100 train steps in 50.19 secs\n",
            "Step  10100: train CrossEntropyLoss |  6.88179779\n",
            "Step  10100: eval  CrossEntropyLoss |  7.08646774\n",
            "Step  10100: eval          Accuracy |  0.12621360\n",
            "\n",
            "Step  10200: Ran 100 train steps in 50.14 secs\n",
            "Step  10200: train CrossEntropyLoss |  6.87674475\n",
            "Step  10200: eval  CrossEntropyLoss |  7.05170393\n",
            "Step  10200: eval          Accuracy |  0.10280374\n",
            "\n",
            "Step  10300: Ran 100 train steps in 50.24 secs\n",
            "Step  10300: train CrossEntropyLoss |  6.87051487\n",
            "Step  10300: eval  CrossEntropyLoss |  6.65751266\n",
            "Step  10300: eval          Accuracy |  0.10400000\n",
            "\n",
            "Step  10400: Ran 100 train steps in 50.19 secs\n",
            "Step  10400: train CrossEntropyLoss |  6.85890484\n",
            "Step  10400: eval  CrossEntropyLoss |  6.72728062\n",
            "Step  10400: eval          Accuracy |  0.11374408\n",
            "\n",
            "Step  10500: Ran 100 train steps in 50.18 secs\n",
            "Step  10500: train CrossEntropyLoss |  6.82617617\n",
            "Step  10500: eval  CrossEntropyLoss |  6.58705902\n",
            "Step  10500: eval          Accuracy |  0.11666667\n",
            "\n",
            "Step  10600: Ran 100 train steps in 50.11 secs\n",
            "Step  10600: train CrossEntropyLoss |  6.86266994\n",
            "Step  10600: eval  CrossEntropyLoss |  7.08990002\n",
            "Step  10600: eval          Accuracy |  0.09574468\n",
            "\n",
            "Step  10700: Ran 100 train steps in 50.06 secs\n",
            "Step  10700: train CrossEntropyLoss |  6.85329151\n",
            "Step  10700: eval  CrossEntropyLoss |  6.92573166\n",
            "Step  10700: eval          Accuracy |  0.11458334\n",
            "\n",
            "Step  10800: Ran 100 train steps in 50.12 secs\n",
            "Step  10800: train CrossEntropyLoss |  6.84396172\n",
            "Step  10800: eval  CrossEntropyLoss |  6.57640982\n",
            "Step  10800: eval          Accuracy |  0.12834224\n",
            "\n",
            "Step  10900: Ran 100 train steps in 50.03 secs\n",
            "Step  10900: train CrossEntropyLoss |  6.78897095\n",
            "Step  10900: eval  CrossEntropyLoss |  6.99727964\n",
            "Step  10900: eval          Accuracy |  0.08547009\n",
            "\n",
            "Step  11000: Ran 100 train steps in 50.06 secs\n",
            "Step  11000: train CrossEntropyLoss |  6.80165148\n",
            "Step  11000: eval  CrossEntropyLoss |  6.68704462\n",
            "Step  11000: eval          Accuracy |  0.09090909\n",
            "\n",
            "Step  11100: Ran 100 train steps in 50.22 secs\n",
            "Step  11100: train CrossEntropyLoss |  6.80731630\n",
            "Step  11100: eval  CrossEntropyLoss |  6.72236538\n",
            "Step  11100: eval          Accuracy |  0.11111111\n",
            "\n",
            "Step  11200: Ran 100 train steps in 50.17 secs\n",
            "Step  11200: train CrossEntropyLoss |  6.78881264\n",
            "Step  11200: eval  CrossEntropyLoss |  6.86424828\n",
            "Step  11200: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step  11300: Ran 100 train steps in 50.02 secs\n",
            "Step  11300: train CrossEntropyLoss |  6.75843573\n",
            "Step  11300: eval  CrossEntropyLoss |  6.30947399\n",
            "Step  11300: eval          Accuracy |  0.15625000\n",
            "\n",
            "Step  11400: Ran 100 train steps in 50.26 secs\n",
            "Step  11400: train CrossEntropyLoss |  6.78108978\n",
            "Step  11400: eval  CrossEntropyLoss |  6.58474350\n",
            "Step  11400: eval          Accuracy |  0.12213740\n",
            "\n",
            "Step  11500: Ran 100 train steps in 49.99 secs\n",
            "Step  11500: train CrossEntropyLoss |  6.79948425\n",
            "Step  11500: eval  CrossEntropyLoss |  6.75284004\n",
            "Step  11500: eval          Accuracy |  0.14634146\n",
            "\n",
            "Step  11600: Ran 100 train steps in 50.39 secs\n",
            "Step  11600: train CrossEntropyLoss |  6.73690796\n",
            "Step  11600: eval  CrossEntropyLoss |  6.49929571\n",
            "Step  11600: eval          Accuracy |  0.14009662\n",
            "\n",
            "Step  11700: Ran 100 train steps in 50.12 secs\n",
            "Step  11700: train CrossEntropyLoss |  6.74611092\n",
            "Step  11700: eval  CrossEntropyLoss |  6.72702360\n",
            "Step  11700: eval          Accuracy |  0.12727273\n",
            "\n",
            "Step  11800: Ran 100 train steps in 50.27 secs\n",
            "Step  11800: train CrossEntropyLoss |  6.75948334\n",
            "Step  11800: eval  CrossEntropyLoss |  6.70324564\n",
            "Step  11800: eval          Accuracy |  0.14851485\n",
            "\n",
            "Step  11900: Ran 100 train steps in 50.15 secs\n",
            "Step  11900: train CrossEntropyLoss |  6.71754742\n",
            "Step  11900: eval  CrossEntropyLoss |  6.65569210\n",
            "Step  11900: eval          Accuracy |  0.08771930\n",
            "\n",
            "Step  12000: Ran 100 train steps in 50.16 secs\n",
            "Step  12000: train CrossEntropyLoss |  6.67940283\n",
            "Step  12000: eval  CrossEntropyLoss |  6.96265268\n",
            "Step  12000: eval          Accuracy |  0.12254903\n",
            "\n",
            "Step  12100: Ran 100 train steps in 50.16 secs\n",
            "Step  12100: train CrossEntropyLoss |  6.72163296\n",
            "Step  12100: eval  CrossEntropyLoss |  6.69175768\n",
            "Step  12100: eval          Accuracy |  0.06956521\n",
            "\n",
            "Step  12200: Ran 100 train steps in 50.04 secs\n",
            "Step  12200: train CrossEntropyLoss |  6.70630312\n",
            "Step  12200: eval  CrossEntropyLoss |  6.53829193\n",
            "Step  12200: eval          Accuracy |  0.11818182\n",
            "\n",
            "Step  12300: Ran 100 train steps in 50.11 secs\n",
            "Step  12300: train CrossEntropyLoss |  6.69996691\n",
            "Step  12300: eval  CrossEntropyLoss |  6.56955957\n",
            "Step  12300: eval          Accuracy |  0.12643678\n",
            "\n",
            "Step  12400: Ran 100 train steps in 50.34 secs\n",
            "Step  12400: train CrossEntropyLoss |  6.69337320\n",
            "Step  12400: eval  CrossEntropyLoss |  6.25981474\n",
            "Step  12400: eval          Accuracy |  0.14912280\n",
            "\n",
            "Step  12500: Ran 100 train steps in 50.26 secs\n",
            "Step  12500: train CrossEntropyLoss |  6.65915251\n",
            "Step  12500: eval  CrossEntropyLoss |  6.93258095\n",
            "Step  12500: eval          Accuracy |  0.12244898\n",
            "\n",
            "Step  12600: Ran 100 train steps in 49.86 secs\n",
            "Step  12600: train CrossEntropyLoss |  6.62571526\n",
            "Step  12600: eval  CrossEntropyLoss |  6.69154024\n",
            "Step  12600: eval          Accuracy |  0.09223301\n",
            "\n",
            "Step  12700: Ran 100 train steps in 50.12 secs\n",
            "Step  12700: train CrossEntropyLoss |  6.66667700\n",
            "Step  12700: eval  CrossEntropyLoss |  6.61254311\n",
            "Step  12700: eval          Accuracy |  0.12380953\n",
            "\n",
            "Step  12800: Ran 100 train steps in 50.21 secs\n",
            "Step  12800: train CrossEntropyLoss |  6.62299347\n",
            "Step  12800: eval  CrossEntropyLoss |  6.51750183\n",
            "Step  12800: eval          Accuracy |  0.14285713\n",
            "\n",
            "Step  12900: Ran 100 train steps in 49.93 secs\n",
            "Step  12900: train CrossEntropyLoss |  6.63644600\n",
            "Step  12900: eval  CrossEntropyLoss |  6.59166908\n",
            "Step  12900: eval          Accuracy |  0.09278351\n",
            "\n",
            "Step  13000: Ran 100 train steps in 50.20 secs\n",
            "Step  13000: train CrossEntropyLoss |  6.63459444\n",
            "Step  13000: eval  CrossEntropyLoss |  6.62533522\n",
            "Step  13000: eval          Accuracy |  0.10824743\n",
            "\n",
            "Step  13100: Ran 100 train steps in 49.99 secs\n",
            "Step  13100: train CrossEntropyLoss |  6.60812855\n",
            "Step  13100: eval  CrossEntropyLoss |  6.87918377\n",
            "Step  13100: eval          Accuracy |  0.10204081\n",
            "\n",
            "Step  13200: Ran 100 train steps in 50.35 secs\n",
            "Step  13200: train CrossEntropyLoss |  6.61632299\n",
            "Step  13200: eval  CrossEntropyLoss |  6.68288231\n",
            "Step  13200: eval          Accuracy |  0.11428572\n",
            "\n",
            "Step  13300: Ran 100 train steps in 50.00 secs\n",
            "Step  13300: train CrossEntropyLoss |  6.63238192\n",
            "Step  13300: eval  CrossEntropyLoss |  6.40170002\n",
            "Step  13300: eval          Accuracy |  0.12745099\n",
            "\n",
            "Step  13400: Ran 100 train steps in 49.99 secs\n",
            "Step  13400: train CrossEntropyLoss |  6.59203005\n",
            "Step  13400: eval  CrossEntropyLoss |  6.49075747\n",
            "Step  13400: eval          Accuracy |  0.14492753\n",
            "\n",
            "Step  13500: Ran 100 train steps in 50.06 secs\n",
            "Step  13500: train CrossEntropyLoss |  6.54644156\n",
            "Step  13500: eval  CrossEntropyLoss |  6.53690529\n",
            "Step  13500: eval          Accuracy |  0.13461539\n",
            "\n",
            "Step  13600: Ran 100 train steps in 50.34 secs\n",
            "Step  13600: train CrossEntropyLoss |  6.53754950\n",
            "Step  13600: eval  CrossEntropyLoss |  6.33124113\n",
            "Step  13600: eval          Accuracy |  0.12280702\n",
            "\n",
            "Step  13700: Ran 100 train steps in 50.12 secs\n",
            "Step  13700: train CrossEntropyLoss |  6.55792475\n",
            "Step  13700: eval  CrossEntropyLoss |  6.42240381\n",
            "Step  13700: eval          Accuracy |  0.17241380\n",
            "\n",
            "Step  13800: Ran 100 train steps in 50.03 secs\n",
            "Step  13800: train CrossEntropyLoss |  6.58345222\n",
            "Step  13800: eval  CrossEntropyLoss |  6.41946888\n",
            "Step  13800: eval          Accuracy |  0.13065326\n",
            "\n",
            "Step  13900: Ran 100 train steps in 50.18 secs\n",
            "Step  13900: train CrossEntropyLoss |  6.59171915\n",
            "Step  13900: eval  CrossEntropyLoss |  6.83978271\n",
            "Step  13900: eval          Accuracy |  0.12871286\n",
            "\n",
            "Step  14000: Ran 100 train steps in 50.32 secs\n",
            "Step  14000: train CrossEntropyLoss |  6.58621264\n",
            "Step  14000: eval  CrossEntropyLoss |  6.09040594\n",
            "Step  14000: eval          Accuracy |  0.16822429\n",
            "\n",
            "Step  14100: Ran 100 train steps in 50.26 secs\n",
            "Step  14100: train CrossEntropyLoss |  6.54224777\n",
            "Step  14100: eval  CrossEntropyLoss |  6.33912039\n",
            "Step  14100: eval          Accuracy |  0.16393442\n",
            "\n",
            "Step  14200: Ran 100 train steps in 50.26 secs\n",
            "Step  14200: train CrossEntropyLoss |  6.48170280\n",
            "Step  14200: eval  CrossEntropyLoss |  6.94049120\n",
            "Step  14200: eval          Accuracy |  0.07766990\n",
            "\n",
            "Step  14300: Ran 100 train steps in 50.14 secs\n",
            "Step  14300: train CrossEntropyLoss |  6.51939583\n",
            "Step  14300: eval  CrossEntropyLoss |  6.33812428\n",
            "Step  14300: eval          Accuracy |  0.11764707\n",
            "\n",
            "Step  14400: Ran 100 train steps in 50.08 secs\n",
            "Step  14400: train CrossEntropyLoss |  6.53173876\n",
            "Step  14400: eval  CrossEntropyLoss |  6.58470345\n",
            "Step  14400: eval          Accuracy |  0.13636363\n",
            "\n",
            "Step  14500: Ran 100 train steps in 50.22 secs\n",
            "Step  14500: train CrossEntropyLoss |  6.46768713\n",
            "Step  14500: eval  CrossEntropyLoss |  6.55240202\n",
            "Step  14500: eval          Accuracy |  0.12121212\n",
            "\n",
            "Step  14600: Ran 100 train steps in 50.34 secs\n",
            "Step  14600: train CrossEntropyLoss |  6.52629137\n",
            "Step  14600: eval  CrossEntropyLoss |  6.51131964\n",
            "Step  14600: eval          Accuracy |  0.10714287\n",
            "\n",
            "Step  14700: Ran 100 train steps in 50.12 secs\n",
            "Step  14700: train CrossEntropyLoss |  6.50295448\n",
            "Step  14700: eval  CrossEntropyLoss |  6.41712713\n",
            "Step  14700: eval          Accuracy |  0.14814815\n",
            "\n",
            "Step  14800: Ran 100 train steps in 50.36 secs\n",
            "Step  14800: train CrossEntropyLoss |  6.47413874\n",
            "Step  14800: eval  CrossEntropyLoss |  6.01517057\n",
            "Step  14800: eval          Accuracy |  0.15517241\n",
            "\n",
            "Step  14900: Ran 100 train steps in 50.13 secs\n",
            "Step  14900: train CrossEntropyLoss |  6.47771358\n",
            "Step  14900: eval  CrossEntropyLoss |  6.35193062\n",
            "Step  14900: eval          Accuracy |  0.15740740\n",
            "\n",
            "Step  15000: Ran 100 train steps in 50.19 secs\n",
            "Step  15000: train CrossEntropyLoss |  6.44488382\n",
            "Step  15000: eval  CrossEntropyLoss |  6.69237137\n",
            "Step  15000: eval          Accuracy |  0.13461539\n",
            "\n",
            "Step  15100: Ran 100 train steps in 50.97 secs\n",
            "Step  15100: train CrossEntropyLoss |  6.47282314\n",
            "Step  15100: eval  CrossEntropyLoss |  6.53956318\n",
            "Step  15100: eval          Accuracy |  0.10994764\n",
            "\n",
            "Step  15200: Ran 100 train steps in 50.66 secs\n",
            "Step  15200: train CrossEntropyLoss |  6.46925545\n",
            "Step  15200: eval  CrossEntropyLoss |  6.27794409\n",
            "Step  15200: eval          Accuracy |  0.16129032\n",
            "\n",
            "Step  15300: Ran 100 train steps in 50.51 secs\n",
            "Step  15300: train CrossEntropyLoss |  6.45448589\n",
            "Step  15300: eval  CrossEntropyLoss |  6.41788483\n",
            "Step  15300: eval          Accuracy |  0.10185185\n",
            "\n",
            "Step  15400: Ran 100 train steps in 50.34 secs\n",
            "Step  15400: train CrossEntropyLoss |  6.41072512\n",
            "Step  15400: eval  CrossEntropyLoss |  6.44436789\n",
            "Step  15400: eval          Accuracy |  0.14583334\n",
            "\n",
            "Step  15500: Ran 100 train steps in 50.39 secs\n",
            "Step  15500: train CrossEntropyLoss |  6.50067902\n",
            "Step  15500: eval  CrossEntropyLoss |  6.13147783\n",
            "Step  15500: eval          Accuracy |  0.11616161\n",
            "\n",
            "Step  15600: Ran 100 train steps in 50.39 secs\n",
            "Step  15600: train CrossEntropyLoss |  6.40824747\n",
            "Step  15600: eval  CrossEntropyLoss |  6.75267506\n",
            "Step  15600: eval          Accuracy |  0.16216217\n",
            "\n",
            "Step  15700: Ran 100 train steps in 50.18 secs\n",
            "Step  15700: train CrossEntropyLoss |  6.44539976\n",
            "Step  15700: eval  CrossEntropyLoss |  6.97916985\n",
            "Step  15700: eval          Accuracy |  0.12149532\n",
            "\n",
            "Step  15800: Ran 100 train steps in 50.25 secs\n",
            "Step  15800: train CrossEntropyLoss |  6.41733503\n",
            "Step  15800: eval  CrossEntropyLoss |  6.38032007\n",
            "Step  15800: eval          Accuracy |  0.14414415\n",
            "\n",
            "Step  15900: Ran 100 train steps in 50.14 secs\n",
            "Step  15900: train CrossEntropyLoss |  6.42097330\n",
            "Step  15900: eval  CrossEntropyLoss |  6.09558868\n",
            "Step  15900: eval          Accuracy |  0.13872832\n",
            "\n",
            "Step  16000: Ran 100 train steps in 50.07 secs\n",
            "Step  16000: train CrossEntropyLoss |  6.39058161\n",
            "Step  16000: eval  CrossEntropyLoss |  6.42583609\n",
            "Step  16000: eval          Accuracy |  0.14285713\n",
            "\n",
            "Step  16100: Ran 100 train steps in 50.28 secs\n",
            "Step  16100: train CrossEntropyLoss |  6.43139315\n",
            "Step  16100: eval  CrossEntropyLoss |  6.36326218\n",
            "Step  16100: eval          Accuracy |  0.13445379\n",
            "\n",
            "Step  16200: Ran 100 train steps in 50.09 secs\n",
            "Step  16200: train CrossEntropyLoss |  6.40868092\n",
            "Step  16200: eval  CrossEntropyLoss |  6.44632483\n",
            "Step  16200: eval          Accuracy |  0.10344827\n",
            "\n",
            "Step  16300: Ran 100 train steps in 49.93 secs\n",
            "Step  16300: train CrossEntropyLoss |  6.38988590\n",
            "Step  16300: eval  CrossEntropyLoss |  6.31460476\n",
            "Step  16300: eval          Accuracy |  0.15957446\n",
            "\n",
            "Step  16400: Ran 100 train steps in 50.34 secs\n",
            "Step  16400: train CrossEntropyLoss |  6.37703943\n",
            "Step  16400: eval  CrossEntropyLoss |  6.24740219\n",
            "Step  16400: eval          Accuracy |  0.13000000\n",
            "\n",
            "Step  16500: Ran 100 train steps in 50.02 secs\n",
            "Step  16500: train CrossEntropyLoss |  6.36889029\n",
            "Step  16500: eval  CrossEntropyLoss |  6.04870558\n",
            "Step  16500: eval          Accuracy |  0.15841584\n",
            "\n",
            "Step  16600: Ran 100 train steps in 50.15 secs\n",
            "Step  16600: train CrossEntropyLoss |  6.38171434\n",
            "Step  16600: eval  CrossEntropyLoss |  6.76238441\n",
            "Step  16600: eval          Accuracy |  0.10377359\n",
            "\n",
            "Step  16700: Ran 100 train steps in 50.18 secs\n",
            "Step  16700: train CrossEntropyLoss |  6.39598751\n",
            "Step  16700: eval  CrossEntropyLoss |  5.87681484\n",
            "Step  16700: eval          Accuracy |  0.18691587\n",
            "\n",
            "Step  16800: Ran 100 train steps in 50.11 secs\n",
            "Step  16800: train CrossEntropyLoss |  6.34097338\n",
            "Step  16800: eval  CrossEntropyLoss |  6.49560070\n",
            "Step  16800: eval          Accuracy |  0.09345794\n",
            "\n",
            "Step  16900: Ran 100 train steps in 50.08 secs\n",
            "Step  16900: train CrossEntropyLoss |  6.38472843\n",
            "Step  16900: eval  CrossEntropyLoss |  6.80179977\n",
            "Step  16900: eval          Accuracy |  0.12371135\n",
            "\n",
            "Step  17000: Ran 100 train steps in 50.20 secs\n",
            "Step  17000: train CrossEntropyLoss |  6.32712460\n",
            "Step  17000: eval  CrossEntropyLoss |  6.34487820\n",
            "Step  17000: eval          Accuracy |  0.15075377\n",
            "\n",
            "Step  17100: Ran 100 train steps in 50.08 secs\n",
            "Step  17100: train CrossEntropyLoss |  6.36742830\n",
            "Step  17100: eval  CrossEntropyLoss |  6.60792875\n",
            "Step  17100: eval          Accuracy |  0.10483871\n",
            "\n",
            "Step  17200: Ran 100 train steps in 50.09 secs\n",
            "Step  17200: train CrossEntropyLoss |  6.31959009\n",
            "Step  17200: eval  CrossEntropyLoss |  6.12737179\n",
            "Step  17200: eval          Accuracy |  0.14492753\n",
            "\n",
            "Step  17300: Ran 100 train steps in 50.05 secs\n",
            "Step  17300: train CrossEntropyLoss |  6.34802866\n",
            "Step  17300: eval  CrossEntropyLoss |  6.61256361\n",
            "Step  17300: eval          Accuracy |  0.12612613\n",
            "\n",
            "Step  17400: Ran 100 train steps in 50.16 secs\n",
            "Step  17400: train CrossEntropyLoss |  6.33759308\n",
            "Step  17400: eval  CrossEntropyLoss |  6.50713348\n",
            "Step  17400: eval          Accuracy |  0.11926605\n",
            "\n",
            "Step  17500: Ran 100 train steps in 50.05 secs\n",
            "Step  17500: train CrossEntropyLoss |  6.31645250\n",
            "Step  17500: eval  CrossEntropyLoss |  6.80343819\n",
            "Step  17500: eval          Accuracy |  0.09278351\n",
            "\n",
            "Step  17600: Ran 100 train steps in 50.08 secs\n",
            "Step  17600: train CrossEntropyLoss |  6.32909632\n",
            "Step  17600: eval  CrossEntropyLoss |  6.46388626\n",
            "Step  17600: eval          Accuracy |  0.11650486\n",
            "\n",
            "Step  17700: Ran 100 train steps in 50.12 secs\n",
            "Step  17700: train CrossEntropyLoss |  6.31676912\n",
            "Step  17700: eval  CrossEntropyLoss |  5.83994484\n",
            "Step  17700: eval          Accuracy |  0.15000001\n",
            "\n",
            "Step  17800: Ran 100 train steps in 50.12 secs\n",
            "Step  17800: train CrossEntropyLoss |  6.30875158\n",
            "Step  17800: eval  CrossEntropyLoss |  6.64190674\n",
            "Step  17800: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step  17900: Ran 100 train steps in 49.91 secs\n",
            "Step  17900: train CrossEntropyLoss |  6.27232265\n",
            "Step  17900: eval  CrossEntropyLoss |  6.57887745\n",
            "Step  17900: eval          Accuracy |  0.13861386\n",
            "\n",
            "Step  18000: Ran 100 train steps in 50.06 secs\n",
            "Step  18000: train CrossEntropyLoss |  6.32781315\n",
            "Step  18000: eval  CrossEntropyLoss |  6.55383444\n",
            "Step  18000: eval          Accuracy |  0.12500000\n",
            "\n",
            "Step  18100: Ran 100 train steps in 49.72 secs\n",
            "Step  18100: train CrossEntropyLoss |  6.28484058\n",
            "Step  18100: eval  CrossEntropyLoss |  6.46541595\n",
            "Step  18100: eval          Accuracy |  0.13170731\n",
            "\n",
            "Step  18200: Ran 100 train steps in 49.80 secs\n",
            "Step  18200: train CrossEntropyLoss |  6.27226162\n",
            "Step  18200: eval  CrossEntropyLoss |  6.29990196\n",
            "Step  18200: eval          Accuracy |  0.11702127\n",
            "\n",
            "Step  18300: Ran 100 train steps in 49.70 secs\n",
            "Step  18300: train CrossEntropyLoss |  6.27534914\n",
            "Step  18300: eval  CrossEntropyLoss |  6.01379728\n",
            "Step  18300: eval          Accuracy |  0.18461539\n",
            "\n",
            "Step  18400: Ran 100 train steps in 49.84 secs\n",
            "Step  18400: train CrossEntropyLoss |  6.28085518\n",
            "Step  18400: eval  CrossEntropyLoss |  6.15342283\n",
            "Step  18400: eval          Accuracy |  0.14141414\n",
            "\n",
            "Step  18500: Ran 100 train steps in 49.83 secs\n",
            "Step  18500: train CrossEntropyLoss |  6.25526333\n",
            "Step  18500: eval  CrossEntropyLoss |  6.19679594\n",
            "Step  18500: eval          Accuracy |  0.13114755\n",
            "\n",
            "Step  18600: Ran 100 train steps in 49.81 secs\n",
            "Step  18600: train CrossEntropyLoss |  6.26577997\n",
            "Step  18600: eval  CrossEntropyLoss |  6.07865429\n",
            "Step  18600: eval          Accuracy |  0.16504854\n",
            "\n",
            "Step  18700: Ran 100 train steps in 49.72 secs\n",
            "Step  18700: train CrossEntropyLoss |  6.25280523\n",
            "Step  18700: eval  CrossEntropyLoss |  6.34764004\n",
            "Step  18700: eval          Accuracy |  0.11304347\n",
            "\n",
            "Step  18800: Ran 100 train steps in 49.81 secs\n",
            "Step  18800: train CrossEntropyLoss |  6.24185038\n",
            "Step  18800: eval  CrossEntropyLoss |  6.11367464\n",
            "Step  18800: eval          Accuracy |  0.17142858\n",
            "\n",
            "Step  18900: Ran 100 train steps in 49.80 secs\n",
            "Step  18900: train CrossEntropyLoss |  6.26990271\n",
            "Step  18900: eval  CrossEntropyLoss |  6.68312931\n",
            "Step  18900: eval          Accuracy |  0.15887851\n",
            "\n",
            "Step  19000: Ran 100 train steps in 49.95 secs\n",
            "Step  19000: train CrossEntropyLoss |  6.19684458\n",
            "Step  19000: eval  CrossEntropyLoss |  5.78214455\n",
            "Step  19000: eval          Accuracy |  0.14655173\n",
            "\n",
            "Step  19100: Ran 100 train steps in 49.79 secs\n",
            "Step  19100: train CrossEntropyLoss |  6.23431206\n",
            "Step  19100: eval  CrossEntropyLoss |  6.15469408\n",
            "Step  19100: eval          Accuracy |  0.12886599\n",
            "\n",
            "Step  19200: Ran 100 train steps in 49.94 secs\n",
            "Step  19200: train CrossEntropyLoss |  6.23459959\n",
            "Step  19200: eval  CrossEntropyLoss |  6.36301994\n",
            "Step  19200: eval          Accuracy |  0.11214953\n",
            "\n",
            "Step  19300: Ran 100 train steps in 49.89 secs\n",
            "Step  19300: train CrossEntropyLoss |  6.26486874\n",
            "Step  19300: eval  CrossEntropyLoss |  6.48112202\n",
            "Step  19300: eval          Accuracy |  0.15178572\n",
            "\n",
            "Step  19400: Ran 100 train steps in 49.79 secs\n",
            "Step  19400: train CrossEntropyLoss |  6.24755192\n",
            "Step  19400: eval  CrossEntropyLoss |  6.02087164\n",
            "Step  19400: eval          Accuracy |  0.15841584\n",
            "\n",
            "Step  19500: Ran 100 train steps in 49.92 secs\n",
            "Step  19500: train CrossEntropyLoss |  6.21984625\n",
            "Step  19500: eval  CrossEntropyLoss |  6.14580822\n",
            "Step  19500: eval          Accuracy |  0.18750000\n",
            "\n",
            "Step  19600: Ran 100 train steps in 49.98 secs\n",
            "Step  19600: train CrossEntropyLoss |  6.22368479\n",
            "Step  19600: eval  CrossEntropyLoss |  6.16593981\n",
            "Step  19600: eval          Accuracy |  0.13846155\n",
            "\n",
            "Step  19700: Ran 100 train steps in 50.45 secs\n",
            "Step  19700: train CrossEntropyLoss |  6.21846962\n",
            "Step  19700: eval  CrossEntropyLoss |  6.19036865\n",
            "Step  19700: eval          Accuracy |  0.14406779\n",
            "\n",
            "Step  19800: Ran 100 train steps in 49.89 secs\n",
            "Step  19800: train CrossEntropyLoss |  6.22194147\n",
            "Step  19800: eval  CrossEntropyLoss |  5.84591103\n",
            "Step  19800: eval          Accuracy |  0.16990292\n",
            "\n",
            "Step  19900: Ran 100 train steps in 49.83 secs\n",
            "Step  19900: train CrossEntropyLoss |  6.23195362\n",
            "Step  19900: eval  CrossEntropyLoss |  5.98723745\n",
            "Step  19900: eval          Accuracy |  0.15730338\n",
            "\n",
            "Step  20000: Ran 100 train steps in 49.91 secs\n",
            "Step  20000: train CrossEntropyLoss |  6.21828222\n",
            "Step  20000: eval  CrossEntropyLoss |  6.45892525\n",
            "Step  20000: eval          Accuracy |  0.13559322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "# !rsync -a /content/drive/MyDrive/model2/ ~/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dyYwnk_1i56"
      },
      "source": [
        "# copy the model to Google Drive\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPWbvRXFyAC8"
      },
      "source": [
        "# sync Google Drive dir with the train dir\r\n",
        "!rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e771de8b-9d1d-4f07-9c75-6a608bbe91c0"
      },
      "source": [
        "train_text_pairs[0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('свидетелям обвинения по делу об убийстве оппозиционера бориса немцова поступали угрозы во время разбирательства в московском окружном военном суде. об этом агентству тасс рассказала прокурор по уголовному делу мария семененко. «да, было. по крайней мере, нам достоверно известно о нескольких таких случаях в отношении свидетелей», — отметила она. семененко не стала отвечать на вопрос, находится ли кто-то из свидетелей под государственной защитой. по ее словам, такая информация является закрытой. в четверг, 13 июля, суд приговорил заура дадаева, признанного виновным в убийстве политика, к 20 годам лишения свободы в колонии строгого режима. помимо дадаева, по делу проходят темирлан эскерханов, а также братья анзор и шадид губашевы и хамзат бахаев. соучастники преступления получили сроки от 11 до 19 лет. немцова застрелили на большом москворецком мосту в центре столицы 27 февраля 2015 года. по версии следствия, руслан мухудинов, предполагаемый организатор преступления, предложил исполнителям 15 миллионов рублей за убийство, он объявлен в международный розыск. киллером следствие и суд сочли дадаева.',\n",
              " 'прокурор рассказала про угрозы свидетелям по\\xa0делу об\\xa0убийстве немцова')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYS0jHDJkAd5",
        "outputId": "166f1956-081e-46fb-bc93-e59e6c884b33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "eval_text_pairs[1]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('московский «локомотив» сыграл вничью с тульским «арсеналом» в матче 9-го тура российской футбольной премьер-лиги (рфпл). об этом в субботу, 1 октября, сообщается на сайте турнира. встреча, прошедшая на стадионе «локомотив» в москве, завершилась со счетом 1:1. гости вышли вперед на 44-й минуте, отличился фелицио браун форбс. железнодорожники ушли от поражения на последней минуте матча — александр самедов реализовал пенальти на ведране чорлуке. железнодорожники набрали  8-е очко и остались на 13-м месте в турнирной таблице. «арсенал» имеет на одно очко меньше и располагается строчкой ниже. подопечные юрия семина прервали серию из трех поражений в рфпл подряд. ранее «локомотив» последовательно уступал «спартаку», «уфе» и «ростову» с одинаковым счетом 0:1. 29 сентября на встрече с болельщиками столичного клуба семин извинился перед поклонниками за неудовлетворительный результат команды в последнее время. семин был назначен главным тренером «локомотива» 26 августа. с тех пор клуб одержал одну победу и уступил трижды при одной ничьей.',\n",
              " '«локомотив» сыграл вничью с «арсеналом» и\\xa0прервал серию из\\xa0трех поражений кряду')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU9OvGMPiVLR"
      },
      "source": [
        "# fixed input for comparison purposes\r\n",
        "train_input = \"свидетелям обвинения по делу об убийстве оппозиционера бориса немцова поступали угрозы во время разбирательства в московском окружном военном суде. об этом агентству тасс рассказала прокурор по уголовному делу мария семененко. «да, было. по крайней мере, нам достоверно известно о нескольких таких случаях в отношении свидетелей», — отметила она. семененко не стала отвечать на вопрос, находится ли кто-то из свидетелей под государственной защитой. по ее словам, такая информация является закрытой. в четверг, 13 июля, суд приговорил заура дадаева, признанного виновным в убийстве политика, к 20 годам лишения свободы в колонии строгого режима. помимо дадаева, по делу проходят темирлан эскерханов, а также братья анзор и шадид губашевы и хамзат бахаев. соучастники преступления получили сроки от 11 до 19 лет. немцова застрелили на большом москворецком мосту в центре столицы 27 февраля 2015 года. по версии следствия, руслан мухудинов, предполагаемый организатор преступления, предложил исполнителям 15 миллионов рублей за убийство, он объявлен в международный розыск. киллером следствие и суд сочли дадаева.\"\r\n",
        "eval_input = \"московский «локомотив» сыграл вничью с тульским «арсеналом» в матче 9-го тура российской футбольной премьер-лиги (рфпл). об этом в субботу, 1 октября, сообщается на сайте турнира. встреча, прошедшая на стадионе «локомотив» в москве, завершилась со счетом 1:1. гости вышли вперед на 44-й минуте, отличился фелицио браун форбс. железнодорожники ушли от поражения на последней минуте матча — александр самедов реализовал пенальти на ведране чорлуке. железнодорожники набрали  8-е очко и остались на 13-м месте в турнирной таблице. «арсенал» имеет на одно очко меньше и располагается строчкой ниже. подопечные юрия семина прервали серию из трех поражений в рфпл подряд. ранее «локомотив» последовательно уступал «спартаку», «уфе» и «ростову» с одинаковым счетом 0:1. 29 сентября на встрече с болельщиками столичного клуба семин извинился перед поклонниками за неудовлетворительный результат команды в последнее время. семин был назначен главным тренером «локомотива» 26 августа. с тех пор клуб одержал одну победу и уступил трижды при одной ничьей.\""
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV-2j7AKT1pT",
        "outputId": "b5d2ca85-14a2-4f8c-dd88-854c90361fa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('прокурор рассказала про угрозы свидетелям по\\xa0делу об\\xa0убийстве немцова')\r\n",
        "print('')\r\n",
        "print(wrapper.fill(train_input))\r\n",
        "print('')\r\n",
        "print('«локомотив» сыграл вничью с «арсеналом» и\\xa0прервал серию из\\xa0трех поражений кряду')\r\n",
        "print('')\r\n",
        "print(wrapper.fill(eval_input))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "прокурор рассказала про угрозы свидетелям по делу об убийстве немцова\n",
            "\n",
            "свидетелям обвинения по делу об убийстве оппозиционера бориса немцова\n",
            "поступали угрозы во время разбирательства в московском окружном\n",
            "военном суде. об этом агентству тасс рассказала прокурор по уголовному\n",
            "делу мария семененко. «да, было. по крайней мере, нам достоверно\n",
            "известно о нескольких таких случаях в отношении свидетелей», —\n",
            "отметила она. семененко не стала отвечать на вопрос, находится ли кто-\n",
            "то из свидетелей под государственной защитой. по ее словам, такая\n",
            "информация является закрытой. в четверг, 13 июля, суд приговорил заура\n",
            "дадаева, признанного виновным в убийстве политика, к 20 годам лишения\n",
            "свободы в колонии строгого режима. помимо дадаева, по делу проходят\n",
            "темирлан эскерханов, а также братья анзор и шадид губашевы и хамзат\n",
            "бахаев. соучастники преступления получили сроки от 11 до 19 лет.\n",
            "немцова застрелили на большом москворецком мосту в центре столицы 27\n",
            "февраля 2015 года. по версии следствия, руслан мухудинов,\n",
            "предполагаемый организатор преступления, предложил исполнителям 15\n",
            "миллионов рублей за убийство, он объявлен в международный розыск.\n",
            "киллером следствие и суд сочли дадаева.\n",
            "\n",
            "«локомотив» сыграл вничью с «арсеналом» и прервал серию из трех поражений кряду\n",
            "\n",
            "московский «локомотив» сыграл вничью с тульским «арсеналом» в матче\n",
            "9-го тура российской футбольной премьер-лиги (рфпл). об этом в\n",
            "субботу, 1 октября, сообщается на сайте турнира. встреча, прошедшая на\n",
            "стадионе «локомотив» в москве, завершилась со счетом 1:1. гости вышли\n",
            "вперед на 44-й минуте, отличился фелицио браун форбс. железнодорожники\n",
            "ушли от поражения на последней минуте матча — александр самедов\n",
            "реализовал пенальти на ведране чорлуке. железнодорожники набрали  8-е\n",
            "очко и остались на 13-м месте в турнирной таблице. «арсенал» имеет на\n",
            "одно очко меньше и располагается строчкой ниже. подопечные юрия семина\n",
            "прервали серию из трех поражений в рфпл подряд. ранее «локомотив»\n",
            "последовательно уступал «спартаку», «уфе» и «ростову» с одинаковым\n",
            "счетом 0:1. 29 сентября на встрече с болельщиками столичного клуба\n",
            "семин извинился перед поклонниками за неудовлетворительный результат\n",
            "команды в последнее время. семин был назначен главным тренером\n",
            "«локомотива» 26 августа. с тех пор клуб одержал одну победу и уступил\n",
            "трижды при одной ничьей.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36e3102-3462-409a-c61d-2067055e5f4f"
      },
      "source": [
        "print(detokenize([next_symbol(tokenize(train_input)+[0], model)]))\r\n",
        "print(detokenize([next_symbol(tokenize(eval_input)+[0], model)]))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "россия\n",
            "«\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "        if len(generated_output) >= 20:\r\n",
        "            print(detokenize(generated_output))\r\n",
        "            break\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "    \r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d099963c-1f99-4feb-fc99-35e1570ff7cd"
      },
      "source": [
        "print(greedy_decode(train_input, model))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "россия\n",
            "россия не\n",
            "россия неровал\n",
            "россия неровал в\n",
            "россия неровал в отставку\n",
            "россия неровал в отставку\n",
            "россия неровал в отставку\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "outputId": "81b2598c-c166-4c01-d637-a318b2df4725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(greedy_decode(eval_input, model))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "«\n",
            "«роснефть\n",
            "«роснефть»\n",
            "«роснефть» и\n",
            "«роснефть» и «\n",
            "«роснефть» и «роснефть\n",
            "«роснефть» и «роснефть»\n",
            "«роснефть» и «роснефть» отказался\n",
            "«роснефть» и «роснефть» отказался от\n",
            "«роснефть» и «роснефть» отказался от «\n",
            "«роснефть» и «роснефть» отказался от «по\n",
            "«роснефть» и «роснефть» отказался от «по»\n",
            "«роснефть» и «роснефть» отказался от «по»\n",
            "«роснефть» и «роснефть» отказался от «по»\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW",
        "outputId": "5ad480b8-a603-41ba-da08-6e740cfc3053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(tokenize(eval_input))[None, :],\r\n",
        "                                        temperature=0.5, max_length=20)\r\n",
        "print(wrapper.fill(detokenize(output[0])))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ри евро российских израильтя евро евро евро грузови мило погиб\n",
            "израильские рублей мин евро на путин пропа врач уби евро\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}