{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuSummarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2Z/bgDGof4a7hEvTsyvNj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuSummarizer/blob/main/TraxRuSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSNuqhfqcihA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd776716-f213-4cc2-b2a9-13141ebee66f"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 10.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 51.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 53.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 51.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 51.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 44.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 44.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_aqU1MnqnJJ"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "# from unicodedata import normalize\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vipmj4Jw0E"
      },
      "source": [
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTcmoVfgqzYQ",
        "outputId": "3d44a134-1969-4b42-dd35-93764d1c55a0"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7edZqAdIew0"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cEHo8bdTrbAL",
        "outputId": "c9a91cd3-abd9-4fa1-d165-10c55c4fb313"
      },
      "source": [
        "# data = pd.read_csv('/content/drive/MyDrive/lenta-ru-news.csv.zip')\r\n",
        "# data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/hungarnn/</td>\n",
              "      <td>1914. Русские войска вступили в пределы Венгрии</td>\n",
              "      <td>Бои у Сопоцкина и Друскеник закончились отступ...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://lenta.ru/news/1914/09/16/lermontov/</td>\n",
              "      <td>1914. Празднование столетия М.Ю. Лермонтова от...</td>\n",
              "      <td>Министерство народного просвещения, в виду про...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/nesteroff/</td>\n",
              "      <td>1914. Das ist Nesteroff!</td>\n",
              "      <td>Штабс-капитан П. Н. Нестеров на днях, увидев в...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://lenta.ru/news/1914/09/17/bulldogn/</td>\n",
              "      <td>1914. Бульдог-гонец под Льежем</td>\n",
              "      <td>Фотограф-корреспондент Daily Mirror рассказыва...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://lenta.ru/news/1914/09/18/zver/</td>\n",
              "      <td>1914. Под Люблином пойман швабский зверь</td>\n",
              "      <td>Лица, приехавшие в Варшаву из Люблина, передаю...</td>\n",
              "      <td>Библиотека</td>\n",
              "      <td>Первая мировая</td>\n",
              "      <td>1914/09/18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           url  ...        date\n",
              "0   https://lenta.ru/news/1914/09/16/hungarnn/  ...  1914/09/16\n",
              "1  https://lenta.ru/news/1914/09/16/lermontov/  ...  1914/09/16\n",
              "2  https://lenta.ru/news/1914/09/17/nesteroff/  ...  1914/09/17\n",
              "3   https://lenta.ru/news/1914/09/17/bulldogn/  ...  1914/09/17\n",
              "4       https://lenta.ru/news/1914/09/18/zver/  ...  1914/09/18\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f1Kj8ssZdUkc",
        "outputId": "311b32a7-5ede-4a83-9308-8b100d43d5fd"
      },
      "source": [
        "# data['text_len'] = [len(x) if not type(x)==float else 0 for x in data.text]\r\n",
        "# data.text_len[data.text_len < 2000].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16f0055ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCVmpc9dUvL",
        "outputId": "1a9626b1-7830-4063-b95a-912f661ebd2d"
      },
      "source": [
        "# text_full = []  # full text list for train senttence piece tokenizer\r\n",
        "# text_pairs = [] # paired data for train the model, format: (title, text)\r\n",
        "# for i in tqdm(range(data.shape[0])):\r\n",
        "    # if data.iloc[i, 6] >= 200 and data.iloc[i, 6] <= 2000:\r\n",
        "        # text_full.append(data.iloc[i, 1].lower() + '\\n' + data.iloc[i, 2].lower())\r\n",
        "        # list of (article, summary)\r\n",
        "        # text_pairs.append((data.iloc[i, 2].lower(), data.iloc[i, 1].lower()))\r\n",
        "\r\n",
        "# save full text to text file        \r\n",
        "# with open('full_text.txt', 'w', encoding='utf-8') as file:\r\n",
        "#     file.write('\\n'.join(text_full))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 800975/800975 [01:05<00:00, 12189.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HaJAqFDGEcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafd90d3-592e-44a8-d602-30424903033f"
      },
      "source": [
        "# text_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('бои у сопоцкина и друскеник закончились отступлением германцев. неприятель, приблизившись с севера к осовцу начал артиллерийскую борьбу с крепостью. в артиллерийском бою принимают участие тяжелые калибры. с раннего утра 14 сентября огонь достиг значительного напряжения. попытка германской пехоты пробиться ближе к крепости отражена. в галиции мы заняли дембицу. большая колонна, отступавшая по шоссе от перемышля к саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили. вылазки гарнизона перемышля остаются безуспешными. при продолжающемся отступлении австрийцев обнаруживается полное перемешивание их частей, захватываются новые партии пленных, орудия и прочая материальная часть. на перевале ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы венгрии. \\n«русский инвалид», 16 сентября 1914 года.',\n",
              " '1914. русские войска вступили в\\xa0пределы венгрии  ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxoWoTT11HC"
      },
      "source": [
        "## Load / Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2u9ccqdUyr"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=full_text.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                --model_prefix=bpe --vocab_size=32000 --model_type=bpe')\r\n",
        "# sp = spm.SentencePieceProcessor()\r\n",
        "# sp.load('/content/drive/MyDrive/bpe.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJ0tetM1znK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcS3BZXUdU2L",
        "outputId": "9a443a1c-929d-46b7-a380-16c307c65131"
      },
      "source": [
        "# s0 = text_pairs[10][0]\r\n",
        "# text_list = wrapper.wrap(s0[:300])\r\n",
        "# for line in text_list:\r\n",
        "#     print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегодня областной центр сахалина и курил получил статус очага\n",
            "распространения холеры. как сообщает итар-тасс со ссылкой на пресс-\n",
            "центр администрации сахалинской области, в лечебных учреждениях южно-\n",
            "сахалинска уже находятсятся 5 горожан, причем у двоих из них болезнь\n",
            "проходит в средне-тяжелой форме.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989eG7badU5d"
      },
      "source": [
        "# # tokenizer check\r\n",
        "# print('encode: text => id:')\r\n",
        "# print(sp.encode_as_pieces(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print(sp.encode_as_ids(s0[:300]))\r\n",
        "# print('')\r\n",
        "# print('decode: id => text:')\r\n",
        "# print(sp.decode_pieces(sp.encode_as_pieces(s0[:300])))\r\n",
        "# print('')\r\n",
        "# print(f'Beginning of sentence id: {sp.bos_id()}')\r\n",
        "# print(f'Pad id: {sp.pad_id()}')\r\n",
        "# print(f'End of sentence id: {sp.eos_id()}')\r\n",
        "# print(f'Unknown id: {sp.unk_id()}')\r\n",
        "# print(f'Vocab size: {sp.vocab_size()}')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOS2ZUWdU84"
      },
      "source": [
        "# uid = 18298\r\n",
        "# spiece = \"\\u2581Саха\"\r\n",
        "# unknown = \"_НЕИЗВЕСТНОСТЬ_\"\r\n",
        "\r\n",
        "# # id <=> piece conversion\r\n",
        "# print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\r\n",
        "# print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\r\n",
        "\r\n",
        "# # returns 0 for unknown tokens (we can change the id for UNK)\r\n",
        "# print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFOPw70bdVAT"
      },
      "source": [
        "# # vocab's head and tail test\r\n",
        "# print('\\nId\\tSentP\\tControl?')\r\n",
        "# print('------------------------')\r\n",
        "# for uid in range(7):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\r\n",
        "    \r\n",
        "# for uid in range(sp.vocab_size()-7,sp.vocab_size()):\r\n",
        "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2NwNe142mOa"
      },
      "source": [
        "## Data: preprocess and create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLsMmUFFnHA",
        "outputId": "a7afc6c5-6578-4fa6-b9c0-29b13caa9872"
      },
      "source": [
        "text_pairs = pd.read_csv('/content/drive/MyDrive/lenta.csv', sep=';')\r\n",
        "text_pairs = [(x, y) for x, y in zip(text_pairs.article, text_pairs.summary)]\r\n",
        "print(wrapper.fill(text_pairs[0][0]))\r\n",
        "print(wrapper.fill(text_pairs[0][1]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n",
            "в киеве исключили новый раунд минских переговоров\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPM2r5L3s4i",
        "outputId": "ee4e9d55-f859-4a8d-b9c2-3ffc9e69bb8a"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(text_pairs)*0.95)\r\n",
        "train_text_pairs = text_pairs[:margin]\r\n",
        "print('train cases: ', len(train_text_pairs))\r\n",
        "eval_text_pairs = text_pairs[margin:]\r\n",
        "print('eval cases: ', len(eval_text_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  686277\n",
            "eval cases:  36120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC79r_7EPy6",
        "outputId": "b37e863d-f795-4d9f-a924-a8d3c066ef72"
      },
      "source": [
        "print(wrapper.fill(train_text_pairs[0][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "проведение нового раунда минских переговоров по ситуации на украине в\n",
            "ближайшее время исключено. об этом, как сообщает тасс, заявил советник\n",
            "главы службы безопасности украины (сбу) маркиян лубкивский. «минска-2\n",
            "не будет. у нас есть минск-1 и те договоренности, которые нужно\n",
            "выполнять», — сказал лубкивский в эфире телеканала \"1+1\", обвинив\n",
            "ополченцев в нарушении прежних договоренностей. в четверг полномочный\n",
            "представитель самопровозглашенной донецкой народной республики (днр)\n",
            "на переговорах контактной группы в минске денис пушилин заявил, что\n",
            "власти республики готовы к возобновлению минского процесса. при этом\n",
            "он выразил недоумение относительно позиции киева, который, по его\n",
            "мнению, «стремится к полному прекращению минского процесса, чтобы\n",
            "отказаться от прямых контактов с республиками». в сентябре 2014 года в\n",
            "минске состоялись две встречи контактной группы по украине. 20\n",
            "сентября был принят меморандум по осуществлению режима прекращения\n",
            "огня, состоящий из девяти пунктов. 5 сентября был подписан протокол о\n",
            "мирном урегулировании на юго-востоке украины. главными пунктами\n",
            "протокола стали соглашение о прекращении огня и обмене военнопленными.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEO4D0w5mtq"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list containing tuples (article, summary)\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample)\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_text_pairs, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_text_pairs, shuffle=True)        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF_P5KCb7hPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60eb3545-98e4-4628-fdce-682deaaf8a97"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/')\r\n",
        "    return wrapper.fill(s)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type='sentencepiece',\r\n",
        "        vocab_file='bpe.model',\r\n",
        "        vocab_dir='/content/drive/MyDrive/'))\r\n",
        "    \r\n",
        "    return list(inputs) + [EOS]\r\n",
        " \r\n",
        "    \r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='sentencepiece',\r\n",
        "    vocab_file='bpe.model',\r\n",
        "    vocab_dir='/content/drive/MyDrive/')\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiAbTnyQHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841286e1-4311-478c-e536-8ce0d1a649d9"
      },
      "source": [
        "tokenize('тест')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15117, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvHY7mzQboWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c798fef3-4d8f-4985-b841-6324b08b3c4f"
      },
      "source": [
        "tokenize('НЕИЗВЕСТНОСТЬ')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15924, 2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfl-ORS_fyb"
      },
      "source": [
        "# tokenized = tokenize('сведения о пассажирах на всех видах транспорта, где используются именные проездные билеты')\r\n",
        "# print('tokenized:')\r\n",
        "# print(tokenized)\r\n",
        "# print('len=', len(tokenized))\r\n",
        "# detokenized = detokenize(tokenized)\r\n",
        "# print('detokenized:')\r\n",
        "# print(detokenized)\r\n",
        "# print('len=', len(detokenized.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTKFA_059NFp"
      },
      "source": [
        "# Concatenate tokenized inputs and targets using 0 as separator.\r\n",
        "def preprocess(stream):\r\n",
        "    for (article, summary) in stream:\r\n",
        "        joint = np.array(list(article) + [EOS, PAD] + list(summary) + [EOS])\r\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \r\n",
        "        yield joint, joint, np.array(mask)\r\n",
        "\r\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\r\n",
        "input_pipeline = trax.data.Serial(\r\n",
        "    # Tokenizes\r\n",
        "    trax.data.Tokenize(vocab_type='sentencepiece',\r\n",
        "                       vocab_dir='/content/drive/MyDrive/',\r\n",
        "                       vocab_file='bpe.model'),\r\n",
        "    # Uses function defined above\r\n",
        "    preprocess,\r\n",
        "    trax.data.FilterByLength(2048)\r\n",
        ")\r\n",
        "\r\n",
        "# Apply preprocessing to data streams.\r\n",
        "train_stream = input_pipeline(train_data_stream())\r\n",
        "eval_stream = input_pipeline(eval_data_stream())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tshiu0tGHb61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edab2ca-b147-4bf2-d1f2-b145d0dde843"
      },
      "source": [
        "train_input, train_target, train_mask = next(train_stream)\r\n",
        "# assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\r\n",
        "# check pad (id:0) and sep/eos (id:1)\r\n",
        "print(train_input[-20:])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15949     1     0  1291  3387   705  6511 15970 13976  1571    25   273\n",
            "   446    81 15931    35   694 14410 15957     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjB-_IHLCO4"
      },
      "source": [
        "## Batching and Bucketing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBM6LWbtLJZW"
      },
      "source": [
        "# batch of 8 sentences of length < 256 , 4 of length < 512....\r\n",
        "boundaries =  [512, 1024]\r\n",
        "batch_sizes = [8, 4, 2]\r\n",
        "\r\n",
        "# Create the streams.\r\n",
        "train_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(train_stream)\r\n",
        "\r\n",
        "eval_batch_stream = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wUSPsJLvnP",
        "outputId": "f56c61a3-cb64-47e1-9c14-d410850aaaf2"
      },
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\r\n",
        "\r\n",
        "# Shape of the input_batch\r\n",
        "input_batch.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iuq_rS2MTeM",
        "outputId": "735ad82d-07f6-47fe-9ec2-08517f1dbfa7"
      },
      "source": [
        "# check autopadding endig of sample\r\n",
        "# 1, 0, <not 0 digit>... - end of article and start of summary\r\n",
        "input_batch[0]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4253,   500,  7429, 14620,   127,   280,  1392, 15939, 15945,\n",
              "        6447,   647,    25,  1256,  1222, 15960,   159,   637, 15940,\n",
              "         280,  5577,     5,   203, 15934,    37, 15949,   207,   948,\n",
              "          81,  8675,  2062,  3312,  1530,   294,     5,  1733, 15945,\n",
              "        1517,  1215, 15945,  3678,  6846,  1696,   687,   303,  2168,\n",
              "        6823,    24,     9,  3857,   163,  1313,     6, 11420, 15945,\n",
              "         977,  4179,    43, 14929,    25,  4157,  1472, 15947,   432,\n",
              "         101, 15949,  2186, 15933,     4,  3310,   739, 15945,  7429,\n",
              "       15250,    44,   235, 15949, 15384,  6049,  8518,  1544,    86,\n",
              "        4268,    36,   182, 15949,  4668, 15945,    79,   236, 10851,\n",
              "           5,   203, 15934,   158,    25,  2071,  3345,  1514,     4,\n",
              "        8916,   861, 15949,  4872,  1743,   164,  3801, 11385,  2224,\n",
              "        1280, 15949, 11096,  8749,   913,  1442,  1479,    18,  2312,\n",
              "       15949,   207,   948,  1530,    81,  3177,   294,  1888,  6049,\n",
              "         280,  5577,     5,   203, 15934,    37,  2644, 15945,    79,\n",
              "           5, 15482,   288,   712,    29,    16, 15895,   203, 15934,\n",
              "         189, 15949,    17,   563,  3755,  5201, 12128,  1472, 15947,\n",
              "         432,   101,   657,   981,    29,   249,    59,  2033,    25,\n",
              "        5175,  3712, 11795,    16, 15945,    17,   444,  6497,   111,\n",
              "        7252, 15945,  2408,     5,   171,    90,   432,   101,    17,\n",
              "         227,    96, 15937,   897,  9637, 15554, 15940,   136,    16,\n",
              "       14070,  5175,  1101,  5294, 15949,   586,   102,    14,    95,\n",
              "          18,     5,   171,    90,   799,   442,   642,  6001,    25,\n",
              "        3365,    16,  2765,  8182,  7310,   159, 15945,  7959,   253,\n",
              "         358,  3012,  1681, 15949,     1,     0,     5,   203, 15934,\n",
              "          37,   177,  2285,  7429,   280,  1392, 15939,     1,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsAxgBNv1t"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiM2BOTka_Q"
      },
      "source": [
        "### Positional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJLcdurkngQ"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noeDrbSOk1Mr"
      },
      "source": [
        "### Feed-Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzUPRrxknsz"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter\r\n",
        "        ff_activation(),  # ReLU\r\n",
        "        # Add dropout with rate and mode specified (don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ERXQvXmlGV1"
      },
      "source": [
        "### Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWvL2ayknwD"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\r\n",
        "                 dropout, mode, ff_activation):\r\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n",
        "\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # List of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # The feed-forward block takes care of normalization\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyDT0rvlVU8"
      },
      "source": [
        "### Trnsformer (decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBjgQXQYkn0d"
      },
      "source": [
        "def SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"Returns a Transformer language model.\r\n",
        "\r\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\r\n",
        "    decoder part of the overall Transformer.)\r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Stack of decoder blocks with n_layers with necessary parameters\r\n",
        "    decoder_blocks = [ \r\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n",
        "\r\n",
        "    # The complete model\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), \r\n",
        "        # Add embedding inputs and positional encoder\r\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n",
        "        # Add decoder blocks\r\n",
        "        decoder_blocks, \r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(), \r\n",
        "\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size), \r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax() \r\n",
        "    )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-ElzVnlxjr",
        "outputId": "73709340-9daa-4618-a759-866c9c71513f"
      },
      "source": [
        "print(SumTransformer(n_layers=1))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_16000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Serial[\n",
            "            Serial[\n",
            "              Branch_out3[\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "                [Dense_512, Serial[\n",
            "                  SplitIntoHeads\n",
            "                ]]\n",
            "              ]\n",
            "              DotProductCausalAttention_in3\n",
            "              Serial[\n",
            "                MergeHeads\n",
            "              ]\n",
            "              Dense_512\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_16000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n63WBWmE1-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytlBNiWN13D"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "def training_loop(SumTransformer, train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        SumTransformer (trax.layers.combinators.Serial): The transformer model.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "\r\n",
        "    # for initial train\r\n",
        "    # lr_schedule = trax.lr.warmup(n_warmup_steps=4000, max_value=0.00015)\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=8000, max_value=0.00015)\r\n",
        "    \r\n",
        "    # for re-train\r\n",
        "    lr_schedule = trax.supervised.lr_schedules.constant(0.000075)\r\n",
        "\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data=train_gen, # The training generator\r\n",
        "      loss_layer=tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer=trax.optimizers.Adam(0.0001), # Optimizer \r\n",
        "      lr_schedule=lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data=eval_gen, \r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(SumTransformer(d_model=512,\r\n",
        "                                       d_ff=2048,\r\n",
        "                                       n_layers=6,\r\n",
        "                                       n_heads=8,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7c4DZ6aGI8L"
      },
      "source": [
        "!cp /content/drive/MyDrive/model/model.pkl.gz ~/model/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JVjnU1nAxv",
        "outputId": "aaf8caf3-a545-4502-9315-1a4b6b760468"
      },
      "source": [
        "# Should take around 1 minute per 100 step on GPU\r\n",
        "# !rm -f ~/model/model.pkl.gz\r\n",
        "loop = training_loop(SumTransformer, train_batch_stream, eval_batch_stream)\r\n",
        "loop.run(20000)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model/"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step  180100: Ran 100 train steps in 71.88 secs\n",
            "Step  180100: train CrossEntropyLoss |  3.37807584\n",
            "Step  180100: eval  CrossEntropyLoss |  2.97063446\n",
            "Step  180100: eval          Accuracy |  0.45977011\n",
            "\n",
            "Step  180200: Ran 100 train steps in 66.45 secs\n",
            "Step  180200: train CrossEntropyLoss |  3.50126934\n",
            "Step  180200: eval  CrossEntropyLoss |  3.65752649\n",
            "Step  180200: eval          Accuracy |  0.29670331\n",
            "\n",
            "Step  180300: Ran 100 train steps in 47.90 secs\n",
            "Step  180300: train CrossEntropyLoss |  3.35455823\n",
            "Step  180300: eval  CrossEntropyLoss |  3.98713398\n",
            "Step  180300: eval          Accuracy |  0.35789475\n",
            "\n",
            "Step  180400: Ran 100 train steps in 48.26 secs\n",
            "Step  180400: train CrossEntropyLoss |  3.36919904\n",
            "Step  180400: eval  CrossEntropyLoss |  3.66653109\n",
            "Step  180400: eval          Accuracy |  0.47524750\n",
            "\n",
            "Step  180500: Ran 100 train steps in 48.26 secs\n",
            "Step  180500: train CrossEntropyLoss |  3.31150293\n",
            "Step  180500: eval  CrossEntropyLoss |  2.91002131\n",
            "Step  180500: eval          Accuracy |  0.52884614\n",
            "\n",
            "Step  180600: Ran 100 train steps in 48.52 secs\n",
            "Step  180600: train CrossEntropyLoss |  3.36776662\n",
            "Step  180600: eval  CrossEntropyLoss |  4.11931944\n",
            "Step  180600: eval          Accuracy |  0.38775510\n",
            "\n",
            "Step  180700: Ran 100 train steps in 48.47 secs\n",
            "Step  180700: train CrossEntropyLoss |  3.25493336\n",
            "Step  180700: eval  CrossEntropyLoss |  3.19702959\n",
            "Step  180700: eval          Accuracy |  0.50505048\n",
            "\n",
            "Step  180800: Ran 100 train steps in 48.44 secs\n",
            "Step  180800: train CrossEntropyLoss |  3.25765920\n",
            "Step  180800: eval  CrossEntropyLoss |  3.34940004\n",
            "Step  180800: eval          Accuracy |  0.44117647\n",
            "\n",
            "Step  180900: Ran 100 train steps in 48.71 secs\n",
            "Step  180900: train CrossEntropyLoss |  3.23116875\n",
            "Step  180900: eval  CrossEntropyLoss |  3.01545715\n",
            "Step  180900: eval          Accuracy |  0.49019611\n",
            "\n",
            "Step  181000: Ran 100 train steps in 48.74 secs\n",
            "Step  181000: train CrossEntropyLoss |  3.18267083\n",
            "Step  181000: eval  CrossEntropyLoss |  3.38815594\n",
            "Step  181000: eval          Accuracy |  0.43750000\n",
            "\n",
            "Step  181100: Ran 100 train steps in 48.76 secs\n",
            "Step  181100: train CrossEntropyLoss |  3.16436243\n",
            "Step  181100: eval  CrossEntropyLoss |  3.20845819\n",
            "Step  181100: eval          Accuracy |  0.52830189\n",
            "\n",
            "Step  181200: Ran 100 train steps in 48.79 secs\n",
            "Step  181200: train CrossEntropyLoss |  3.26007795\n",
            "Step  181200: eval  CrossEntropyLoss |  2.75529051\n",
            "Step  181200: eval          Accuracy |  0.50450450\n",
            "\n",
            "Step  181300: Ran 100 train steps in 48.80 secs\n",
            "Step  181300: train CrossEntropyLoss |  3.20351672\n",
            "Step  181300: eval  CrossEntropyLoss |  3.11018753\n",
            "Step  181300: eval          Accuracy |  0.48672566\n",
            "\n",
            "Step  181400: Ran 100 train steps in 48.85 secs\n",
            "Step  181400: train CrossEntropyLoss |  3.15813541\n",
            "Step  181400: eval  CrossEntropyLoss |  2.88986850\n",
            "Step  181400: eval          Accuracy |  0.54945058\n",
            "\n",
            "Step  181500: Ran 100 train steps in 48.77 secs\n",
            "Step  181500: train CrossEntropyLoss |  3.16650271\n",
            "Step  181500: eval  CrossEntropyLoss |  3.41921830\n",
            "Step  181500: eval          Accuracy |  0.45098040\n",
            "\n",
            "Step  181600: Ran 100 train steps in 48.85 secs\n",
            "Step  181600: train CrossEntropyLoss |  3.18857694\n",
            "Step  181600: eval  CrossEntropyLoss |  3.16551208\n",
            "Step  181600: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  181700: Ran 100 train steps in 48.69 secs\n",
            "Step  181700: train CrossEntropyLoss |  3.13041472\n",
            "Step  181700: eval  CrossEntropyLoss |  3.04356813\n",
            "Step  181700: eval          Accuracy |  0.54081631\n",
            "\n",
            "Step  181800: Ran 100 train steps in 48.75 secs\n",
            "Step  181800: train CrossEntropyLoss |  3.10571527\n",
            "Step  181800: eval  CrossEntropyLoss |  2.98683763\n",
            "Step  181800: eval          Accuracy |  0.51937985\n",
            "\n",
            "Step  181900: Ran 100 train steps in 49.02 secs\n",
            "Step  181900: train CrossEntropyLoss |  3.11043715\n",
            "Step  181900: eval  CrossEntropyLoss |  3.48712492\n",
            "Step  181900: eval          Accuracy |  0.44444448\n",
            "\n",
            "Step  182000: Ran 100 train steps in 49.16 secs\n",
            "Step  182000: train CrossEntropyLoss |  3.11038375\n",
            "Step  182000: eval  CrossEntropyLoss |  3.70846772\n",
            "Step  182000: eval          Accuracy |  0.39603961\n",
            "\n",
            "Step  182100: Ran 100 train steps in 49.07 secs\n",
            "Step  182100: train CrossEntropyLoss |  2.99646235\n",
            "Step  182100: eval  CrossEntropyLoss |  4.19028091\n",
            "Step  182100: eval          Accuracy |  0.38383839\n",
            "\n",
            "Step  182200: Ran 100 train steps in 49.12 secs\n",
            "Step  182200: train CrossEntropyLoss |  3.12401295\n",
            "Step  182200: eval  CrossEntropyLoss |  3.02988410\n",
            "Step  182200: eval          Accuracy |  0.48623851\n",
            "\n",
            "Step  182300: Ran 100 train steps in 49.32 secs\n",
            "Step  182300: train CrossEntropyLoss |  3.12196398\n",
            "Step  182300: eval  CrossEntropyLoss |  2.45798874\n",
            "Step  182300: eval          Accuracy |  0.57547170\n",
            "\n",
            "Step  182400: Ran 100 train steps in 48.93 secs\n",
            "Step  182400: train CrossEntropyLoss |  3.02594852\n",
            "Step  182400: eval  CrossEntropyLoss |  3.75559616\n",
            "Step  182400: eval          Accuracy |  0.37837839\n",
            "\n",
            "Step  182500: Ran 100 train steps in 48.99 secs\n",
            "Step  182500: train CrossEntropyLoss |  3.03976274\n",
            "Step  182500: eval  CrossEntropyLoss |  2.70135713\n",
            "Step  182500: eval          Accuracy |  0.56410259\n",
            "\n",
            "Step  182600: Ran 100 train steps in 48.94 secs\n",
            "Step  182600: train CrossEntropyLoss |  3.07425904\n",
            "Step  182600: eval  CrossEntropyLoss |  3.01280236\n",
            "Step  182600: eval          Accuracy |  0.52777779\n",
            "\n",
            "Step  182700: Ran 100 train steps in 48.94 secs\n",
            "Step  182700: train CrossEntropyLoss |  3.08648634\n",
            "Step  182700: eval  CrossEntropyLoss |  3.72299600\n",
            "Step  182700: eval          Accuracy |  0.40707964\n",
            "\n",
            "Step  182800: Ran 100 train steps in 48.99 secs\n",
            "Step  182800: train CrossEntropyLoss |  3.12204766\n",
            "Step  182800: eval  CrossEntropyLoss |  3.07102108\n",
            "Step  182800: eval          Accuracy |  0.44230771\n",
            "\n",
            "Step  182900: Ran 100 train steps in 48.79 secs\n",
            "Step  182900: train CrossEntropyLoss |  3.04673362\n",
            "Step  182900: eval  CrossEntropyLoss |  2.59525561\n",
            "Step  182900: eval          Accuracy |  0.62184876\n",
            "\n",
            "Step  183000: Ran 100 train steps in 48.74 secs\n",
            "Step  183000: train CrossEntropyLoss |  3.04295373\n",
            "Step  183000: eval  CrossEntropyLoss |  3.01003265\n",
            "Step  183000: eval          Accuracy |  0.52830189\n",
            "\n",
            "Step  183100: Ran 100 train steps in 48.64 secs\n",
            "Step  183100: train CrossEntropyLoss |  3.00861049\n",
            "Step  183100: eval  CrossEntropyLoss |  3.36198258\n",
            "Step  183100: eval          Accuracy |  0.44144145\n",
            "\n",
            "Step  183200: Ran 100 train steps in 48.78 secs\n",
            "Step  183200: train CrossEntropyLoss |  3.08214927\n",
            "Step  183200: eval  CrossEntropyLoss |  2.59511423\n",
            "Step  183200: eval          Accuracy |  0.58333337\n",
            "\n",
            "Step  183300: Ran 100 train steps in 48.90 secs\n",
            "Step  183300: train CrossEntropyLoss |  3.06192255\n",
            "Step  183300: eval  CrossEntropyLoss |  3.53398538\n",
            "Step  183300: eval          Accuracy |  0.36904761\n",
            "\n",
            "Step  183400: Ran 100 train steps in 48.75 secs\n",
            "Step  183400: train CrossEntropyLoss |  2.99164176\n",
            "Step  183400: eval  CrossEntropyLoss |  3.20305634\n",
            "Step  183400: eval          Accuracy |  0.44628099\n",
            "\n",
            "Step  183500: Ran 100 train steps in 48.75 secs\n",
            "Step  183500: train CrossEntropyLoss |  3.04762816\n",
            "Step  183500: eval  CrossEntropyLoss |  3.17368841\n",
            "Step  183500: eval          Accuracy |  0.44117647\n",
            "\n",
            "Step  183600: Ran 100 train steps in 48.94 secs\n",
            "Step  183600: train CrossEntropyLoss |  3.02790928\n",
            "Step  183600: eval  CrossEntropyLoss |  2.38646007\n",
            "Step  183600: eval          Accuracy |  0.57281554\n",
            "\n",
            "Step  183700: Ran 100 train steps in 48.97 secs\n",
            "Step  183700: train CrossEntropyLoss |  3.02765894\n",
            "Step  183700: eval  CrossEntropyLoss |  3.15585589\n",
            "Step  183700: eval          Accuracy |  0.53125000\n",
            "\n",
            "Step  183800: Ran 100 train steps in 49.07 secs\n",
            "Step  183800: train CrossEntropyLoss |  3.01582384\n",
            "Step  183800: eval  CrossEntropyLoss |  3.22045040\n",
            "Step  183800: eval          Accuracy |  0.47115386\n",
            "\n",
            "Step  183900: Ran 100 train steps in 49.03 secs\n",
            "Step  183900: train CrossEntropyLoss |  3.01566124\n",
            "Step  183900: eval  CrossEntropyLoss |  3.56419301\n",
            "Step  183900: eval          Accuracy |  0.43269232\n",
            "\n",
            "Step  184000: Ran 100 train steps in 49.08 secs\n",
            "Step  184000: train CrossEntropyLoss |  3.05700493\n",
            "Step  184000: eval  CrossEntropyLoss |  3.12964845\n",
            "Step  184000: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  184100: Ran 100 train steps in 49.19 secs\n",
            "Step  184100: train CrossEntropyLoss |  2.99752474\n",
            "Step  184100: eval  CrossEntropyLoss |  2.21730328\n",
            "Step  184100: eval          Accuracy |  0.55913979\n",
            "\n",
            "Step  184200: Ran 100 train steps in 49.18 secs\n",
            "Step  184200: train CrossEntropyLoss |  3.00005555\n",
            "Step  184200: eval  CrossEntropyLoss |  3.87538052\n",
            "Step  184200: eval          Accuracy |  0.39805827\n",
            "\n",
            "Step  184300: Ran 100 train steps in 48.89 secs\n",
            "Step  184300: train CrossEntropyLoss |  3.08571506\n",
            "Step  184300: eval  CrossEntropyLoss |  3.31168580\n",
            "Step  184300: eval          Accuracy |  0.46728972\n",
            "\n",
            "Step  184400: Ran 100 train steps in 69.74 secs\n",
            "Step  184400: train CrossEntropyLoss |  2.96758485\n",
            "Step  184400: eval  CrossEntropyLoss |  3.64174867\n",
            "Step  184400: eval          Accuracy |  0.41747573\n",
            "\n",
            "Step  184500: Ran 100 train steps in 49.06 secs\n",
            "Step  184500: train CrossEntropyLoss |  2.95016360\n",
            "Step  184500: eval  CrossEntropyLoss |  3.26381278\n",
            "Step  184500: eval          Accuracy |  0.47368422\n",
            "\n",
            "Step  184600: Ran 100 train steps in 48.91 secs\n",
            "Step  184600: train CrossEntropyLoss |  3.02092767\n",
            "Step  184600: eval  CrossEntropyLoss |  3.00823641\n",
            "Step  184600: eval          Accuracy |  0.47368422\n",
            "\n",
            "Step  184700: Ran 100 train steps in 49.01 secs\n",
            "Step  184700: train CrossEntropyLoss |  2.93742800\n",
            "Step  184700: eval  CrossEntropyLoss |  3.36311078\n",
            "Step  184700: eval          Accuracy |  0.44999999\n",
            "\n",
            "Step  184800: Ran 100 train steps in 49.18 secs\n",
            "Step  184800: train CrossEntropyLoss |  2.94978595\n",
            "Step  184800: eval  CrossEntropyLoss |  2.46478748\n",
            "Step  184800: eval          Accuracy |  0.57943922\n",
            "\n",
            "Step  184900: Ran 100 train steps in 49.04 secs\n",
            "Step  184900: train CrossEntropyLoss |  3.00312948\n",
            "Step  184900: eval  CrossEntropyLoss |  2.87180805\n",
            "Step  184900: eval          Accuracy |  0.52475244\n",
            "\n",
            "Step  185000: Ran 100 train steps in 48.94 secs\n",
            "Step  185000: train CrossEntropyLoss |  2.98814893\n",
            "Step  185000: eval  CrossEntropyLoss |  3.34847546\n",
            "Step  185000: eval          Accuracy |  0.43809524\n",
            "\n",
            "Step  185100: Ran 100 train steps in 48.97 secs\n",
            "Step  185100: train CrossEntropyLoss |  2.98226595\n",
            "Step  185100: eval  CrossEntropyLoss |  4.00514078\n",
            "Step  185100: eval          Accuracy |  0.36440679\n",
            "\n",
            "Step  185200: Ran 100 train steps in 49.19 secs\n",
            "Step  185200: train CrossEntropyLoss |  2.95651340\n",
            "Step  185200: eval  CrossEntropyLoss |  3.04684520\n",
            "Step  185200: eval          Accuracy |  0.46391755\n",
            "\n",
            "Step  185300: Ran 100 train steps in 49.02 secs\n",
            "Step  185300: train CrossEntropyLoss |  2.98143816\n",
            "Step  185300: eval  CrossEntropyLoss |  2.91539454\n",
            "Step  185300: eval          Accuracy |  0.52941179\n",
            "\n",
            "Step  185400: Ran 100 train steps in 49.10 secs\n",
            "Step  185400: train CrossEntropyLoss |  2.92473602\n",
            "Step  185400: eval  CrossEntropyLoss |  3.61874461\n",
            "Step  185400: eval          Accuracy |  0.42857143\n",
            "\n",
            "Step  185500: Ran 100 train steps in 48.95 secs\n",
            "Step  185500: train CrossEntropyLoss |  2.98491764\n",
            "Step  185500: eval  CrossEntropyLoss |  3.15504074\n",
            "Step  185500: eval          Accuracy |  0.47422683\n",
            "\n",
            "Step  185600: Ran 100 train steps in 48.88 secs\n",
            "Step  185600: train CrossEntropyLoss |  3.00802326\n",
            "Step  185600: eval  CrossEntropyLoss |  3.52546000\n",
            "Step  185600: eval          Accuracy |  0.45918366\n",
            "\n",
            "Step  185700: Ran 100 train steps in 48.76 secs\n",
            "Step  185700: train CrossEntropyLoss |  2.99443054\n",
            "Step  185700: eval  CrossEntropyLoss |  3.15230560\n",
            "Step  185700: eval          Accuracy |  0.47656250\n",
            "\n",
            "Step  185800: Ran 100 train steps in 48.93 secs\n",
            "Step  185800: train CrossEntropyLoss |  2.97107077\n",
            "Step  185800: eval  CrossEntropyLoss |  2.73003387\n",
            "Step  185800: eval          Accuracy |  0.48672566\n",
            "\n",
            "Step  185900: Ran 100 train steps in 48.58 secs\n",
            "Step  185900: train CrossEntropyLoss |  2.93056750\n",
            "Step  185900: eval  CrossEntropyLoss |  2.85434437\n",
            "Step  185900: eval          Accuracy |  0.51219517\n",
            "\n",
            "Step  186000: Ran 100 train steps in 48.91 secs\n",
            "Step  186000: train CrossEntropyLoss |  2.86754084\n",
            "Step  186000: eval  CrossEntropyLoss |  3.06777000\n",
            "Step  186000: eval          Accuracy |  0.47916669\n",
            "\n",
            "Step  186100: Ran 100 train steps in 48.88 secs\n",
            "Step  186100: train CrossEntropyLoss |  2.98090100\n",
            "Step  186100: eval  CrossEntropyLoss |  3.11066484\n",
            "Step  186100: eval          Accuracy |  0.43902439\n",
            "\n",
            "Step  186200: Ran 100 train steps in 48.87 secs\n",
            "Step  186200: train CrossEntropyLoss |  3.00077701\n",
            "Step  186200: eval  CrossEntropyLoss |  2.59947896\n",
            "Step  186200: eval          Accuracy |  0.55789477\n",
            "\n",
            "Step  186300: Ran 100 train steps in 48.97 secs\n",
            "Step  186300: train CrossEntropyLoss |  2.98898554\n",
            "Step  186300: eval  CrossEntropyLoss |  2.86836934\n",
            "Step  186300: eval          Accuracy |  0.55670106\n",
            "\n",
            "Step  186400: Ran 100 train steps in 49.01 secs\n",
            "Step  186400: train CrossEntropyLoss |  2.98438954\n",
            "Step  186400: eval  CrossEntropyLoss |  3.01725936\n",
            "Step  186400: eval          Accuracy |  0.48623851\n",
            "\n",
            "Step  186500: Ran 100 train steps in 49.01 secs\n",
            "Step  186500: train CrossEntropyLoss |  2.98337555\n",
            "Step  186500: eval  CrossEntropyLoss |  3.11589003\n",
            "Step  186500: eval          Accuracy |  0.45999998\n",
            "\n",
            "Step  186600: Ran 100 train steps in 49.13 secs\n",
            "Step  186600: train CrossEntropyLoss |  2.95566821\n",
            "Step  186600: eval  CrossEntropyLoss |  3.11819553\n",
            "Step  186600: eval          Accuracy |  0.46739131\n",
            "\n",
            "Step  186700: Ran 100 train steps in 49.11 secs\n",
            "Step  186700: train CrossEntropyLoss |  2.99908471\n",
            "Step  186700: eval  CrossEntropyLoss |  2.09778261\n",
            "Step  186700: eval          Accuracy |  0.59223300\n",
            "\n",
            "Step  186800: Ran 100 train steps in 48.91 secs\n",
            "Step  186800: train CrossEntropyLoss |  3.03313708\n",
            "Step  186800: eval  CrossEntropyLoss |  3.84278250\n",
            "Step  186800: eval          Accuracy |  0.37383175\n",
            "\n",
            "Step  186900: Ran 100 train steps in 49.00 secs\n",
            "Step  186900: train CrossEntropyLoss |  2.90439844\n",
            "Step  186900: eval  CrossEntropyLoss |  2.69217086\n",
            "Step  186900: eval          Accuracy |  0.53488374\n",
            "\n",
            "Step  187000: Ran 100 train steps in 49.01 secs\n",
            "Step  187000: train CrossEntropyLoss |  3.01468658\n",
            "Step  187000: eval  CrossEntropyLoss |  3.19033813\n",
            "Step  187000: eval          Accuracy |  0.46296296\n",
            "\n",
            "Step  187100: Ran 100 train steps in 48.77 secs\n",
            "Step  187100: train CrossEntropyLoss |  3.05245781\n",
            "Step  187100: eval  CrossEntropyLoss |  3.49770355\n",
            "Step  187100: eval          Accuracy |  0.45544553\n",
            "\n",
            "Step  187200: Ran 100 train steps in 48.99 secs\n",
            "Step  187200: train CrossEntropyLoss |  2.95221663\n",
            "Step  187200: eval  CrossEntropyLoss |  2.77943110\n",
            "Step  187200: eval          Accuracy |  0.54545450\n",
            "\n",
            "Step  187300: Ran 100 train steps in 49.05 secs\n",
            "Step  187300: train CrossEntropyLoss |  2.93952560\n",
            "Step  187300: eval  CrossEntropyLoss |  3.73089957\n",
            "Step  187300: eval          Accuracy |  0.47368422\n",
            "\n",
            "Step  187400: Ran 100 train steps in 48.99 secs\n",
            "Step  187400: train CrossEntropyLoss |  2.90606642\n",
            "Step  187400: eval  CrossEntropyLoss |  3.10639453\n",
            "Step  187400: eval          Accuracy |  0.51041669\n",
            "\n",
            "Step  187500: Ran 100 train steps in 48.72 secs\n",
            "Step  187500: train CrossEntropyLoss |  2.98193598\n",
            "Step  187500: eval  CrossEntropyLoss |  2.78434157\n",
            "Step  187500: eval          Accuracy |  0.53061223\n",
            "\n",
            "Step  187600: Ran 100 train steps in 49.20 secs\n",
            "Step  187600: train CrossEntropyLoss |  2.91908145\n",
            "Step  187600: eval  CrossEntropyLoss |  2.27028561\n",
            "Step  187600: eval          Accuracy |  0.60396039\n",
            "\n",
            "Step  187700: Ran 100 train steps in 49.66 secs\n",
            "Step  187700: train CrossEntropyLoss |  2.96077967\n",
            "Step  187700: eval  CrossEntropyLoss |  3.21008945\n",
            "Step  187700: eval          Accuracy |  0.42105263\n",
            "\n",
            "Step  187800: Ran 100 train steps in 49.77 secs\n",
            "Step  187800: train CrossEntropyLoss |  2.94710112\n",
            "Step  187800: eval  CrossEntropyLoss |  2.49263597\n",
            "Step  187800: eval          Accuracy |  0.53921568\n",
            "\n",
            "Step  187900: Ran 100 train steps in 50.17 secs\n",
            "Step  187900: train CrossEntropyLoss |  2.92750430\n",
            "Step  187900: eval  CrossEntropyLoss |  2.74208331\n",
            "Step  187900: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  188000: Ran 100 train steps in 50.26 secs\n",
            "Step  188000: train CrossEntropyLoss |  2.86895537\n",
            "Step  188000: eval  CrossEntropyLoss |  3.18921471\n",
            "Step  188000: eval          Accuracy |  0.41525424\n",
            "\n",
            "Step  188100: Ran 100 train steps in 50.40 secs\n",
            "Step  188100: train CrossEntropyLoss |  2.87664485\n",
            "Step  188100: eval  CrossEntropyLoss |  2.74290657\n",
            "Step  188100: eval          Accuracy |  0.53921568\n",
            "\n",
            "Step  188200: Ran 100 train steps in 50.58 secs\n",
            "Step  188200: train CrossEntropyLoss |  2.97108698\n",
            "Step  188200: eval  CrossEntropyLoss |  3.01497030\n",
            "Step  188200: eval          Accuracy |  0.47674417\n",
            "\n",
            "Step  188300: Ran 100 train steps in 50.83 secs\n",
            "Step  188300: train CrossEntropyLoss |  2.92105913\n",
            "Step  188300: eval  CrossEntropyLoss |  2.48393178\n",
            "Step  188300: eval          Accuracy |  0.57142860\n",
            "\n",
            "Step  188400: Ran 100 train steps in 50.93 secs\n",
            "Step  188400: train CrossEntropyLoss |  2.92720485\n",
            "Step  188400: eval  CrossEntropyLoss |  3.35289025\n",
            "Step  188400: eval          Accuracy |  0.44761905\n",
            "\n",
            "Step  188500: Ran 100 train steps in 50.87 secs\n",
            "Step  188500: train CrossEntropyLoss |  2.91080737\n",
            "Step  188500: eval  CrossEntropyLoss |  2.69260073\n",
            "Step  188500: eval          Accuracy |  0.52884614\n",
            "\n",
            "Step  188600: Ran 100 train steps in 50.63 secs\n",
            "Step  188600: train CrossEntropyLoss |  2.97781920\n",
            "Step  188600: eval  CrossEntropyLoss |  2.81348753\n",
            "Step  188600: eval          Accuracy |  0.52066112\n",
            "\n",
            "Step  188700: Ran 100 train steps in 50.66 secs\n",
            "Step  188700: train CrossEntropyLoss |  2.95228696\n",
            "Step  188700: eval  CrossEntropyLoss |  3.10613465\n",
            "Step  188700: eval          Accuracy |  0.44000000\n",
            "\n",
            "Step  188800: Ran 100 train steps in 50.59 secs\n",
            "Step  188800: train CrossEntropyLoss |  2.93877840\n",
            "Step  188800: eval  CrossEntropyLoss |  2.94461632\n",
            "Step  188800: eval          Accuracy |  0.46590909\n",
            "\n",
            "Step  188900: Ran 100 train steps in 50.67 secs\n",
            "Step  188900: train CrossEntropyLoss |  2.96229148\n",
            "Step  188900: eval  CrossEntropyLoss |  3.52078819\n",
            "Step  188900: eval          Accuracy |  0.40196079\n",
            "\n",
            "Step  189000: Ran 100 train steps in 50.56 secs\n",
            "Step  189000: train CrossEntropyLoss |  3.00291777\n",
            "Step  189000: eval  CrossEntropyLoss |  3.60322118\n",
            "Step  189000: eval          Accuracy |  0.42016810\n",
            "\n",
            "Step  189100: Ran 100 train steps in 50.38 secs\n",
            "Step  189100: train CrossEntropyLoss |  2.87170672\n",
            "Step  189100: eval  CrossEntropyLoss |  2.87529302\n",
            "Step  189100: eval          Accuracy |  0.48999998\n",
            "\n",
            "Step  189200: Ran 100 train steps in 50.67 secs\n",
            "Step  189200: train CrossEntropyLoss |  2.95398331\n",
            "Step  189200: eval  CrossEntropyLoss |  3.68356991\n",
            "Step  189200: eval          Accuracy |  0.41935483\n",
            "\n",
            "Step  189300: Ran 100 train steps in 50.37 secs\n",
            "Step  189300: train CrossEntropyLoss |  2.88325429\n",
            "Step  189300: eval  CrossEntropyLoss |  3.27561116\n",
            "Step  189300: eval          Accuracy |  0.44444448\n",
            "\n",
            "Step  189400: Ran 100 train steps in 50.10 secs\n",
            "Step  189400: train CrossEntropyLoss |  3.01451778\n",
            "Step  189400: eval  CrossEntropyLoss |  2.84079790\n",
            "Step  189400: eval          Accuracy |  0.52293575\n",
            "\n",
            "Step  189500: Ran 100 train steps in 50.28 secs\n",
            "Step  189500: train CrossEntropyLoss |  2.92513156\n",
            "Step  189500: eval  CrossEntropyLoss |  3.02834320\n",
            "Step  189500: eval          Accuracy |  0.55652171\n",
            "\n",
            "Step  189600: Ran 100 train steps in 50.13 secs\n",
            "Step  189600: train CrossEntropyLoss |  2.95073056\n",
            "Step  189600: eval  CrossEntropyLoss |  3.51200485\n",
            "Step  189600: eval          Accuracy |  0.38775510\n",
            "\n",
            "Step  189700: Ran 100 train steps in 50.14 secs\n",
            "Step  189700: train CrossEntropyLoss |  2.96594191\n",
            "Step  189700: eval  CrossEntropyLoss |  2.78561115\n",
            "Step  189700: eval          Accuracy |  0.51546395\n",
            "\n",
            "Step  189800: Ran 100 train steps in 50.10 secs\n",
            "Step  189800: train CrossEntropyLoss |  2.90151048\n",
            "Step  189800: eval  CrossEntropyLoss |  3.41233397\n",
            "Step  189800: eval          Accuracy |  0.44444448\n",
            "\n",
            "Step  189900: Ran 100 train steps in 50.26 secs\n",
            "Step  189900: train CrossEntropyLoss |  2.99601030\n",
            "Step  189900: eval  CrossEntropyLoss |  2.59450865\n",
            "Step  189900: eval          Accuracy |  0.58771932\n",
            "\n",
            "Step  190000: Ran 100 train steps in 50.27 secs\n",
            "Step  190000: train CrossEntropyLoss |  2.82066655\n",
            "Step  190000: eval  CrossEntropyLoss |  3.50974011\n",
            "Step  190000: eval          Accuracy |  0.42857146\n",
            "\n",
            "Step  190100: Ran 100 train steps in 50.18 secs\n",
            "Step  190100: train CrossEntropyLoss |  2.91469622\n",
            "Step  190100: eval  CrossEntropyLoss |  3.01517415\n",
            "Step  190100: eval          Accuracy |  0.48039219\n",
            "\n",
            "Step  190200: Ran 100 train steps in 50.10 secs\n",
            "Step  190200: train CrossEntropyLoss |  2.94536400\n",
            "Step  190200: eval  CrossEntropyLoss |  2.82875204\n",
            "Step  190200: eval          Accuracy |  0.54716980\n",
            "\n",
            "Step  190300: Ran 100 train steps in 50.25 secs\n",
            "Step  190300: train CrossEntropyLoss |  2.94816732\n",
            "Step  190300: eval  CrossEntropyLoss |  3.26310205\n",
            "Step  190300: eval          Accuracy |  0.49056605\n",
            "\n",
            "Step  190400: Ran 100 train steps in 50.51 secs\n",
            "Step  190400: train CrossEntropyLoss |  2.90879631\n",
            "Step  190400: eval  CrossEntropyLoss |  3.21253991\n",
            "Step  190400: eval          Accuracy |  0.48076925\n",
            "\n",
            "Step  190500: Ran 100 train steps in 50.52 secs\n",
            "Step  190500: train CrossEntropyLoss |  2.93335271\n",
            "Step  190500: eval  CrossEntropyLoss |  2.40788484\n",
            "Step  190500: eval          Accuracy |  0.58139533\n",
            "\n",
            "Step  190600: Ran 100 train steps in 50.28 secs\n",
            "Step  190600: train CrossEntropyLoss |  2.92704320\n",
            "Step  190600: eval  CrossEntropyLoss |  2.82768154\n",
            "Step  190600: eval          Accuracy |  0.51724136\n",
            "\n",
            "Step  190700: Ran 100 train steps in 50.48 secs\n",
            "Step  190700: train CrossEntropyLoss |  2.89990902\n",
            "Step  190700: eval  CrossEntropyLoss |  2.45184875\n",
            "Step  190700: eval          Accuracy |  0.57291669\n",
            "\n",
            "Step  190800: Ran 100 train steps in 50.55 secs\n",
            "Step  190800: train CrossEntropyLoss |  2.88180780\n",
            "Step  190800: eval  CrossEntropyLoss |  3.56448078\n",
            "Step  190800: eval          Accuracy |  0.40740740\n",
            "\n",
            "Step  190900: Ran 100 train steps in 50.44 secs\n",
            "Step  190900: train CrossEntropyLoss |  2.96776581\n",
            "Step  190900: eval  CrossEntropyLoss |  2.52216816\n",
            "Step  190900: eval          Accuracy |  0.53636360\n",
            "\n",
            "Step  191000: Ran 100 train steps in 50.23 secs\n",
            "Step  191000: train CrossEntropyLoss |  2.89893270\n",
            "Step  191000: eval  CrossEntropyLoss |  3.14178061\n",
            "Step  191000: eval          Accuracy |  0.55045867\n",
            "\n",
            "Step  191100: Ran 100 train steps in 50.20 secs\n",
            "Step  191100: train CrossEntropyLoss |  2.94886875\n",
            "Step  191100: eval  CrossEntropyLoss |  2.33210921\n",
            "Step  191100: eval          Accuracy |  0.63366336\n",
            "\n",
            "Step  191200: Ran 100 train steps in 50.07 secs\n",
            "Step  191200: train CrossEntropyLoss |  2.90924239\n",
            "Step  191200: eval  CrossEntropyLoss |  3.53572559\n",
            "Step  191200: eval          Accuracy |  0.38000000\n",
            "\n",
            "Step  191300: Ran 100 train steps in 50.15 secs\n",
            "Step  191300: train CrossEntropyLoss |  2.88586879\n",
            "Step  191300: eval  CrossEntropyLoss |  2.96925282\n",
            "Step  191300: eval          Accuracy |  0.51376146\n",
            "\n",
            "Step  191400: Ran 100 train steps in 50.23 secs\n",
            "Step  191400: train CrossEntropyLoss |  2.85431004\n",
            "Step  191400: eval  CrossEntropyLoss |  2.86656499\n",
            "Step  191400: eval          Accuracy |  0.49056605\n",
            "\n",
            "Step  191500: Ran 100 train steps in 50.27 secs\n",
            "Step  191500: train CrossEntropyLoss |  2.93696046\n",
            "Step  191500: eval  CrossEntropyLoss |  2.75859237\n",
            "Step  191500: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  191600: Ran 100 train steps in 50.32 secs\n",
            "Step  191600: train CrossEntropyLoss |  2.91914630\n",
            "Step  191600: eval  CrossEntropyLoss |  2.15706372\n",
            "Step  191600: eval          Accuracy |  0.58585858\n",
            "\n",
            "Step  191700: Ran 100 train steps in 50.20 secs\n",
            "Step  191700: train CrossEntropyLoss |  2.93167543\n",
            "Step  191700: eval  CrossEntropyLoss |  2.85231423\n",
            "Step  191700: eval          Accuracy |  0.47058827\n",
            "\n",
            "Step  191800: Ran 100 train steps in 50.49 secs\n",
            "Step  191800: train CrossEntropyLoss |  2.90181351\n",
            "Step  191800: eval  CrossEntropyLoss |  2.85047793\n",
            "Step  191800: eval          Accuracy |  0.54545456\n",
            "\n",
            "Step  191900: Ran 100 train steps in 50.18 secs\n",
            "Step  191900: train CrossEntropyLoss |  2.92223167\n",
            "Step  191900: eval  CrossEntropyLoss |  2.83786726\n",
            "Step  191900: eval          Accuracy |  0.48936167\n",
            "\n",
            "Step  192000: Ran 100 train steps in 50.24 secs\n",
            "Step  192000: train CrossEntropyLoss |  2.87741637\n",
            "Step  192000: eval  CrossEntropyLoss |  3.42964864\n",
            "Step  192000: eval          Accuracy |  0.48305085\n",
            "\n",
            "Step  192100: Ran 100 train steps in 50.43 secs\n",
            "Step  192100: train CrossEntropyLoss |  2.85198236\n",
            "Step  192100: eval  CrossEntropyLoss |  2.67865753\n",
            "Step  192100: eval          Accuracy |  0.51020408\n",
            "\n",
            "Step  192200: Ran 100 train steps in 50.53 secs\n",
            "Step  192200: train CrossEntropyLoss |  2.93131471\n",
            "Step  192200: eval  CrossEntropyLoss |  3.22231364\n",
            "Step  192200: eval          Accuracy |  0.39622641\n",
            "\n",
            "Step  192300: Ran 100 train steps in 50.71 secs\n",
            "Step  192300: train CrossEntropyLoss |  2.83419895\n",
            "Step  192300: eval  CrossEntropyLoss |  1.98218393\n",
            "Step  192300: eval          Accuracy |  0.62135923\n",
            "\n",
            "Step  192400: Ran 100 train steps in 50.60 secs\n",
            "Step  192400: train CrossEntropyLoss |  2.90566635\n",
            "Step  192400: eval  CrossEntropyLoss |  3.03578782\n",
            "Step  192400: eval          Accuracy |  0.45192310\n",
            "\n",
            "Step  192500: Ran 100 train steps in 50.58 secs\n",
            "Step  192500: train CrossEntropyLoss |  2.92213535\n",
            "Step  192500: eval  CrossEntropyLoss |  3.13149643\n",
            "Step  192500: eval          Accuracy |  0.45918366\n",
            "\n",
            "Step  192600: Ran 100 train steps in 50.64 secs\n",
            "Step  192600: train CrossEntropyLoss |  2.93609643\n",
            "Step  192600: eval  CrossEntropyLoss |  2.77023625\n",
            "Step  192600: eval          Accuracy |  0.52631581\n",
            "\n",
            "Step  192700: Ran 100 train steps in 50.53 secs\n",
            "Step  192700: train CrossEntropyLoss |  2.92248869\n",
            "Step  192700: eval  CrossEntropyLoss |  2.13523698\n",
            "Step  192700: eval          Accuracy |  0.63218391\n",
            "\n",
            "Step  192800: Ran 100 train steps in 50.49 secs\n",
            "Step  192800: train CrossEntropyLoss |  2.91066122\n",
            "Step  192800: eval  CrossEntropyLoss |  2.70755672\n",
            "Step  192800: eval          Accuracy |  0.58585858\n",
            "\n",
            "Step  192900: Ran 100 train steps in 50.65 secs\n",
            "Step  192900: train CrossEntropyLoss |  2.90757799\n",
            "Step  192900: eval  CrossEntropyLoss |  3.35714436\n",
            "Step  192900: eval          Accuracy |  0.45833334\n",
            "\n",
            "Step  193000: Ran 100 train steps in 50.51 secs\n",
            "Step  193000: train CrossEntropyLoss |  2.86646652\n",
            "Step  193000: eval  CrossEntropyLoss |  2.65352631\n",
            "Step  193000: eval          Accuracy |  0.54347825\n",
            "\n",
            "Step  193100: Ran 100 train steps in 50.57 secs\n",
            "Step  193100: train CrossEntropyLoss |  2.89751005\n",
            "Step  193100: eval  CrossEntropyLoss |  2.56314969\n",
            "Step  193100: eval          Accuracy |  0.55000001\n",
            "\n",
            "Step  193200: Ran 100 train steps in 50.64 secs\n",
            "Step  193200: train CrossEntropyLoss |  2.84478760\n",
            "Step  193200: eval  CrossEntropyLoss |  2.87915206\n",
            "Step  193200: eval          Accuracy |  0.45794392\n",
            "\n",
            "Step  193300: Ran 100 train steps in 50.54 secs\n",
            "Step  193300: train CrossEntropyLoss |  2.87967849\n",
            "Step  193300: eval  CrossEntropyLoss |  2.71955228\n",
            "Step  193300: eval          Accuracy |  0.50549453\n",
            "\n",
            "Step  193400: Ran 100 train steps in 50.68 secs\n",
            "Step  193400: train CrossEntropyLoss |  2.93664479\n",
            "Step  193400: eval  CrossEntropyLoss |  3.24710345\n",
            "Step  193400: eval          Accuracy |  0.42016810\n",
            "\n",
            "Step  193500: Ran 100 train steps in 50.64 secs\n",
            "Step  193500: train CrossEntropyLoss |  2.95346189\n",
            "Step  193500: eval  CrossEntropyLoss |  3.18269253\n",
            "Step  193500: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  193600: Ran 100 train steps in 50.56 secs\n",
            "Step  193600: train CrossEntropyLoss |  2.88689995\n",
            "Step  193600: eval  CrossEntropyLoss |  2.76411843\n",
            "Step  193600: eval          Accuracy |  0.46788988\n",
            "\n",
            "Step  193700: Ran 100 train steps in 50.54 secs\n",
            "Step  193700: train CrossEntropyLoss |  2.86070275\n",
            "Step  193700: eval  CrossEntropyLoss |  2.72902608\n",
            "Step  193700: eval          Accuracy |  0.48543689\n",
            "\n",
            "Step  193800: Ran 100 train steps in 50.61 secs\n",
            "Step  193800: train CrossEntropyLoss |  2.80136871\n",
            "Step  193800: eval  CrossEntropyLoss |  2.88707709\n",
            "Step  193800: eval          Accuracy |  0.49074075\n",
            "\n",
            "Step  193900: Ran 100 train steps in 50.63 secs\n",
            "Step  193900: train CrossEntropyLoss |  2.84344864\n",
            "Step  193900: eval  CrossEntropyLoss |  2.94632888\n",
            "Step  193900: eval          Accuracy |  0.43396229\n",
            "\n",
            "Step  194000: Ran 100 train steps in 50.87 secs\n",
            "Step  194000: train CrossEntropyLoss |  2.85474443\n",
            "Step  194000: eval  CrossEntropyLoss |  3.04616094\n",
            "Step  194000: eval          Accuracy |  0.48543689\n",
            "\n",
            "Step  194100: Ran 100 train steps in 50.84 secs\n",
            "Step  194100: train CrossEntropyLoss |  2.89003325\n",
            "Step  194100: eval  CrossEntropyLoss |  2.45782232\n",
            "Step  194100: eval          Accuracy |  0.59433961\n",
            "\n",
            "Step  194200: Ran 100 train steps in 50.70 secs\n",
            "Step  194200: train CrossEntropyLoss |  2.85336900\n",
            "Step  194200: eval  CrossEntropyLoss |  3.20785332\n",
            "Step  194200: eval          Accuracy |  0.44897959\n",
            "\n",
            "Step  194300: Ran 100 train steps in 50.88 secs\n",
            "Step  194300: train CrossEntropyLoss |  2.85507011\n",
            "Step  194300: eval  CrossEntropyLoss |  3.01636934\n",
            "Step  194300: eval          Accuracy |  0.49549550\n",
            "\n",
            "Step  194400: Ran 100 train steps in 50.74 secs\n",
            "Step  194400: train CrossEntropyLoss |  2.91279483\n",
            "Step  194400: eval  CrossEntropyLoss |  2.55371976\n",
            "Step  194400: eval          Accuracy |  0.52631581\n",
            "\n",
            "Step  194500: Ran 100 train steps in 50.74 secs\n",
            "Step  194500: train CrossEntropyLoss |  2.90662527\n",
            "Step  194500: eval  CrossEntropyLoss |  3.05467939\n",
            "Step  194500: eval          Accuracy |  0.51260507\n",
            "\n",
            "Step  194600: Ran 100 train steps in 50.69 secs\n",
            "Step  194600: train CrossEntropyLoss |  2.92696524\n",
            "Step  194600: eval  CrossEntropyLoss |  2.12126303\n",
            "Step  194600: eval          Accuracy |  0.58585858\n",
            "\n",
            "Step  194700: Ran 100 train steps in 50.96 secs\n",
            "Step  194700: train CrossEntropyLoss |  2.85751581\n",
            "Step  194700: eval  CrossEntropyLoss |  3.39073300\n",
            "Step  194700: eval          Accuracy |  0.43750003\n",
            "\n",
            "Step  194800: Ran 100 train steps in 50.88 secs\n",
            "Step  194800: train CrossEntropyLoss |  2.88028598\n",
            "Step  194800: eval  CrossEntropyLoss |  3.15408492\n",
            "Step  194800: eval          Accuracy |  0.45555556\n",
            "\n",
            "Step  194900: Ran 100 train steps in 51.02 secs\n",
            "Step  194900: train CrossEntropyLoss |  2.82045078\n",
            "Step  194900: eval  CrossEntropyLoss |  3.12564850\n",
            "Step  194900: eval          Accuracy |  0.47663549\n",
            "\n",
            "Step  195000: Ran 100 train steps in 50.77 secs\n",
            "Step  195000: train CrossEntropyLoss |  2.88631654\n",
            "Step  195000: eval  CrossEntropyLoss |  3.37941790\n",
            "Step  195000: eval          Accuracy |  0.47727275\n",
            "\n",
            "Step  195100: Ran 100 train steps in 50.86 secs\n",
            "Step  195100: train CrossEntropyLoss |  2.88564301\n",
            "Step  195100: eval  CrossEntropyLoss |  3.75820541\n",
            "Step  195100: eval          Accuracy |  0.42592594\n",
            "\n",
            "Step  195200: Ran 100 train steps in 50.87 secs\n",
            "Step  195200: train CrossEntropyLoss |  2.86167717\n",
            "Step  195200: eval  CrossEntropyLoss |  3.09864807\n",
            "Step  195200: eval          Accuracy |  0.46226415\n",
            "\n",
            "Step  195300: Ran 100 train steps in 50.86 secs\n",
            "Step  195300: train CrossEntropyLoss |  2.83728361\n",
            "Step  195300: eval  CrossEntropyLoss |  2.90497327\n",
            "Step  195300: eval          Accuracy |  0.54545450\n",
            "\n",
            "Step  195400: Ran 100 train steps in 50.89 secs\n",
            "Step  195400: train CrossEntropyLoss |  2.93384051\n",
            "Step  195400: eval  CrossEntropyLoss |  2.50243998\n",
            "Step  195400: eval          Accuracy |  0.57575756\n",
            "\n",
            "Step  195500: Ran 100 train steps in 50.90 secs\n",
            "Step  195500: train CrossEntropyLoss |  2.83412266\n",
            "Step  195500: eval  CrossEntropyLoss |  2.02029252\n",
            "Step  195500: eval          Accuracy |  0.61616164\n",
            "\n",
            "Step  195600: Ran 100 train steps in 50.86 secs\n",
            "Step  195600: train CrossEntropyLoss |  2.89066744\n",
            "Step  195600: eval  CrossEntropyLoss |  3.62634993\n",
            "Step  195600: eval          Accuracy |  0.38636366\n",
            "\n",
            "Step  195700: Ran 100 train steps in 50.84 secs\n",
            "Step  195700: train CrossEntropyLoss |  2.88352132\n",
            "Step  195700: eval  CrossEntropyLoss |  3.45571518\n",
            "Step  195700: eval          Accuracy |  0.44915253\n",
            "\n",
            "Step  195800: Ran 100 train steps in 50.55 secs\n",
            "Step  195800: train CrossEntropyLoss |  2.85683012\n",
            "Step  195800: eval  CrossEntropyLoss |  2.63992429\n",
            "Step  195800: eval          Accuracy |  0.53061223\n",
            "\n",
            "Step  195900: Ran 100 train steps in 50.86 secs\n",
            "Step  195900: train CrossEntropyLoss |  2.84775758\n",
            "Step  195900: eval  CrossEntropyLoss |  3.35233688\n",
            "Step  195900: eval          Accuracy |  0.39837402\n",
            "\n",
            "Step  196000: Ran 100 train steps in 50.59 secs\n",
            "Step  196000: train CrossEntropyLoss |  2.97811341\n",
            "Step  196000: eval  CrossEntropyLoss |  2.03311896\n",
            "Step  196000: eval          Accuracy |  0.61855674\n",
            "\n",
            "Step  196100: Ran 100 train steps in 50.52 secs\n",
            "Step  196100: train CrossEntropyLoss |  2.88827634\n",
            "Step  196100: eval  CrossEntropyLoss |  2.72791576\n",
            "Step  196100: eval          Accuracy |  0.56363636\n",
            "\n",
            "Step  196200: Ran 100 train steps in 50.70 secs\n",
            "Step  196200: train CrossEntropyLoss |  2.87984824\n",
            "Step  196200: eval  CrossEntropyLoss |  2.79803276\n",
            "Step  196200: eval          Accuracy |  0.61538464\n",
            "\n",
            "Step  196300: Ran 100 train steps in 51.10 secs\n",
            "Step  196300: train CrossEntropyLoss |  2.88718200\n",
            "Step  196300: eval  CrossEntropyLoss |  2.49873471\n",
            "Step  196300: eval          Accuracy |  0.52631581\n",
            "\n",
            "Step  196400: Ran 100 train steps in 50.97 secs\n",
            "Step  196400: train CrossEntropyLoss |  2.84958124\n",
            "Step  196400: eval  CrossEntropyLoss |  2.87704349\n",
            "Step  196400: eval          Accuracy |  0.46938774\n",
            "\n",
            "Step  196500: Ran 100 train steps in 51.06 secs\n",
            "Step  196500: train CrossEntropyLoss |  2.93073916\n",
            "Step  196500: eval  CrossEntropyLoss |  2.79092145\n",
            "Step  196500: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  196600: Ran 100 train steps in 50.91 secs\n",
            "Step  196600: train CrossEntropyLoss |  2.93790126\n",
            "Step  196600: eval  CrossEntropyLoss |  3.34957910\n",
            "Step  196600: eval          Accuracy |  0.44186047\n",
            "\n",
            "Step  196700: Ran 100 train steps in 50.94 secs\n",
            "Step  196700: train CrossEntropyLoss |  2.84841299\n",
            "Step  196700: eval  CrossEntropyLoss |  3.37466717\n",
            "Step  196700: eval          Accuracy |  0.46788988\n",
            "\n",
            "Step  196800: Ran 100 train steps in 51.03 secs\n",
            "Step  196800: train CrossEntropyLoss |  2.82258296\n",
            "Step  196800: eval  CrossEntropyLoss |  2.78223062\n",
            "Step  196800: eval          Accuracy |  0.55660379\n",
            "\n",
            "Step  196900: Ran 100 train steps in 50.96 secs\n",
            "Step  196900: train CrossEntropyLoss |  2.92681241\n",
            "Step  196900: eval  CrossEntropyLoss |  2.76189709\n",
            "Step  196900: eval          Accuracy |  0.59813082\n",
            "\n",
            "Step  197000: Ran 100 train steps in 51.08 secs\n",
            "Step  197000: train CrossEntropyLoss |  2.89887238\n",
            "Step  197000: eval  CrossEntropyLoss |  3.01659083\n",
            "Step  197000: eval          Accuracy |  0.48648649\n",
            "\n",
            "Step  197100: Ran 100 train steps in 51.02 secs\n",
            "Step  197100: train CrossEntropyLoss |  2.81173420\n",
            "Step  197100: eval  CrossEntropyLoss |  2.97607589\n",
            "Step  197100: eval          Accuracy |  0.46788988\n",
            "\n",
            "Step  197200: Ran 100 train steps in 50.91 secs\n",
            "Step  197200: train CrossEntropyLoss |  2.80504012\n",
            "Step  197200: eval  CrossEntropyLoss |  2.90518332\n",
            "Step  197200: eval          Accuracy |  0.52678573\n",
            "\n",
            "Step  197300: Ran 100 train steps in 51.09 secs\n",
            "Step  197300: train CrossEntropyLoss |  2.82807589\n",
            "Step  197300: eval  CrossEntropyLoss |  2.78117418\n",
            "Step  197300: eval          Accuracy |  0.46590909\n",
            "\n",
            "Step  197400: Ran 100 train steps in 50.84 secs\n",
            "Step  197400: train CrossEntropyLoss |  2.80856562\n",
            "Step  197400: eval  CrossEntropyLoss |  2.91211700\n",
            "Step  197400: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step  197500: Ran 100 train steps in 51.06 secs\n",
            "Step  197500: train CrossEntropyLoss |  2.88951826\n",
            "Step  197500: eval  CrossEntropyLoss |  2.24899340\n",
            "Step  197500: eval          Accuracy |  0.61666667\n",
            "\n",
            "Step  197600: Ran 100 train steps in 51.09 secs\n",
            "Step  197600: train CrossEntropyLoss |  2.85500813\n",
            "Step  197600: eval  CrossEntropyLoss |  2.79503131\n",
            "Step  197600: eval          Accuracy |  0.47368422\n",
            "\n",
            "Step  197700: Ran 100 train steps in 51.34 secs\n",
            "Step  197700: train CrossEntropyLoss |  2.87764835\n",
            "Step  197700: eval  CrossEntropyLoss |  3.19066620\n",
            "Step  197700: eval          Accuracy |  0.45631069\n",
            "\n",
            "Step  197800: Ran 100 train steps in 50.74 secs\n",
            "Step  197800: train CrossEntropyLoss |  2.92295098\n",
            "Step  197800: eval  CrossEntropyLoss |  3.23870683\n",
            "Step  197800: eval          Accuracy |  0.47169811\n",
            "\n",
            "Step  197900: Ran 100 train steps in 50.76 secs\n",
            "Step  197900: train CrossEntropyLoss |  2.83700371\n",
            "Step  197900: eval  CrossEntropyLoss |  3.10313439\n",
            "Step  197900: eval          Accuracy |  0.47999999\n",
            "\n",
            "Step  198000: Ran 100 train steps in 50.86 secs\n",
            "Step  198000: train CrossEntropyLoss |  2.87974691\n",
            "Step  198000: eval  CrossEntropyLoss |  2.49781156\n",
            "Step  198000: eval          Accuracy |  0.50999999\n",
            "\n",
            "Step  198100: Ran 100 train steps in 50.75 secs\n",
            "Step  198100: train CrossEntropyLoss |  2.84565830\n",
            "Step  198100: eval  CrossEntropyLoss |  2.47742796\n",
            "Step  198100: eval          Accuracy |  0.57777780\n",
            "\n",
            "Step  198200: Ran 100 train steps in 50.83 secs\n",
            "Step  198200: train CrossEntropyLoss |  2.87468004\n",
            "Step  198200: eval  CrossEntropyLoss |  2.56719851\n",
            "Step  198200: eval          Accuracy |  0.53703701\n",
            "\n",
            "Step  198300: Ran 100 train steps in 50.75 secs\n",
            "Step  198300: train CrossEntropyLoss |  2.76267052\n",
            "Step  198300: eval  CrossEntropyLoss |  2.23902655\n",
            "Step  198300: eval          Accuracy |  0.60714287\n",
            "\n",
            "Step  198400: Ran 100 train steps in 50.72 secs\n",
            "Step  198400: train CrossEntropyLoss |  2.84943509\n",
            "Step  198400: eval  CrossEntropyLoss |  3.05490279\n",
            "Step  198400: eval          Accuracy |  0.48245615\n",
            "\n",
            "Step  198500: Ran 100 train steps in 50.68 secs\n",
            "Step  198500: train CrossEntropyLoss |  2.82055283\n",
            "Step  198500: eval  CrossEntropyLoss |  3.55555153\n",
            "Step  198500: eval          Accuracy |  0.43809524\n",
            "\n",
            "Step  198600: Ran 100 train steps in 50.31 secs\n",
            "Step  198600: train CrossEntropyLoss |  2.84438658\n",
            "Step  198600: eval  CrossEntropyLoss |  3.29333854\n",
            "Step  198600: eval          Accuracy |  0.43396229\n",
            "\n",
            "Step  198700: Ran 100 train steps in 50.41 secs\n",
            "Step  198700: train CrossEntropyLoss |  2.79505157\n",
            "Step  198700: eval  CrossEntropyLoss |  3.15818167\n",
            "Step  198700: eval          Accuracy |  0.41964287\n",
            "\n",
            "Step  198800: Ran 100 train steps in 50.73 secs\n",
            "Step  198800: train CrossEntropyLoss |  2.81512618\n",
            "Step  198800: eval  CrossEntropyLoss |  2.83222485\n",
            "Step  198800: eval          Accuracy |  0.54464287\n",
            "\n",
            "Step  198900: Ran 100 train steps in 50.23 secs\n",
            "Step  198900: train CrossEntropyLoss |  2.80520439\n",
            "Step  198900: eval  CrossEntropyLoss |  2.73414278\n",
            "Step  198900: eval          Accuracy |  0.51648355\n",
            "\n",
            "Step  199000: Ran 100 train steps in 50.42 secs\n",
            "Step  199000: train CrossEntropyLoss |  2.85446978\n",
            "Step  199000: eval  CrossEntropyLoss |  2.46505046\n",
            "Step  199000: eval          Accuracy |  0.60606062\n",
            "\n",
            "Step  199100: Ran 100 train steps in 50.48 secs\n",
            "Step  199100: train CrossEntropyLoss |  2.86278224\n",
            "Step  199100: eval  CrossEntropyLoss |  2.51609230\n",
            "Step  199100: eval          Accuracy |  0.53043479\n",
            "\n",
            "Step  199200: Ran 100 train steps in 50.63 secs\n",
            "Step  199200: train CrossEntropyLoss |  2.85103106\n",
            "Step  199200: eval  CrossEntropyLoss |  2.72287321\n",
            "Step  199200: eval          Accuracy |  0.49504951\n",
            "\n",
            "Step  199300: Ran 100 train steps in 50.87 secs\n",
            "Step  199300: train CrossEntropyLoss |  2.77237034\n",
            "Step  199300: eval  CrossEntropyLoss |  3.17520714\n",
            "Step  199300: eval          Accuracy |  0.46739131\n",
            "\n",
            "Step  199400: Ran 100 train steps in 50.80 secs\n",
            "Step  199400: train CrossEntropyLoss |  2.88110995\n",
            "Step  199400: eval  CrossEntropyLoss |  2.82037902\n",
            "Step  199400: eval          Accuracy |  0.53999996\n",
            "\n",
            "Step  199500: Ran 100 train steps in 50.71 secs\n",
            "Step  199500: train CrossEntropyLoss |  2.91212487\n",
            "Step  199500: eval  CrossEntropyLoss |  3.53984952\n",
            "Step  199500: eval          Accuracy |  0.43809524\n",
            "\n",
            "Step  199600: Ran 100 train steps in 50.66 secs\n",
            "Step  199600: train CrossEntropyLoss |  2.87084293\n",
            "Step  199600: eval  CrossEntropyLoss |  2.41136885\n",
            "Step  199600: eval          Accuracy |  0.54285717\n",
            "\n",
            "Step  199700: Ran 100 train steps in 50.55 secs\n",
            "Step  199700: train CrossEntropyLoss |  2.80284572\n",
            "Step  199700: eval  CrossEntropyLoss |  2.79408813\n",
            "Step  199700: eval          Accuracy |  0.52777779\n",
            "\n",
            "Step  199800: Ran 100 train steps in 50.22 secs\n",
            "Step  199800: train CrossEntropyLoss |  2.90747285\n",
            "Step  199800: eval  CrossEntropyLoss |  3.59637451\n",
            "Step  199800: eval          Accuracy |  0.35922331\n",
            "\n",
            "Step  199900: Ran 100 train steps in 50.35 secs\n",
            "Step  199900: train CrossEntropyLoss |  2.79580188\n",
            "Step  199900: eval  CrossEntropyLoss |  2.47018123\n",
            "Step  199900: eval          Accuracy |  0.57943922\n",
            "\n",
            "Step  200000: Ran 100 train steps in 50.43 secs\n",
            "Step  200000: train CrossEntropyLoss |  2.85295558\n",
            "Step  200000: eval  CrossEntropyLoss |  2.93357468\n",
            "Step  200000: eval          Accuracy |  0.49056605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRiRSHgxplu"
      },
      "source": [
        "# sync the train dir with Google Drive dir\r\n",
        "# !rsync -a /content/drive/MyDrive/model2/ ~/\r\n",
        "\r\n",
        "# copy the model to Google Drive\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model/\r\n",
        "\r\n",
        "# sync Google Drive dir with the train dir\r\n",
        "# !rsync -a ~/model /content/drive/MyDrive/model2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG__Khf34G6H"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5r57o-4sqq"
      },
      "source": [
        "### Predict next symbol (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFtOwQS9xxsC"
      },
      "source": [
        "# Get the model architecture\r\n",
        "model = SumTransformer(mode='eval')\r\n",
        "\r\n",
        "# Load the pre-trained weights\r\n",
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKsw20txykZ"
      },
      "source": [
        "def next_symbol(cur_output_tokens, model):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # current output tokens length\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "    # calculate the minimum power of 2 big enough to store token_length\r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\r\n",
        "    padded = list(cur_output_tokens) + [0] * (padded_length - token_length)\r\n",
        "    padded_with_batch = np.array(padded)[None, :] # setting the batch dim\r\n",
        "\r\n",
        "    # model expects a tuple containing two padded tensors (with batch)\r\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \r\n",
        "    # To get log_probs you need to index output with 0 in the first dim\r\n",
        "    # token_length in the second dim and all of the entries for the last dim.\r\n",
        "    log_probs = output[0, token_length, :]\r\n",
        "    \r\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFQl767yHwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a573868d-694c-49e6-dd1c-dc61dd4256ae"
      },
      "source": [
        "train_article = train_text_pairs[5][0]\r\n",
        "train_summary = train_text_pairs[5][1]\r\n",
        "print(wrapper.fill(train_article))\r\n",
        "print('')\r\n",
        "eval_article = eval_text_pairs[1][0]\r\n",
        "eval_summary = eval_text_pairs[1][1]\r\n",
        "print(wrapper.fill(eval_article))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые придумали новый способ взаимодействия с графеном, который\n",
            "позволяет избавиться от \"слипающихся\" листов. статья ученых появилась\n",
            "в журнале acs nano, а ее краткое изложение приводится на сайте северо-\n",
            "западного университета, сотрудники которого принимали участие в\n",
            "работе. известно, что основной трудностью при работе с графеновыми\n",
            "листами является то, что при соприкосновении они слипаются под\n",
            "воздействием сил ван-дер-ваальса между собой при наложении друг на\n",
            "друга. это приводит к потере большинства уникальных свойств материала.\n",
            "для решения подобной проблемы, например, некоторые исследователи\n",
            "кладут между листами прокладки из другого материала, однако такое\n",
            "решение часто не слишком эффективно - атомы прокладки могут\n",
            "образовывать связи с атомами углерода в графене, что снова приводит к\n",
            "появлению дефектов в материале. в рамках нового исследования ученые\n",
            "предложили использовать графен не в виде ровных листов, а в виде\n",
            "смятых в комок листов. по словам исследователей, в подобном виде\n",
            "графен ведет себя как бумажные комки в мусорной корзине - несмотря на\n",
            "достаточно плотное расположение, поверхности листов, из которых они\n",
            "состоят, не соприкасаются. расчеты показывают, что при подобной\n",
            "упаковке листов графен сохраняет около 45 процентов исходной площади\n",
            "поверхности. для сравнения, при других способах организации удается\n",
            "спасти не более 16 процентов площади. графен как теоретическая\n",
            "абстракция рассматривался еще в конце 20-х годов прошлого века.\n",
            "начиная с 1960-х годов, он выступал в качестве удобной математической\n",
            "модели для расчетов в квантовой механике. впервые графен получили на\n",
            "практике константин новоселов и андрей гейм в 2004 году.\n",
            "\n",
            "сша планируют сократить численность военного контингента в южной\n",
            "корее. по информации корейского министерства иностранных дел, к концу\n",
            "2005 года из страны будет выведена треть американского контингента,\n",
            "составляющего в настоящее время 37500 военнослужащих, сообщает\n",
            "reuters. всего к концу 2005 года страну покинут 12500 американских\n",
            "солдат. 3600 из них продолжат службу в ираке. глава корейского мид\n",
            "отметил, что сша подходят к выводу войск очень внимательно, так как\n",
            "ситуация на полуострове остается напряженной. тем не менее, сша пошли\n",
            "навстречу желанию властей южной кореи иметь более независимую армию, и\n",
            "обещают оказать им в этом всяческое содействие. собственные силы южной\n",
            "кореи составляют на сегодняшний день 690 000 человек. армия северной\n",
            "кореи насчитывает 1 100 000 военнослужащих.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QhMIlexynh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fb8fb7-2d38-4be8-962a-ae5b78f48c64"
      },
      "source": [
        "# checking first symbol generation\r\n",
        "print(detokenize([next_symbol(tokenize(train_article)+[0], model)]))\r\n",
        "print(detokenize([next_symbol(tokenize(eval_article)+[0], model)]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые\n",
            "сша\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjwQxAlL5BkH"
      },
      "source": [
        "### Greedy decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Od6PagJxyrt"
      },
      "source": [
        "def greedy_decode(input_sentence, model):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (string): a sentence or article.\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\r\n",
        "    generated_output = [] \r\n",
        "    cur_output = 0 \r\n",
        "    EOS = 1 \r\n",
        "    \r\n",
        "    while cur_output != EOS:\r\n",
        "\r\n",
        "        print(detokenize(generated_output))\r\n",
        "        # Get next symbol\r\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\r\n",
        "        # Append next symbol to original sentence\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "        \r\n",
        "        # Append next symbol to generated sentence\r\n",
        "        generated_output.append(cur_output)\r\n",
        "\r\n",
        "    return detokenize(generated_output)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_F2WnRyAnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f498a8ac-981d-444e-cea1-1597bbb07788"
      },
      "source": [
        "print(train_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(train_article, model)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ученые предложили использовать графен в мятом виде\n",
            "\n",
            "\n",
            "ученые\n",
            "ученые нашли\n",
            "ученые нашли способ\n",
            "ученые нашли способ с\n",
            "ученые нашли способ сже\n",
            "ученые нашли способ сжечь\n",
            "ученые нашли способ сжечься\n",
            "ученые нашли способ сжечься от\n",
            "ученые нашли способ сжечься от \"\n",
            "ученые нашли способ сжечься от \"сли\n",
            "ученые нашли способ сжечься от \"слипа\n",
            "ученые нашли способ сжечься от \"слипающихся\n",
            "ученые нашли способ сжечься от \"слипающихся\"\n",
            "ученые нашли способ сжечься от \"слипающихся\" ли\n",
            "ученые нашли способ сжечься от \"слипающихся\" листов\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhM5hjzcezLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9a061a-1801-46ae-e56c-daf52894d6a2"
      },
      "source": [
        "print(eval_summary)\r\n",
        "print('')\r\n",
        "_ = greedy_decode(eval_article, model)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша на треть сократят корейскую группировку\n",
            "\n",
            "\n",
            "сша\n",
            "сша сокра\n",
            "сша сократят\n",
            "сша сократят вывод\n",
            "сша сократят вывод войск\n",
            "сша сократят вывод войск в\n",
            "сша сократят вывод войск в южной\n",
            "сша сократят вывод войск в южной корее\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWzzpYrfD1y"
      },
      "source": [
        "from trax.supervised import decoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L83lEskk4L7"
      },
      "source": [
        "model = SumTransformer(vocab_size=vocab_size,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='eval',\r\n",
        "                  ff_activation=tl.Relu)\r\n",
        "\r\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSDbAXjlF2f"
      },
      "source": [
        "model.init_from_file('/root/model/model.pkl.gz', weights_only=True, input_signature=shape11)\r\n",
        "\r\n",
        "# save the starting state\r\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-yINo6McPK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c163f460-e5ce-4b41-d4f3-dc85f032398d"
      },
      "source": [
        "np.array(tokenize(eval_article))[None, :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  401,  5641,  7162, 10989,  4302, 15374,     5,  3396, 11441,\n",
              "        15949,    17,  1205,  8980,   204,  2919,  2799,  1996, 15945,\n",
              "           64,  6453,  2730,   173,    86,   719,   372,   102,  1982,\n",
              "        15926,  1469,  3148, 15374, 15945,  1191,  1839,     5,  1416,\n",
              "          368,  4897, 15974,   423,  4061, 15945,   258,  1761, 15949,\n",
              "          983,    64,  6453,  2730,   173,  4387,  2208,  1132, 14538,\n",
              "          423,  3019,  4848, 15949,  3525,   423,    86,   994,  1869,\n",
              "        15930,  6533,     5,  3961, 15949,   915,  8980,   204,  2872,\n",
              "          996, 15945,    79,   401,   149,  2692,    64,  6696,  3516,\n",
              "         2176,  2557,   833, 15945,   208,   207,  4369,    25, 12391,\n",
              "         4875,  6328, 11725, 15949,   734,    57,  1977, 15945,   401,\n",
              "         6176,    25, 15933,   443,  1323,  3054,   810,  2964,  3396,\n",
              "         5813,  6940,   565,  3121,  4819, 13988, 15945,    16,  4424,\n",
              "          162,  9965,   680,     5,   226,  6550,  7020,    48,  8164,\n",
              "        15949,  9568,  3864,  3396,  5813,  8089,    25,  7553,   892,\n",
              "         9250, 15958, 15924, 10749,   534, 15949,  7248,  3768,  5813,\n",
              "        15210,   107,  1728, 15924, 10749,  4061, 15949,     1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_VpGTZgezTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197d3f96-edf8-4e30-aa29-c3d640ad4787"
      },
      "source": [
        "\r\n",
        "# Temperature is a parameter for sampling.\r\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\r\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\r\n",
        "model.state = STARTING_STATE\r\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(tokenize(eval_article) + [0])[None, :],\r\n",
        "                                        temperature=0.3, max_length=20)\r\n",
        "print(wrapper.fill(detokenize(output[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сша евро euro евро евро евро евро евро евро евро евро евро евро евро\n",
            "евротяже евро евро рублей евро\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}